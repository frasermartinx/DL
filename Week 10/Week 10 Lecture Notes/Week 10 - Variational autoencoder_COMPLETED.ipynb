{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 10: Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. Autoencoders](#autoencoders)\n",
    "\n",
    "[3. CNN Autoencoder Example (\\*)](#cnn_autoencoder_example)\n",
    "\n",
    "[4. The variational autoencoder](#vae)\n",
    "\n",
    "[5. Probabilistic layers (\\*)](#probabilisticlayers)\n",
    "\n",
    "[6. The KLDivergenceAddLoss layer (\\*)](#kldivlayer)\n",
    "\n",
    "[7. Improved posterior approximation with IAF](#iaf)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last weeks of the module we saw how the TensorFlow Probability library can be used to develop normalising flow probabilistic deep learning models. These types of models use two of the core objects in the TFP library: `Distribution` and `Bijector` objects. \n",
    "\n",
    "You learned about autoregressive flows such as the masked autoregressive flow (MAF), NICE and RealNVP, and saw how these flows can be built and trained using pre-defined bijectors in TFP, or with bijector subclassing. \n",
    "\n",
    "In this week of the module, we will look at another important deep learning algorithm: the variational autoencoder, or VAE. The VAE is an algorithm for inference and learning in a latent variable generative model. It has been successfully applied in a variety of application domains, such as neuroimaging ([Benou et al 2016](#Benou16)), drug discovery ([Jin et al 2018](#Jin18)), anomaly detection ([Xu et al 2018](#Xu18)) and music generation ([Dhariwal et al 2020](#Dhariwal20)).\n",
    "\n",
    "In its simplest form, the VAE is an unsupervised learning algorithm, and like normalising flows, the generative model can be used to create new examples similar to the dataset. However, unlike normalising flows, the generative model is not invertible, and so it's not as straightforward to train the model using maximum likelihood.\n",
    "\n",
    "The VAE uses the principle of variational inference to approximate the posterior distribution, by defining a parameterised family of distributions conditioned on a data example, and then maximising a lower bound on the marginal likelihood. This is the evidence lower bound, or ELBO.\n",
    "\n",
    "In this week, we'll build up the pieces we need to implement a variational autoencoder with TensorFlow and TensorFlow Probability, starting with looking at building regular autoencoder architectures. In addition to the tools we have already looked at, we will be making use of probabilistic layers, which are found in the `layers` module of TFP.\n",
    "\n",
    "We will see that a key challenge in the variational autoencoder is the approximation of the true posterior distribution. In the final part of the week we will take a look at one method to improve this approximation, making use of normalising flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"autoencoders\"></a>\n",
    "## Autoencoders\n",
    "\n",
    "In this section, we'll look at how to implement a standard autoencoder architecture. \n",
    "\n",
    "An autoencoder can be viewed as a compression algorithm, similar to a VAE, although it's not a probabilistic model, and it is not a model of the underlying data distribution.\n",
    "\n",
    "The aim of an autoencoder is to learn an efficient data encoding. The network is normally trained in an unsupervised manner, and the task of the network is to reproduce its input as its output. \n",
    "\n",
    "<img src=\"figures/autoencoder.png\" alt=\"Autoencoder network\" style=\"width: 750px;\"/>\n",
    "<center>An autoencoder network architecture, with bottleneck latent variable $\\mathbf{z}$</center>\n",
    "\n",
    "The autoencoder has a bottleneck architecture as in the above figure, and can be broken into two parts: the **encoder** network and the **decoder** network. In the middle of the bottleneck is the latent variable $\\mathbf{z}$, which captures the encoding of the data. The dimensionality of $\\mathbf{z}$ is typically much lower than the data $\\mathbf{x}$, and so the network is trained to perform nonlinear dimension reduction ([Kramer 1991](#Kramer91)). The job of the encoder is to learn an efficient representation of the data in a much lower dimensional encoding space, whilst the decoder is required to decompress the latent code to reconstruct the data input $\\mathbf{x}$.\n",
    "\n",
    "For an autoencoder network $f_{\\mathbf{\\theta}}$, the model is trained to minimise the loss\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal{D}_{train}) = \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{x_i\\in\\mathcal{D}_{train}}l(x_i, f_\\theta(x_i)),\n",
    "$$\n",
    "\n",
    "where $l: \\mathbb{R}^D\\times \\mathbb{R}^D\\mapsto\\mathbb{R}$ is a suitable loss function, such as mean squared error. In practice, the model is trained using minibatches of data as usual.\n",
    "\n",
    "There are several variants of the autoencoder model, one notable example being the **denoising autoencoder** ([Vincent & Larochelle 2010](#Vincent10)). In this model, the input $\\mathbf{x}$ is corrupted with noise to produce the input $\\tilde{\\mathbf{x}}$, and the model is trained to minimise the loss\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal{D}_{train}) = \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{x_i\\in\\mathcal{D}_{train}}l(x_i, f_\\theta(\\tilde{x}_i)).\n",
    "$$\n",
    "\n",
    "In other words, the model is tasked to clean the corrupted input by encoding it into a suitable representation. Intuitively, this is motivated by the idea that good representations should be robust to the corruption of the input $\\mathbf{x}$, and that to denoise the input successfully, the model needs to extract features that capture useful structure in the distribution of the input, and ignore features in the data that are unimportant.\n",
    "\n",
    "The noise is typically injected stochastically during the training run, according to a prescribed distribution $q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})$, so that the noise is different on each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cnn_autoencoder_example\"></a>\n",
    "## CNN Autoencoder Example\n",
    "\n",
    "In this section, we will implement a CNN autoencoder for the Fashion-MNIST dataset, and examine the learned encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fashion-MNIST dataset can be loaded with the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the class names\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class_names = np.array(['T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress',\n",
    "                        'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag','Ankle boot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display a few examples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "inx = np.random.choice(x_train.shape[0], n_rows*n_cols, replace=False)\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "for n, (image, label) in enumerate(zip(x_train[inx], y_train[inx])):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='binary')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -2.5, f'{class_names[label]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the CNN autoencoder model\n",
    "We define the encoder and decoder networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN encoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "encoded_dim = 2\n",
    "\n",
    "cnn_encoder = Sequential([\n",
    "    Conv2D(16, 5, activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPool2D(2),\n",
    "    Conv2D(8, 5, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(encoded_dim)\n",
    "])\n",
    "cnn_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute encodings before training\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 1000, replace=False)\n",
    "untrained_encodings = cnn_encoder(x_test[inx]).numpy()\n",
    "untrained_encoding_labels = y_test[inx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot untrained encodings\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "cmap = cm.get_cmap('jet', 10)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(untrained_encoding_labels == i)[0]\n",
    "    plt.scatter(untrained_encodings[inx, 0], untrained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images before training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN decoder\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, UpSampling2D, Conv2DTranspose\n",
    "\n",
    "cnn_decoder = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(encoded_dim,)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Reshape((8, 8, 8)),\n",
    "    Conv2DTranspose(16, 5, activation='relu'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2DTranspose(1, 5, activation='sigmoid')\n",
    "])\n",
    "cnn_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "cnn_autoencoder = Model(inputs=cnn_encoder.input, outputs=cnn_decoder(cnn_encoder.output)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train and test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train and test sets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, x_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the datasets\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "cnn_autoencoder.compile(loss='binary_crossentropy')\n",
    "cnn_autoencoder.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute encodings after training\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 1000, replace=False)\n",
    "trained_encodings = cnn_encoder(x_test[inx]).numpy()\n",
    "trained_encoding_labels = y_test[inx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot untrained and trained encodings\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "cmap = cm.get_cmap('jet', 10)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(untrained_encoding_labels == i)[0]\n",
    "    plt.scatter(untrained_encodings[inx, 0], untrained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images before training')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, class_label in enumerate(class_names):\n",
    "    inx = np.where(trained_encoding_labels == i)[0]\n",
    "    plt.scatter(trained_encodings[inx, 0], trained_encodings[inx, 1],\n",
    "                color=cmap(i), label=class_label, alpha=0.7)\n",
    "plt.xlabel('$z_1$', fontsize=16) \n",
    "plt.ylabel('$z_2$', fontsize=16)\n",
    "plt.title('Encodings of example images before training')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the autoencoder's reconstructions\n",
    "\n",
    "inx = np.random.choice(x_test.shape[0], 5, replace=False)\n",
    "reconstructed_example_images = cnn_autoencoder(x_test[inx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the autoencoder's reconstructions\n",
    "\n",
    "f, axs = plt.subplots(2, 5, figsize=(15, 4))\n",
    "for j in range(5):\n",
    "    axs[0, j].imshow(x_test[inx][j], cmap='binary')\n",
    "    axs[1, j].imshow(reconstructed_example_images[j].numpy().squeeze(), cmap='binary')\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Redesign the CNN autoencoder above using strides $\\ge2$ for the encoder, and design the decoder to be the reverse architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"vae\"></a>\n",
    "## The variational autoencoder\n",
    "\n",
    "We will now review the variational autoencoder (VAE) algorithm, its derivation from applying the principle of variational inference to a prescribed generative model, and its connection to standard autoencoders. The VAE was developed independently by [Kingma & Welling 2014](#Kingma14) and [Rezende et al 2014](#Rezende14) at about the same time. For a general reference on variational inference, see [Blei et al 2017](#Blei17).\n",
    "\n",
    "First, we describe the generative model behind the variational autoencoder. This is a **latent variable generative model**, where we introduce a latent (unobserved) random variable that is intended to capture hidden causes or explanations of the data. \n",
    "\n",
    "Furthermore, it is a **prescribed model** in the sense that we prescribe a noise model for the observations. Given a latent variable ${z}\\in\\mathbb{R}^l$, this determines a distribution over possible observations $p_\\theta({x} \\mid {z})$, with $x\\in\\mathbb{R}^D$. This class of generative model is also called a **likelihood-based model**, since the observations have an associated likelihood function. \n",
    "\n",
    "This is in contrast to an **implicit model**, where there is no likelihood function on the observations, and instead a realisation of the latent variable ${z}$ implicitly defines the observation ${x}$ (note that this is the case with normalising flows, although there we have the additional special structure that the generative model is invertible, and so the observation likelihood can still be explicitly computed). This is illustrated in the following figure.\n",
    "\n",
    "<img src=\"figures/generative_models.png\" alt=\"Generative models\" style=\"width: 400px;\"/>\n",
    "<center>Latent variable directed graphical models; (a) a prescribed generative model that defines a likelihood for each observation, and (b) an implicit generative model. The VAE is based on the prescribed model</center>\n",
    "\n",
    "The generative model under consideration can be written as $p_\\theta({z})p_\\theta({x} \\mid {z})$, where the conditional distribution $p_\\theta({x} \\mid {z})$ is defined by a neural network. The **marginal likelihood** (or **model evidence**) of an individual datapoint $x\\in\\mathbb{R}^D$ is given by\n",
    "\n",
    "$$\n",
    "p_\\theta({x}) = \\int p_\\theta({z})p_\\theta({x} \\mid {z}) d{z},\\label{evidence}\\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the model parameters. \n",
    "\n",
    "Note that under the usual i.i.d. assumption of our dataset $\\mathbf{x} = (x_i)_{i=1}^N$,  the full data log-likelihood is given by\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\mathbf{x}) = \\sum_{i=1}^N \\log p_\\theta(x_i).\n",
    "$$\n",
    "\n",
    "In the following we will continue to consider the likelihood of a single datapoint $x_i$, and drop the subscript $i$ for notational convenience.\n",
    "\n",
    "Now, we would like to choose the parameters $\\theta$ that maximise the marginal likelihood. Unfortunately, the integral above is intractable in general (as is the true posterior $p({z} \\mid {x})$), so we need to approximate it.\n",
    "\n",
    "The approximation that we will use is the **evidence lower bound** (ELBO), or **variational free energy**, which is a lower bound on the true marginal log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta({x}) &\\ge \\mathbb{E}_{q_\\phi({z}\\mid{x})} \\left[ \\log p_\\theta({x}\\mid{z})\\right] - D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right)\\label{elbo}\\tag{2}\\\\\n",
    "&=: \\mathcal{L}(\\theta, \\phi; {x}), \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $q_\\phi({z} \\mid {x})$ is a parameterised distribution of our choosing, and $D_{KL}$ denotes the Kullback-Leibler divergence, given by\n",
    "\n",
    "$$\n",
    "D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right) = \\int q_\\phi({z}\\mid{x})\\log\\left(\\frac{q_\\phi({z}\\mid{x})}{p_\\theta({z})}\\right)d{z}.\n",
    "$$\n",
    "\n",
    "The two terms in \\eqref{elbo} are often interpreted as a reconstruction loss term and a regularisation term:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) = \\underbrace{\\mathbb{E}_{q_\\phi({z}\\mid{x})} \\left[ \\log p_\\theta({x}\\mid{z})\\right]}_{\\text{reconstruction loss}} - \\underbrace{D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right)}_{\\text{regulariser}}.\n",
    "$$\n",
    "\n",
    "This decomposition shows the connection to autoencoders: if $q_\\phi({z}\\mid{x})$ is a parameterised neural network, then we can view this as the encoder and $p_\\theta({x}\\mid{z})$ as the decoder. Then the reconstruction loss is the probabilistic version of the autoencoder reconstruction loss (where we could consider $q_\\phi({z}\\mid{x})$ as a delta distribution). The second term regularises the encoder, and ensures it doesn't stray too far from the prior distribution $p_\\theta({z})$.\n",
    "\n",
    "The ELBO is also sometimes written as $\\mathcal{L}(\\theta, \\phi; {x}) = \\mathbb{E}_{q_\\phi({z}\\mid {x})}\\left[ -\\log q_\\phi({z}\\mid{x}) + \\log p_\\theta({x}, {z}) \\right]$.\n",
    "\n",
    "#### Derivation of the ELBO\n",
    "We will derive the evidence lower bound in two different ways. The first is a simple derivation using Jensen's inequality, and the second will help to shed some light on the optimal choice for the distribution $q_\\phi({z} \\mid {x})$.\n",
    "\n",
    "_Derivation 1_. The marginal log-likelihood is given by (cf. \\eqref{evidence})\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_\\theta({x}) &= \\log\\int p_\\theta({x}\\mid{z})p_\\theta({z})d{z}\\\\\n",
    "&= \\log \\int p_\\theta({x}\\mid{z})\\frac{p_\\theta({z})}{q_\\phi({z}\\mid{x})}q_\\phi({z}\\mid{x})d{z}\\\\\n",
    "&\\ge \\int\\log\\left(p_\\theta({x}\\mid {z})\\frac{p_\\theta({z})}{q_\\phi({z}\\mid {x})}\\right)q_\\phi({z}\\mid {x})d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log p_\\theta({x}\\mid {z})d{z} - \\int q_\\phi({z}\\mid {x}) \\log \\left(\\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z})}\\right)d{z}\\\\\n",
    "&= \\mathcal{L}(\\theta, \\phi; {x}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the third line in the above uses Jensen's inequality.\n",
    "\n",
    "_Derivation 2._ Let $q_\\phi({z}\\mid {x})$ be a parameterised family of distributions that we use to approximate the true posterior $p_\\theta({z}\\mid {x})$. We define the objective function that we wish to minimise as the KL-divergence $D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x}))$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta( {z}\\mid {x})) &= \\int q_\\phi({z}\\mid {x}) \\log\\left(\\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z}\\mid {x})}\\right) d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log\\left( \\frac{q_\\phi({z}\\mid {x}) p_\\theta({x})}{p_\\theta({x}\\mid{z})p_\\theta({z})}\\right) d{z}\\\\\n",
    "&= \\int q_\\phi({z}\\mid {x}) \\log p_\\theta({x}) d{z} - \\int q_\\phi({z}\\mid{x}) \\log p_\\theta({x}\\mid{z})d{z}\\\\\n",
    "& \\quad + \\int q_\\phi({z}\\mid {x})\\log\\left( \\frac{q_\\phi({z}\\mid {x})}{p_\\theta({z})}  \\right)d{z}\\\\\n",
    "&= \\log p_\\theta({x}) - \\mathcal{L}(\\theta, \\phi; {x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the KL-divergence is always non-negative, the above shows that $\\mathcal{L}(\\theta, \\phi; {x})$ is indeed a lower bound on the marginal log-likelihood $\\log p_\\theta({x})$. Furthermore, it shows that the gap in the bound is given by $D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x}))$. \n",
    "\n",
    "We see that to maximise the ELBO, the distribution $q_\\phi({z}\\mid {x})$ should approximate the true posterior $p_\\theta({z}\\mid {x}))$. And the better the approximation, the tighter the bound.\n",
    "\n",
    "The following figure illustrates that the variational autoencoder adds the variational approximation $q_\\phi({z}\\mid {x})$ to the intractable true posterior $p_\\theta({z}\\mid {x}))$. \n",
    "\n",
    "<img src=\"figures/generative_variational.png\" alt=\"The generative model and variational approximation to the true posterior\" style=\"width: 250px;\"/>\n",
    "<center>The prescribed generative model underlying the variational autoencoder, and the variational approximation $q_\\phi({z}\\mid {x})$ with variational parameters $\\phi$ depicted with dashed lines</center>\n",
    "\n",
    "Note that the generative model parameters $\\theta$ and the variational parameters $\\phi$ are **global variables**, whereas the latent random variable ${z}$ is a **local variable**. The variational parameters $\\phi$ are shared across all data points, and are not specific to individual data points, in contrast to traditional mean-field variational inference. This strategy is known as **amortized inference** ([Hoffman et al 2013](#Hoffman13)).\n",
    "\n",
    "#### The reparameterization trick\n",
    "We have now defined our ELBO objective function that we wish to maximise, which is a lower bound on the marginal log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) = \n",
    " \\mathbb{E}_{q_\\phi({z}\\mid {x})} \\left[ \\log p_\\theta({x}\\mid {z})\\right] - D_{KL}\\left( q_\\phi({z}\\mid {x}) || p_\\theta({z}) \\right)\n",
    "$$\n",
    "\n",
    "Note that we are able to evaluate the densities $q_\\phi({z}\\mid{x})$, $p_\\theta({x}\\mid{z})$, $p_\\theta({z})$ as well as sample from the approximating distribution $q_\\phi({z}\\mid {x})$, so the ELBO can be evaluated using Monte Carlo samples $\\{z^{(j)}\\}_{j=1}^L$, with $z^{(j)}$ sampled from $q_\\phi({z}\\mid{x})$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) \\approx \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) + \\log p_\\theta(z^{(j)}) − \\log q_\\phi(z^{(j)}\\mid x)\n",
    "$$\n",
    "\n",
    "The question remains how to optimise the ELBO with respect to the parameters $\\theta$ and $\\phi$. Note that taking gradients with respect to $\\phi$ is not straightforward, as the $z^{(j)}$ are samples. \n",
    "\n",
    "A typical **score-function estimator** ([Glynn 1990](#Glynn90), [Kleijnen & Rubinstein 1996](#Kleijnen96)) for the general type of problem of taking a gradient of an expectation of some function $f(z)$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}\\left[ f(z) \\right] &= \\mathbb{E}_{q_\\phi(z)} \\left[ f(z) \\nabla_\\phi \\log q_\\phi(z) \\right]\\\\\n",
    "&\\approx \\frac{1}{L} \\sum_{j=1}^L \\left[ f(z^{(j)}) \\nabla_\\phi \\log q_\\phi(z^{(j)}) \\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This estimator is also used in reinforcement learning for policy gradients, where it is often referred to as the REINFORCE algorithm ([Williams 1992](#Williams92)). However, this estimator typically has high variance ([Blei et al 2012](#Blei12)), and in our case we can do better, in particular since our function $f(z)$ $\\left(=\\log p_\\theta(x, z^{(j)})  − \\log q_\\phi(z^{(j)}\\mid x)\\right)$ is differentiable.\n",
    "\n",
    "The 'reparameterization trick' simply reparameterises the random latent variable $z\\sim q_\\phi(z\\mid x)$ using a differentiable transformation $g_\\phi(\\epsilon, x)$ of an auxiliary noise variable $\\epsilon\\sim p(\\epsilon)$:\n",
    "\n",
    "$$\n",
    "z \\sim q_\\phi(z\\mid x)\\\\\n",
    "z = g_\\phi(\\epsilon, x),\\quad \\epsilon\\sim p(\\epsilon)\n",
    "$$\n",
    "\n",
    "An example is if $q_\\phi(z\\mid x)$ a multivariate Gaussian distribution $N(\\mu_\\phi(x), \\Sigma_\\phi(x))$. Then we could reparameterise the distribution as \n",
    "\n",
    "$$\n",
    "p(\\epsilon)=N(\\mathbf{0},\\mathbf{I}),\\quad g_\\phi(\\epsilon,x)=\\mu_\\phi(x)+L_\\phi(x)\\epsilon,\\quad \\text{where }L_\\phi(x)L_\\phi(x)^T =\\Sigma_\\phi(x).\n",
    "$$\n",
    "\n",
    "The encoder network would then output the distribution parameters $\\mu_\\phi(x)$ and $L_\\phi(x)$, which are both fully differentiable with respect to $\\phi$. Note that the noise variable distribution does not depend on any parameters. \n",
    "\n",
    "If the transformation $g_\\phi(\\cdot, x):\\mathbb{R}^d\\mapsto\\mathbb{R}^l$ is invertible, this is nothing more than a change of variables, so the change of variables formula applies to give\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\phi(z\\mid x) &= \\left|\\, \\det J_{g_\\phi}(\\epsilon)\\,\\right|^{-1}\\cdot p(\\epsilon)\\\\\n",
    "&= \\left|\\, \\frac{d\\epsilon}{dz}\\,\\right|\\cdot p(\\epsilon)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This leads to the **pathwise estimator** ([Devroye 1996](#Devroye96)), which for a general function $f(z)$ and reparameterization $g_\\phi(\\epsilon)$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)}\\left[ f(z) \\right] &= \\nabla_\\phi \\int {q_\\phi(z)} f(z)dz\\\\\n",
    "&= \\nabla_\\phi \\int \\left|\\, \\frac{d\\epsilon}{dz}\\,\\right|\\cdot p(\\epsilon) f(g_\\phi(\\epsilon)) \\left|\\, \\frac{dz}{d\\epsilon}\\,\\right| d\\epsilon\\\\\n",
    "&= \\nabla_\\phi \\mathbb{E}_{p(\\epsilon)} \\left[ f(g_\\phi(\\epsilon)) \\right]\\\\\n",
    "&=  \\mathbb{E}_{p(\\epsilon)} \\left[ \\nabla_\\phi f(g_\\phi(\\epsilon)) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our case, the reparameterization $g_\\phi(\\epsilon, x)$ and noise variable $\\epsilon\\sim p(\\epsilon)$ leads to the **Stochastic Gradient Variational Bayes** (SGVB) estimator $\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) \\approx \\mathcal{L}(\\theta, \\phi; {x})$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x,z^{(j)})− \\log q_\\phi(z^{(j)}|x)\\label{sgvba}\\tag{3}\\\\\n",
    "\\text{where }z^{(j)} = g_\\phi(\\epsilon^{(j)}, x)\\quad\\text{and}\\quad\\epsilon^{(j)}\\sim p(\\epsilon)\n",
    "$$\n",
    "\n",
    "We can now use this estimator to approximate the ELBO objective, and take its gradients with respect to the parameters $(\\theta, \\phi)$ on minibatches of data to optimise them:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{x\\sim p_{data}}\\left[ \\log p_\\theta(x)\\right] &\\approx \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\log p_\\theta(x_i)\\\\\n",
    "&\\ge \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\mathcal{L}(\\theta, \\phi; {x_i})\\\\\n",
    "&\\approx \\frac{1}{|\\mathcal{D}_{train}|}\\sum_{i\\in\\mathcal{D}_{train}} \\hat{\\mathcal{L}}^A(\\theta,\\phi;x)\\\\\n",
    "&\\approx \\frac{1}{M} \\sum_{i\\in\\mathcal{D}_{m}}  \\hat{\\mathcal{L}}^A(\\theta,\\phi;x),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where as usual $\\mathcal{D}_m$ is a randomly sampled minibatch of training data points, and $M=|\\mathcal{D}_m|$. \n",
    "\n",
    "Note that we wish to maximise the above objective, so in practice we will take the negative of the quantity above to minimise.\n",
    "\n",
    "Finally, depending on the choice of prior $p_\\theta(z)$ and variational posterior $q_\\phi(z\\mid x)$, it may be possible to analytically evaluate the KL-divergence term $D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z))$. This is true, for example, in the case where both distributions are Gaussian. In this case, there is no need to approximate this term in the ELBO with Monte Carlo samples, and we can instead use the alternate version of the SGVB estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) − D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z)),\\label{sgvbb}\\tag{4}\n",
    "$$\n",
    "\n",
    "where as before $z^{(j)} = g_\\phi(\\epsilon^{(j)}, x)$ and $\\epsilon^{(j)}\\sim p(\\epsilon)$.\n",
    "\n",
    "It is worth noting that it is common in practice to take a single Monte Carlo sample ($L=1$) in the SGVB estimator \\eqref{sgvba} or \\eqref{sgvbb}, particularly for larger minibatch sizes.\n",
    "\n",
    "We summarise the Auto-Encoding Variational Bayes (variational autoencoder) algorithm as follows. The algorithm inputs are the encoder and decoder networks $q_\\phi(z \\mid x)$, $p_\\theta(x \\mid z)$, prior distribution $p_\\theta(z)$, minibatch size $M$ and number of Monte Carlo samples $L$.\n",
    "\n",
    "> Initialise $\\phi$, $\\theta$ randomly<br>\n",
    "> **while** not converged:<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; sample minibatch $\\mathcal{D}_m$ of $M$ data examples<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; sample $M\\times L$ noise variables $\\epsilon^{(i, j)}$ for each $x_i\\in\\mathcal{D}_m$ and $j=1,\\ldots,L$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; compute gradient $\\frac{1}{M}\\sum_{x_i\\in\\mathcal{D}_m}\\nabla_{\\phi, \\theta} \\hat{\\mathcal{L}}(\\theta,\\phi;x_i)$, where $\\hat{\\mathcal{L}}$ is $\\hat{\\mathcal{L}}^A$ or $\\hat{\\mathcal{L}}^B$<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; update parameters by applying gradient with a NN optimiser (e.g. SGD, Adam)\n",
    "\n",
    "<img src=\"figures/vae.png\" alt=\"VAE sketch\" style=\"width: 900px;\"/>\n",
    "<center>The variational encoder. The encoder/inference network defines the latent variable distribution via the reparameterization trick, the decoder/generative network reconstructs the original input by defining a likelihood $p_\\theta(x\\mid z)$. The variational posterior $q_\\phi(z \\mid x)$ is penalised for varying too much from the prior $p_\\theta(z)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"probabilisticlayers\"></a>\n",
    "## Probabilistic layers\n",
    "\n",
    "In this section we will develop a full implementation of the variational autoencoder. This implementation will make use of probabilistic layers from the TensorFlow Probability library, which can be found in the [layers module](https://www.tensorflow.org/probability/api_docs/python/tfp/layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Frey Face dataset\n",
    "We will use the [Frey Face](https://cs.nyu.edu/~roweis/data.html) dataset to demonstrate the VAE, as in the original paper by [Kingma & Welling](#Kingma14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "faces_data = np.load('./data/frey_faces.npy')\n",
    "faces_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val = train_test_split(faces_data, test_size=0.1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View a sample of the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 4, 10\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "inx = np.random.choice(x_train.shape[0], n_rows*n_cols, replace=False)\n",
    "fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "\n",
    "for n, image in enumerate(x_train[inx]):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Datasets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(x_val)\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "\n",
    "def process(img):\n",
    "    return tf.cast(img, tf.float32) / 255.\n",
    "\n",
    "train_dataset = train_dataset.map(process).shuffle(500).batch(100).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(process).shuffle(200).batch(20).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative model\n",
    "Next, we define the prior and decoder that define the generative model $p_\\theta({z})p_\\theta({x} \\mid {z})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior\n",
    "\n",
    "latent_dim = 2\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1.), reinterpreted_batch_ndims=1)\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder, we follow [Kingma & Welling](#Kingma14) and use a Gaussian likelihood, but constrain the mean to $[0, 1]$. \n",
    "\n",
    "It is worth mentioning that it is also common practice to use an independent Bernoulli likelihood per pixel in the decoder for similar image datasets ([Kingma & Welling](#Kingma14) uses this for MNIST), despite this being incorrect as the data is not binary. This is studied in [Loaiza-Ganem & Cunningham 2019](#Loaiza-Ganem19) where the continuous Bernoulli distribution is introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct probabilistic encoders and decoders with probabilistic layers. These are layers that output a `tfd.Distribution` object instead of a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "\n",
    "img_h, img_w = 28, 20\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(200, activation='relu', input_shape=(latent_dim,)),\n",
    "    Dense(img_h * img_w * 2),\n",
    "    Reshape((img_h, img_w, 2)),\n",
    "    tfpl.DistributionLambda(\n",
    "    lambda t: tfd.Independent(tfd.Normal(loc=tf.math.sigmoid(t[..., 0]),\n",
    "                                         scale=tf.math.exp(t[..., 1])), reinterpreted_batch_ndims=2))\n",
    "], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference model\n",
    "We now define the encoder, or inference model $q_\\phi(z\\mid x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "encoder = Sequential([\n",
    "    Flatten(input_shape=(img_h, img_w)),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(tfpl.IndependentNormal.params_size(latent_dim)),\n",
    "    tfpl.IndependentNormal(latent_dim)\n",
    "], name='encoder')\n",
    "\n",
    "encoder(tf.random.uniform((16, img_h, img_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the encoder and decoder\n",
    "We now compile and fit the encoder and decoder networks. Since the prior and approximate posterior are both Gaussian, we will use the second form of the SGVB estimator (setting $L=1$):\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^B(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta(x\\mid z^{(j)}) − D_{KL}(q_\\phi(z\\mid x) || p_\\theta(z)),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-divergence between two `Distribution` objects can be computed by the `tfd.kl_divergence` function, where a closed form expression is registered in the TFP library. See [the documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/kl_divergence) for a list of such registered distribution pairs.\n",
    "\n",
    "To implement the VAE, we will use [model subclassing](https://www.tensorflow.org/guide/keras/c…), specifically to override the default behaviour that happens when we call the `.fit()` method. We can [customise this behaviour](https://keras.io/guides/customizing_what_happens_in_fit/) by overriding the `train_step` method of the `Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE Model object\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "loss_metric = Mean(name='loss')\n",
    "kl_metric = Mean(name='kl')\n",
    "nll_metric = Mean(name='nll')\n",
    "\n",
    "class VAE(Model):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, prior, **kwargs):\n",
    "        super(VAE, self).__init__(inputs=encoder.inputs, \n",
    "                                  outputs=decoder(encoder.outputs), \n",
    "                                  **kwargs)\n",
    "        self.prior = prior\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def _get_losses(self, data):\n",
    "        approx_posterior = self.encoder(data)\n",
    "        kl_loss = tfd.kl_divergence(approx_posterior, self.prior)\n",
    "        \n",
    "        approx_posterior_sample = approx_posterior.sample()\n",
    "        x_pred = self.decoder(approx_posterior_sample)\n",
    "        \n",
    "        nll_loss = -x_pred.log_prob(data)\n",
    "        \n",
    "        loss = kl_loss + nll_loss\n",
    "        loss, kl_loss, nll_loss = tf.reduce_mean(loss), tf.reduce_mean(kl_loss), tf.reduce_mean(nll_loss)\n",
    "        return loss, kl_loss, nll_loss\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        kl_metric.update_state(kl_loss)\n",
    "        nll_metric.update_state(nll_loss)\n",
    "        loss_metric.update_state(loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        loss, kl_loss, nll_loss = self._get_losses(data)\n",
    "        kl_metric.update_state(kl_loss)\n",
    "        nll_metric.update_state(nll_loss)\n",
    "        loss_metric.update_state(loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_metric, kl_metric, nll_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "\n",
    "vae = VAE(encoder, decoder, prior, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the Model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "history = vae.fit(train_dataset, validation_data=val_dataset, epochs=200, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.plot(history.history['kl'], label='train')\n",
    "plt.plot(history.history['val_kl'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"KL loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.plot(history.history['nll'], label='train')\n",
    "plt.plot(history.history['val_nll'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"NLL loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on the validation set\n",
    "\n",
    "vae.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View samples and reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the generative model\n",
    "\n",
    "samples = vae.decoder(vae.prior.sample(40)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View the samples\n",
    "\n",
    "n_rows, n_cols = 4, 10\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "\n",
    "for n, image in enumerate(samples):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstructions from the validation dataset\n",
    "\n",
    "for images in val_dataset.take(1):\n",
    "    reconstructions = vae(images).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some reconstructions from the test dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "outer = gridspec.GridSpec(1, 2, hspace=0.2)\n",
    "n_rows, n_cols = 4, 5\n",
    "fig.text(0.23, 0.9, \"Test dataset images\", fontsize=14)\n",
    "fig.text(0.66, 0.9, \"VAE reconstructions\", fontsize=14)\n",
    "for i in range(2):\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(n_rows, n_cols,\n",
    "                    subplot_spec=outer[i], wspace=0., hspace=0.)\n",
    "    display_images = [images, reconstructions][i]\n",
    "    for j in range(n_rows * n_cols):\n",
    "        row = j // n_cols\n",
    "        col = j % n_cols\n",
    "        ax = plt.Subplot(fig, inner[j])\n",
    "        ax.imshow(display_images[j].numpy(), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        fig.add_subplot(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Rewrite the loss function above so the KL divergence is approximated with Monte Carlo samples, so the SGVB estimator $\\hat{\\mathcal{L}}^A(\\theta,\\phi;x)$ is used instead. Also try modifying the posterior to be a full covariance Gaussian using the probabilistic layer `MultivariateNormalTriL`. Does this improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kldivlayer\"></a>\n",
    "## The KLDivergenceAddLoss layer\n",
    "In the above implementation we had to customise the training step to account for the custom loss function. An alternative approach is to add the KL term within the model itself, which can be done using the `KLDivergenceAddLoss` layer from the `tfp.layers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will work with the binarized MNIST dataset originally used in [Salakhutdinov & Murray 2008](#Salakhutdinov08).\n",
    "\n",
    "This dataset is available from the [TensorFlow Datasets](https://www.tensorflow.org/datasets) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a binarised version of MNIST\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = tfds.load('binarized_mnist', \n",
    "                                                     split=['train', 'validation', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "\n",
    "def get_image(d):\n",
    "    return tf.cast(d['image'], tf.float32)\n",
    "\n",
    "def repeat_image(image):\n",
    "    return image, image\n",
    "\n",
    "train_dataset = train_dataset.map(get_image).map(repeat_image).shuffle(500)\n",
    "val_dataset = val_dataset.map(get_image).map(repeat_image).shuffle(500)\n",
    "test_dataset = test_dataset.map(get_image).map(repeat_image).shuffle(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some examples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "\n",
    "for n, image in enumerate(train_dataset.take(n_rows * n_cols)):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image[0], cmap='plasma')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and prefetch the datasets\n",
    "\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `KLDivergenceAddLoss` layer\n",
    "This is a 'pass-through' layer that can be included in the encoder. It expects a distribution to be output from the previous layer (i.e. a probabilistic layer). It does not alter its inputs in any way, but adds the KL-divergence to the encoder losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior\n",
    "\n",
    "latent_size = 2\n",
    "prior = tfd.MultivariateNormalDiag(loc=tf.zeros(latent_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(8, (5, 5), activation='relu', padding=\"SAME\", input_shape=(28, 28, 1)),\n",
    "    MaxPool2D((2, 2)),\n",
    "    Conv2D(8, (5, 5), activation='relu', padding=\"SAME\"),\n",
    "    MaxPool2D((2, 2)),\n",
    "    Conv2D(8, (5, 5), activation='relu', padding=\"SAME\"),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "    Dense(tfpl.IndependentNormal.params_size(latent_size)),\n",
    "    tfpl.IndependentNormal(latent_size),\n",
    "    tfpl.KLDivergenceAddLoss(prior, use_exact_kl=False,\n",
    "                             weight=None,\n",
    "                             test_points_fn=lambda q: q.sample(10),\n",
    "                             test_points_reduce_axis=None \n",
    "                            )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the losses\n",
    "\n",
    "for image_batch in train_dataset.take(1):\n",
    "    encoder(image_batch[0])\n",
    "encoder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, UpSampling2D, Conv2DTranspose\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(latent_size,)),\n",
    "    Reshape((4, 4, 4)),\n",
    "    Conv2D(16, (5, 5), padding=\"SAME\", activation='relu'),\n",
    "    Conv2DTranspose(8, (5, 5), strides=2, activation='relu', output_padding=1),\n",
    "    Conv2D(8, (5, 5), padding=\"SAME\", activation='relu'),\n",
    "    Conv2DTranspose(8, (3, 3), activation='relu'),\n",
    "    Conv2D(8, (5, 5), padding=\"SAME\", activation='relu'),\n",
    "    Conv2DTranspose(8, (5, 5), padding=\"SAME\", strides=2, activation='relu', output_padding=1),\n",
    "    Conv2D(1, 1, activation=None),\n",
    "    Flatten(),\n",
    "    tfpl.IndependentBernoulli((28, 28, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build end-to-end architecture\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "vae = Model(inputs=encoder.input, outputs=decoder(encoder.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reconstruction error loss\n",
    "\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "\n",
    "csv_logger = CSVLogger(\"logs.csv\")\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=reconstruction_loss, metrics=['accuracy'])\n",
    "history = vae.fit(train_dataset, validation_data=val_dataset, epochs=50, \n",
    "                  callbacks=[csv_logger, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on the test set\n",
    "\n",
    "vae.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the saved file\n",
    "\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the saved CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv(\"logs.csv\")\n",
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View samples and encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from the generative model\n",
    "\n",
    "samples = decoder(prior.sample(40)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View the samples\n",
    "\n",
    "n_rows, n_cols = 4, 10\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 7))\n",
    "fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "\n",
    "for n, image in enumerate(samples):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate within the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate within the latent space\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_time_steps = 200\n",
    "t = np.arange(num_time_steps) * 0.01\n",
    "w1, a1 = 0.5, 0.5\n",
    "w2, a2 = 1.3, -0.2\n",
    "z1 = 2 * np.sin(2*np.pi*w1*t + a1)\n",
    "z2 = 2 * np.sin(2*np.pi*w2*t + a2)\n",
    "z = np.transpose(np.stack((z1, z2)))\n",
    "decoded = decoder(z).mean().numpy()[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the animation\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "axes[0].set(title=\"Generated image\")\n",
    "axes[1].set(title=\"Latent variable\", xlim=(-3, 3), ylim=(-3, 3), aspect='equal')\n",
    "scat = axes[1].scatter(z[0][0], z[0][1])\n",
    "im = axes[0].imshow(np.squeeze(decoded[0]))\n",
    "\n",
    "def animate(i):\n",
    "    im.set_array(np.squeeze(decoded[i]))\n",
    "    scat.set_offsets(z[i])\n",
    "    \n",
    "anim = FuncAnimation(\n",
    "    fig, animate, interval=100, frames=len(t)-1)\n",
    "\n",
    "anim.save(\"latent_interpolation.mp4\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"latent_interpolation.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "! rm latent_interpolation.mp4\n",
    "! rm logs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Try adjusting the `weight` option in the KL-divergence layer. Re-train the model with higher (and lower) weighting. What affect does this have on the data encodings, the samples and the reconstructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"iaf\"></a>\n",
    "## Improved posterior approximation with IAF\n",
    "\n",
    "One limitation of the standard formulation of the variational autoencoder that we have implemented above is the lack of flexibility of the variational posterior. Recall that the ELBO is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) = \\mathbb{E}_{q_\\phi({z}\\mid{x})} \\left[ \\log p_\\theta({x}\\mid{z})\\right] - D_{KL}\\left( q_\\phi({z}\\mid{x}) || p_\\theta({z}) \\right),\n",
    "$$\n",
    "\n",
    "and we have\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(x) = \\mathcal{L}(\\theta, \\phi; {x}) + D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x})).\n",
    "$$\n",
    "\n",
    "Therefore, the closer $D_{KL}(q_\\phi({z}\\mid {x}) || p_\\theta({z}\\mid {x}))$ is to $0$, the closer $\\mathcal{L}(\\theta, \\phi; {x})$ will be to $\\log p_\\theta(x)$, and the better an approximation the optimization objective $\\mathcal{L}(\\theta, \\phi; {x})$ will be to the true objective $\\log p_\\theta(x)$.\n",
    "\n",
    "One approach to improve the flexibility of the posterior approximation was first proposed by [Rezende & Mohamed 2015](#Rezende15), that makes use of normalising flows to construct more complex posterior distributions. [Kingma et al 2016](#Kingma16) builds on this work, specifically using the Inverse Autoregressive Flow (IAF).\n",
    "\n",
    "#### IAF and MAF\n",
    "Recall that the IAF is the inverse of the masked autoregressive flow (MAF), which we quickly review. This is an autoregressive flow which models the conditional densities as Gaussian:\n",
    "\n",
    "$$\n",
    "p(x_i \\mid x_{1:i-1}) = \\mathcal{N}(x_i \\mid \\mu_i, \\sigma_i^2),\\quad i=1,\\ldots, D,\n",
    "$$\n",
    "\n",
    "where the input $\\mathbf{x}\\in\\mathbb{R}^D$. The mean and standard deviation parameters are output by MADE networks, which satisfy the autoregressive property:\n",
    "\n",
    "$$\n",
    "\\left.\n",
    "\\begin{array}{rcl}\n",
    "\\mu_i \\hspace{-1ex}&=& \\hspace{-1ex}f_{\\mu_i}(x_{1:i-1})\\\\\n",
    "\\sigma_i \\hspace{-1ex}&=& \\hspace{-1ex}f_{\\sigma_i}(x_{1:i-1})\n",
    "\\end{array}\n",
    "\\quad\n",
    "\\right\\}\n",
    "\\quad\n",
    "i=1,\\ldots D.\n",
    "$$\n",
    "\n",
    "In practice, the output $f_{\\sigma_i}(x_{1:i-1})$ is often taken to be the log-standard deviation. The forward transformation is then given by\n",
    "\n",
    "$$\n",
    "x_i = f_{\\sigma_i}(x_{1:i-1})z_i + f_{\\mu_i}(x_{1:i-1}),\\qquad i=1,\\ldots, D,\\label{maf}\\tag{5}\n",
    "$$\n",
    "\n",
    "where $z_i\\sim \\mathcal{N}(0, 1)$. We denote the MAF flow \\eqref{maf} as the transformation $\\mathbf{x} = f(\\mathbf{z})$, with the base distribution $\\mathbf{z}\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. \n",
    "\n",
    "In the forward transformation, the $x_i$ are sequentially sampled, which is slow for $D\\gg 1$. The inverse flow however is parallelisable, and so is fast:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\frac{\\mathbf{x} - f_{\\mathbf{\\mu}}(\\mathbf{x})}{f_{\\mathbf{\\sigma}}(\\mathbf{x})}.\\label{iaf}\\tag{6}\n",
    "$$\n",
    "\n",
    "where $f_{\\mathbf{\\mu}}(\\mathbf{x}):=\\left(f_{\\mu_i}(x_{1:i-1})\\right)_{i\\in\\{1,\\ldots,D\\}}$ and $f_{\\mathbf{\\sigma}}(\\mathbf{x}):=\\left(f_{\\sigma_i}(x_{1:i-1})\\right)_{i\\in\\{1,\\ldots,D\\}}$. We denote the IAF flow \\eqref{iaf} as the transformation $\\mathbf{z} = f^{-1}\\mathbf{x}$.\n",
    "\n",
    "#### Variational inference with IAF\n",
    "Normalising flows are designed to model complex distributions through layers of invertible transformations. This is precisely what we need to improve the posterior approximation: the encoder (or inference network) parameterises a simple distribution such as a diagonal Gaussian. This distribution can then be passed through a normalising flow to increase the flexibility of the posterior approximation. \n",
    "\n",
    "[Kingma et al 2016](#Kingma16) propose using inverse autoregressive flow (IAF) for this purpose. With this choice, the forward pass is fast to compute, and the autoregressive transformation is powerful enough to scale to high dimensions.\n",
    "\n",
    "<img src=\"figures/vae-iaf.png\" alt=\"VAE-IAF sketch\" style=\"width: 500px;\"/>\n",
    "<center>Using inverse autoregressive flow for improved posterior approximation. The increased flexibility allows the variational posterior to closer match the true posterior $p_\\theta(z_T\\mid x)$</center>\n",
    "\n",
    "Several steps of IAF can composed for a more expressive transformation. If the output of the encoder is the distribution $q_\\phi(z_0 \\mid x)$, then we apply $T$ steps of IAF flow:\n",
    "\n",
    "$$\n",
    "z_i = f^{(i)}(z_{i-1}),\\qquad i=1,\\ldots,T.\n",
    "$$\n",
    "\n",
    "This defines a final posterior distribution $q_\\phi(z_T\\mid x)$ (where we absorb parameters of the IAF into $\\phi$), from which we sample and then pass through the decoder to produce the likelihood $p_\\theta(x\\mid z_T)$.\n",
    "\n",
    "Recall that the transformed log-density under IAF from $z_{i-1}$ to $z_i$ is given by\n",
    "\n",
    "$$\n",
    "\\log q_\\phi(z_i\\mid x) = \\log q_\\phi(z_{i-1}\\mid x) + \\sum_{k=1}^l \\log \\sigma^{(i)}_k,\n",
    "$$\n",
    "\n",
    "where $l$ is the dimension of the latent space, and we abuse notation by using $q_\\phi$ to denote the density at each stage of the latent variable transformation. After $T$ steps of IAF, the log-density transformation is\n",
    "\n",
    "$$\n",
    "\\log q_\\phi(z_T\\mid x) = \\log q_\\phi(z_0\\mid x) + \\sum_{i=1}^T\\sum_{k=1}^l \\log \\sigma^{(i)}_k.\n",
    "$$\n",
    "\n",
    "Substituting the above expression into the ELBO objective gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi; {x}) &= \\mathbb{E}_{q_\\phi(z_0\\mid {x}),\\,z_T=f^{-1}(z_0)}\\left[ \\log p_\\theta(x \\mid {z}_T) + \\log p_\\theta(z_T) -\\log q_\\phi({z}_T\\mid{x}) \\right]\\\\\n",
    "&= \\mathbb{E}_{q_\\phi(z_0\\mid {x}),\\,z_T=f^{-1}(z_0)}\\left[ \\log p_\\theta(x \\mid {z}_T) + \\log p_\\theta(z_T) - \\log q_\\phi(z_0\\mid x) - \\sum_{i=1}^T\\sum_{k=1}^l \\log \\sigma^{(i)}_k \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As before, this objective can be estimated using the SGVB estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}^A(\\theta,\\phi;x) := \\frac{1}{L} \\sum_{j=1}^L \\log p_\\theta( z_T^{(j)}) + \\log p_\\theta(x\\mid z_T^{(j)}) − \\log q_\\phi(z_0^{(j)}|x) - \\sum_{i=1}^T\\sum_{k=1}^l \\log \\sigma^{(i)}_k \\\\\n",
    "\\text{where }z_0^{(j)} = g_\\phi(\\epsilon^{(j)}, x),\\quad\\epsilon^{(j)}\\sim p(\\epsilon)\\quad\\text{and}\\quad z_T=f^{-1}(z_0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Benou16\"></a>\n",
    "* Benou, A., Veksler, R., Friedman, A. & Raviv, T.R. (2016), \"De-noising of contrast-enhanced MRI sequences by an ensemble of expert deep neural networks\", in *International Workshop on Deep Learning in Medical Image Analysis*, Athens, Greece, 21 October 2016.\n",
    "<a class=\"anchor\" id=\"Blei17\"></a>\n",
    "* Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. (2017), \"Variational Inference: A Review for Statisticians\", *Journal of the American Statistical Association*, **112** (518), 859-877.\n",
    "<a class=\"anchor\" id=\"Blei12\"></a>\n",
    "* Blei, D. M., Jordan, M. I. & Paisley, J. W. (2012), \"Variational bayesian inference with stochastic search\", in *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 1367–1374.\n",
    "<a class=\"anchor\" id=\"Devroye96\"></a>\n",
    "* Devroye, L. (1996), \"Random Variate Generation in One Line of Code\", in *Proceedings of the 28th Conference on Winter Simulation*, Coronado, California, USA, 265-272.\n",
    "<a class=\"anchor\" id=\"Dhariwal20\"></a>\n",
    "* Dhariwal, P., Heewoo, J., Payne, C., Kim, J. W., Radford, A. & Sutskever, I. (2020), \"Jukebox: A Generative Model for Music\", arXiv preprint, abs/2005.00341.\n",
    "<a class=\"anchor\" id=\"Glynn90\"></a>\n",
    "* Glynn, P. W. (1990), \"Likelihood Ratio Gradient Estimation for Stochastic Systems\", *Communications of the ACM*, **33** (10), 75-84.\n",
    "<a class=\"anchor\" id=\"Hoffman13\"></a>\n",
    "* Hoffman, M. D., Blei, D. M., Wang, C, & Paisley, J. (2013), \"Stochastic Variational Inference\", *Journal of Machine Learning Research*, **14** (1), 1532-4435.\n",
    "<a class=\"anchor\" id=\"Jin18\"></a>\n",
    "* Jin, W., Barzilay, R. & Jaakkola, T. (2018), \"Junction Tree Variational Autoencoder for Molecular Graph Generation\", in *Proceedings of Machine Learning Research*, **80**, 2323-2332.\n",
    "<a class=\"anchor\" id=\"Kingma14\"></a>\n",
    "* Kingma, D. P. & Welling, M., \"Auto-Encoding Variational Bayes\" (2014), in *Proceedings of the 2nd International Conference on Learning Representations (ICLR)*, Banff, AB, Canada, April 14-16, 2014.\n",
    "<a class=\"anchor\" id=\"Kingma16\"></a>\n",
    "* Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I. & Welling, M. (2016), \"Improved Variational Inference with Inverse Autoregressive Flow\", *Advances in Neural Information Processing Systems*, **29**, 4743--4751.\n",
    "<a class=\"anchor\" id=\"Kleijnen96\"></a>\n",
    "* Kleijnen, J. P. C. & Rubinstein, R. Y. (1996), \"Optimization and sensitivity analysis of computer simulation models by the score function method\", *European Journal of Operational Research*, **88**, 413-427 .\n",
    "<a class=\"anchor\" id=\"Kramer91\"></a>\n",
    "* Kramer, M. A. (1991), \"Nonlinear principal component analysis using autoassociative neural networks\", *AIChE Journal*, **37** (2), 233–243. \n",
    "<a class=\"anchor\" id=\"Loaiza-Ganem19\"></a>\n",
    "* Loaiza-Ganem, G. & Cunningham, J. P. (2019), \"The continuous Bernoulli: fixing a pervasive error in variational autoencoders\", *Advances in Neural Information Processing Systems* **32**, 13287--13297.\n",
    "<a class=\"anchor\" id=\"Rezende14\"></a>\n",
    "* Rezende, D. J., Mohamed, S. & Wierstra, D. (2014), \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", in *Proceedings of the 31st International Conference on Machine Learning, PMLR*, **32** (2), 1278-1286.\n",
    "<a class=\"anchor\" id=\"Rezende15\"></a>\n",
    "* Rezende, D. & Mohamed, S. (2015), \"Variational Inference with Normalizing Flows\", in *Proceedings of Machine Learning Research*, **37**, 1530-1538.\n",
    "<a class=\"anchor\" id=\"Salakhutdinov08\"></a>\n",
    "* Salakhutdinov, R. and Murray, I. (2008), \"On the quantitative analysis of deep belief networks\", in *Proceedings of the 25th international conference on Machine learning*, 892-879.\n",
    "<a class=\"anchor\" id=\"Vincent10\"></a>\n",
    "* Vincent, P. & Larochelle, H. (2010), \"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\", *Journal of Machine Learning Research*, **11**, 3371–3408.\n",
    "<a class=\"anchor\" id=\"Williams92\"></a>\n",
    "* Williams, R. J. (1992), \"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\", *Machine Learning*, **8** (3-4), 229-256.\n",
    "<a class=\"anchor\" id=\"Xu18\"></a>\n",
    "* Xu, H., Chen, W., Zhao, N., Li, Z., Bu, J., Li, Z., Liu, Y., Zhao, Y., Pei, D., Feng, Y., Chen, J. J., Wanb, Z. & Qiao, H. (2018), \"Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications\", *Proceedings of the 2018 World Wide Web Conference*, Palais des congrès de Lyon, Lyon, France, 23-27 April 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
