{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34567ec",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 8: Normalising flows I: Autoregressive flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d7e39",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement and train an autoregressive normalising flow density estimation model.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell to import all required packages. \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e23ce4",
   "metadata": {},
   "source": [
    "<img src=\"figures/mlds.png\" title=\"MLDS\" style=\"width: 600px;\"/>  \n",
    "  \n",
    "#### The MLDS dataset\n",
    "\n",
    "In this assignment, you will use a synthetic point cloud dataset to train an autoregressive normalising flow model. The dataset consists of 5,376 points in two-dimensions.\n",
    "\n",
    "Your goal is to build and train a masked autoregressive flow as a density estimation model for this point cloud dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a641674",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a numpy array\n",
    "\n",
    "dataset = np.load(Path(\"./data/mlds.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20789abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], alpha=0.1)\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba245d60",
   "metadata": {},
   "source": [
    "Complete the following `get_dataset` function to load and prepare the data into a `tf.data.Dataset` object.\n",
    "\n",
    "* The function takes `data_array`, `shuffle_buffer_size` and `batch_size` as arguments\n",
    "  * `data_array` is a numpy array of shape `(num_examples, 2)`\n",
    "  * `shuffle_buffer_size` and `batch_size` are integers\n",
    "* You should load the array into a Dataset object and process as follows:\n",
    "  * Shuffle the Dataset with a buffer size of `shuffle_buffer_size`\n",
    "  * Batch the Dataset with `batch_size`\n",
    "  * Prefetch the Dataset with `tf.data.AUTOTUNE`\n",
    "* Your function should then return the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_dataset(data_array, shuffle_buffer_size, batch_size):\n",
    "    \"\"\"\n",
    "    This function should create a Dataset object as described above.\n",
    "    The function should then return the Dataset object.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data_array)\n",
    "    ds = ds.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7247681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to create the Datasets\n",
    "\n",
    "train_ds = get_dataset(dataset, shuffle_buffer_size=500, batch_size=512)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0883e",
   "metadata": {},
   "source": [
    "#### Build the normalising flow model\n",
    "We will use the Masked Autoregressive Flow (MAF) to model the MLDS dataset. Recall that MAF is an implementation of the Gaussian-conditional autoregressive flow:\n",
    "\n",
    "$$\n",
    "p(x_i \\mid x_{1:i-1}) = N(x_i \\mid \\mu_i, \\sigma_i^2),\\qquad i=1,\\ldots,D\n",
    "$$\n",
    "\n",
    "$$\\left.\n",
    "\\begin{array}{rcl}\n",
    "\\mu_i & = & f_{\\mu_i}(x_{1:i-1})\\\\\n",
    "\\sigma_i & = & f_{\\sigma_i}(x_{1:i-1})\n",
    "\\end{array}\\right\\}\\qquad i = 1,\\ldots,D\n",
    "$$\n",
    "\n",
    "where $D$ is the dimension of the data example $x$. The forward and inverse transformations are given as follows:\n",
    "\n",
    "$$\n",
    "\\text{Forward transformation:} \\quad x_i = f_{\\sigma_i}(x_{1:i-1}) z_i + f_{\\mu_i}(x_{1:i-1})\n",
    "$$</br>\n",
    "$$\n",
    "\\text{Inverse transformation:} \\quad z_i = \\frac{x_i - f_{\\mu_i}(x_{1:i-1})}{f_{\\sigma_i}(x_{1:i-1})}\n",
    "$$\n",
    "\n",
    "where $z_i$ is distributed according to a base distribution, typically a standard Normal distribution.\n",
    "\n",
    "In the MAF, the functions $f_{\\mu_i}$ and $f_{\\sigma_i}$ $(i=1,\\ldots,D)$ are modelled with MADE networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264270d",
   "metadata": {},
   "source": [
    "We will start by defining a suitable base distribution. You should complete the following `base_distribution` function, according to the following specification:\n",
    "\n",
    "* The function should define a `tfd.Distribution` object\n",
    "* The distribution should be a 2-D Gaussian with zero mean and covariance matrix $\\sigma^2I$, where $\\sigma=2$\n",
    "* The `batch_shape` of the Distribution object should be empty, and the `event_shape` should be `[2]`\n",
    "* The function should then return the `tfd.Distribution` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def base_distribution():\n",
    "    \"\"\"\n",
    "    This function should define a tfd.Distribution object as described above.\n",
    "    The function should then return the Distribution object.\n",
    "    \"\"\"\n",
    "    return tfd.MultivariateNormalDiag(loc=[0., 0.], scale_diag=[2., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d449989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to define the base distribution\n",
    "\n",
    "base = base_distribution()\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5eb7f4",
   "metadata": {},
   "source": [
    "You should now complete the following function `maf_flow` to define the `TransformedDistribution` object for the MAF normalising flow.\n",
    "\n",
    "* The function takes `base_dist`, `num_layers` and `hidden_units` as arguments\n",
    "* `base_dist` is a `tfd.Distribution` object to be used as the base distribution, with empty `batch_shape` and `event_shape=[2]`\n",
    "* `hidden_units` is a list of integers\n",
    "* `num_layers` is an integer to define the number of steps of flow\n",
    "* Each step of flow should consist of a MAF bijector, where the autoregressive transformation is defined by a MADE network, followed by a permutation of the input dimensions\n",
    "  * The MADE network should have number and width of hidden layers defined by `hidden_units`\n",
    "  * The MADE network should use a ReLU activation, and output a mean and log-scale parameter for each input dimension\n",
    "  * After each MAF bijector, the order of the 2 event dimensions should be permuted\n",
    "* This sequence of bijectors should be used to define a `TransformedDistribution` to transform `base_dist`\n",
    "* The function should then return the `TransformedDistribution` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f59748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def maf_flow(base_dist, num_layers, hidden_units):\n",
    "    \"\"\"\n",
    "    This function should define a tfd.TransformedDistribution object as described above.\n",
    "    The function should then return the TransformedDistribution object.\n",
    "    \"\"\"\n",
    "    def get_maf_bijector():\n",
    "        made = tfb.AutoregressiveNetwork(params=2, hidden_units=hidden_units, activation='relu')\n",
    "        return tfb.MaskedAutoregressiveFlow(made)\n",
    "\n",
    "    maf_bijs = []\n",
    "    for _ in range(num_layers):\n",
    "        maf_bijs.append(get_maf_bijector())\n",
    "        maf_bijs.append(tfb.Permute([1, 0]))\n",
    "\n",
    "    maf_bijector = tfb.Chain(list(reversed(maf_bijs)))\n",
    "    maf = tfd.TransformedDistribution(base_dist, maf_bijector)\n",
    "    return maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MAF TransformedDistribution\n",
    "    \n",
    "maf = maf_flow(base, num_layers=6, hidden_units=[128, 128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea6dee",
   "metadata": {},
   "source": [
    "#### Train the MAF model\n",
    "\n",
    "We will use a custom training loop to train the normalising flow. You should now complete the following `train_model` function to perform the custom training loop. \n",
    "\n",
    "* The function takes `maf_model`, `opt`, `training_dataset`, `epochs` and `loss_metric` as arguments\n",
    "* The custom training loop should consist of an outer loop for the epochs, that runs for `epochs` number of times\n",
    "* At the start of each epoch, the metric state should be reset using the `reset_state` method\n",
    "* Within each epoch, the function should loop over `training_dataset` to pull batches of data\n",
    "* For each batch, it should compute the loss and gradients and use the `opt` optimizer to update the model parameters\n",
    "  * For each batch, the metric should be updated, using the `update_state` method\n",
    "* The average loss over each epoch should each be stored in a list of floats\n",
    "  * The average loss can be retrieved from the metrics at the end of the epoch using the `result` method\n",
    "* The function should return a list of floats for the average loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model(maf_model, opt, training_dataset, epochs, loss_metric):\n",
    "    \"\"\"\n",
    "    This function should run the custom training loop as described above.\n",
    "    The function should return a list with the average loss per epoch.\n",
    "    \"\"\"\n",
    "    epoch_losses = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            nll = -tf.reduce_mean(maf_model.log_prob(batch)) \n",
    "            grads = tape.gradient(nll, maf_model.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, maf_model.trainable_variables))\n",
    "        return nll\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_metric.reset_state()\n",
    "        \n",
    "        for train_batch in training_dataset:\n",
    "            loss = train_step(train_batch)\n",
    "            loss_metric.update_state(loss)\n",
    "            \n",
    "        avg_epoch_loss = float(loss_metric.result().numpy())\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: loss - {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric and optimizer to be used in the above function\n",
    "\n",
    "mean_metric = tf.keras.metrics.Mean()\n",
    "rmsprop = tf.keras.optimizers.RMSprop(rho=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768787c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the custom training loop\n",
    "\n",
    "losses = train_model(maf, rmsprop, train_ds, 800, mean_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe164a",
   "metadata": {},
   "source": [
    "_Note: training normalising flow models can sometimes be numerically unstable if the NLL calculation happens to be very high for a given batch of data and model parameter settings. In this case you might see the loss go to NaN. If that happens, re-initialise the model and run the training again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef01696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(2.0, 5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0940b",
   "metadata": {},
   "source": [
    "#### View some samples\n",
    "\n",
    "The following code will draw 2,000 samples from your MAF normalising flow model and display them as a scatter plot. If the model has trained well, this should be a reasonable approximation of the MLDS point cloud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples and display them\n",
    "\n",
    "maf_samples = maf.sample(2000).numpy()\n",
    "plt.scatter(maf_samples[:, 0], maf_samples[:, 1], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bdd57",
   "metadata": {},
   "source": [
    "We will visualise the transformations contained in each of the steps of flow of our normalising model, where each step consists of a MAF bijector followed by a permutation.\n",
    "\n",
    "You should now complete the following function `layer_transformations` to compute and store the transformations after each step of flow.\n",
    "\n",
    "* The function takes `maf_model` and `base_dist_samples` as arguments\n",
    "  * `maf_model` is a `TransformedDistribution` object as defined by your `maf_flow` function\n",
    "  * This MAF flow has a number of steps of flow, each consisting of a MAF bijector followed by a permutation bijector\n",
    "  * `base_dist_samples` is a Tensor of shape `(num_samples, 2)`\n",
    "* Your function should pass `base_dist_samples` through the individual bijectors of `maf_model`\n",
    "  * After each step of flow, append the transformed samples to a list as a numpy array\n",
    "* Your function should then return the list of numpy arrays\n",
    "  * Each array should have shape `(num_samples, 2)`\n",
    "  * There should be one array for each step of flow in `maf_model`\n",
    "  \n",
    "_Hint: the bijectors defining a `TransformedDistribution` can be accessed through its `bijector` property. Similarly, a `tfb.Chain` bijector has a `bijectors` property which contains the list of bijectors used to define it. Note that `tfb.Chain` transforms its inputs through this list of bijectors in reverse order._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def layer_transformations(maf_model, base_dist_samples):\n",
    "    \"\"\"\n",
    "    This function should compute and store the transformed samples as above.\n",
    "    The function should return a list of numpy arrays.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "\n",
    "    h = base_dist_samples\n",
    "    for i, bij in enumerate(reversed(maf_model.bijector.bijectors)):\n",
    "        h = bij.forward(h)\n",
    "        if (i+1) % 2 == 0:\n",
    "            transforms.append(h.numpy())\n",
    "            \n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18203068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualise some samples from the base distribution\n",
    "\n",
    "base_samples = base.sample(2000)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(base_samples.numpy()[:, 0], base_samples.numpy()[:, 1], alpha=0.2)\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to compute the transformations from your MAF normalising flow\n",
    "\n",
    "maf_transforms = layer_transformations(maf, base_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the layer transformations\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.set_title(f\"Layer {i+1}\")\n",
    "    transform = maf_transforms[i]\n",
    "    ax.scatter(transform[:, 0], transform[:, 1], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d5a62",
   "metadata": {},
   "source": [
    "#### Try a different base distribution\n",
    "\n",
    "There are several hyperparameter settings for our MAF flow that you could try varying to see if it improves performance. Below we will try experimenting with a different base distribution.\n",
    "\n",
    "You should now complete the following `gaussian_mixture` function to define a mixture of Gaussians to use for our base distribution.\n",
    "\n",
    "* The function takes no arguments as input. It should define a [`tfd.MixtureSameFamily`](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily) object\n",
    "* The mixture should consist of four 2-d Gaussian distributions, each with equal mixture weight\n",
    "* The means of each Gaussian component should as follows:\n",
    "  * Component 1: $[-3.6, 0]$\n",
    "  * Component 2: $[-1.2, 0]$\n",
    "  * Component 3: $[1, 0]$ \n",
    "  * Component 4: $[3.7, 0]$\n",
    "* Each Gaussian component should have a diagonal covariance matrix with diagonal elements $[0.16, 0.16]$\n",
    "* The function should then return the `MixtureSameFamily` Distribution object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def gaussian_mixture():\n",
    "    \"\"\"\n",
    "    This function should define a tfd.MixtureSameFamily object as described above.\n",
    "    The function should then return the Distribution object.\n",
    "    \"\"\"\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "          mixture_distribution=tfd.Categorical(\n",
    "              probs=[0.25, 0.25, 0.25, 0.25]),\n",
    "          components_distribution=tfd.MultivariateNormalDiag(\n",
    "            loc=[[-3.6, 0], [-1.2, 0.], [1., 0], [3.7, 0]],\n",
    "            scale_diag=[0.4, 0.4]))\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe24815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to define a new base distribution\n",
    "\n",
    "base_gm = gaussian_mixture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new MAF TransformedDistribution\n",
    "    \n",
    "maf2 = maf_flow(base_gm, num_layers=6, hidden_units=[128, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the new MAF model\n",
    "\n",
    "mean_metric = tf.keras.metrics.Mean()\n",
    "rmsprop = tf.keras.optimizers.RMSprop(rho=0.99)\n",
    "losses2 = train_model(maf2, rmsprop, train_ds, 800, mean_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(losses2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(2.0, 5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f172244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples and display them\n",
    "\n",
    "maf_samples2 = maf2.sample(2000).numpy()\n",
    "plt.scatter(maf_samples2[:, 0], maf_samples2[:, 1], alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b50e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualise some samples from the Gaussian mixture base distribution\n",
    "\n",
    "base_gm_samples = base_gm.sample(2000)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(base_gm_samples.numpy()[:, 0], base_gm_samples.numpy()[:, 1], alpha=0.2)\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to compute the transformations from your new MAF normalising flow\n",
    "\n",
    "maf2_transforms = layer_transformations(maf2, base_gm_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9432dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the layer transformations\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.set_title(f\"Layer {i+1}\")\n",
    "    transform = maf2_transforms[i]\n",
    "    ax.scatter(transform[:, 0], transform[:, 1], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd5b6b",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented and trained a Masked Autoregressive Flow (MAF) normalising flow model on a point cloud dataset, and experimented with using different base distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
