{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e6c497",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Summative assessment\n",
    "### Coursework 1: MLPs and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedeb7a9",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "This coursework is released on **Wednesday 31st January 9.00** and is due by **Wednesday 7th February 23.59**. It is worth **10%** of your overall mark. There are 3 questions in this assessment, worth a total of 90 marks. A further 10 marks are awarded for good code quality, clarity and presentation. **You should attempt to answer all questions.** \n",
    "\n",
    "This assessment mainly assesses your understanding of the multilayer perceptron model and the backpropagation algorithm, as well as your ability to use the high-level Keras API.\n",
    "\n",
    "You can make imports as and when you need them throughout the notebook, and add code cells where necessary. Make sure your notebook executes correctly in sequence before submitting.\n",
    "\n",
    "#### Submission instructions\n",
    "\n",
    "Ensure your notebook executes correctly in order. Save your notebook .ipynb file **after you have executed it** (so that outputs are all showing). It is recommended to also export a PDF file of your executed notebook. Upload a zip file containing your notebook (and separate PDF file) to Coursera by the deadline above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0942eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\University-local\\Imperial\\Term 2\\Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You will need the following imports for this assessment. You can make additional imports when you need them\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For some reason I have only been able to get it to work using just keras, not tensorflow.keras:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "#from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88838a0b",
   "metadata": {},
   "source": [
    "### Question 1 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15773a28",
   "metadata": {},
   "source": [
    "a) Load the Boston housing dataset using the Keras API, with a 75/25 train/validation split. \n",
    "\n",
    "Standardise the input features by subtracting the mean and dividing by the standard deviation, where the per-feature statistics are computed from the training dataset. You can use numpy or sklearn for this part if you wish.\n",
    "\n",
    "Load the data into `tf.data.Dataset` objects, shuffle and batch the datasets with a batch size of 32. Print out the `element_spec` of one of the Datasets. \n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc796e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mboston_housing\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      2\u001b[0m inputs, features \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      4\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(data,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data = tf.keras.datasets.boston_housing.load_data()\n",
    "inputs, features = data\n",
    "print(inputs.shape)\n",
    "train, test = train_test_split(data,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7822a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f13309",
   "metadata": {},
   "source": [
    "b) Create a TensorFlow `Sequential` model object according to the following spec:\n",
    "\n",
    "* The model should have 2 hidden layers, with 32 and 16 neurons respectively\n",
    "* Each hidden layer should use a 'swish' activation\n",
    "\n",
    "The model should be an multilayer perceptron (MLP) model suitable for regression on the Boston housing dataset.\n",
    "\n",
    "Train the model for 300 epochs using the training Dataset object, but terminate the training if the validation mean absolute error (MAE) doesn't improve after 30 epochs. Use the stochastic gradient descent (SGD) optimizer with Nesterov momentum, with the momentum hyperparameter set to 0.9, and a learning rate of $10^{-3}$. You should use the high-level Keras API (using `compile`, `fit` methods) for this. The model should be trained with a mean squared error (MSE) loss function. The mean absolute error should also be computed and recorded on the training and validation sets.\n",
    "\n",
    "Plot the MSE and MAE learning curves for training and validation sets, and compute the MSE loss and MAE on the validation set for the best set of model parameters (according to the validation set MAE).\n",
    "\n",
    "**(15 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c798291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2208c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef930df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb01d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ae68ac",
   "metadata": {},
   "source": [
    "c) What do you expect would be the effect of training the same model architecture on the Boston housing dataset where the input features have not been standardised? Briefly justify your answer. \n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd671bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ead1e95e-38bb-4fe0-95bc-0985d2b63d29",
   "metadata": {},
   "source": [
    "d) In terms of the computations carried out, describe in a few sentences what the differences would be (if any) between standardising the input features as above, and inserting a batch normalisation layer before the first dense layer of the model. \n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babed92-3ffd-48ec-8e84-b0ad64b4d0c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2202a29",
   "metadata": {},
   "source": [
    "### Question 2 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee0711",
   "metadata": {},
   "source": [
    "In this question you will empirically study the post-activation statistics in the hidden layers of an MLP model under different initialisation strategies. \n",
    "\n",
    "Consider an MLP model with 5 hidden layers with 8192, 8192, 8192, 4096 and 4096 neurons respectively. Each hidden layer uses a tanh activation function. Let $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$ and $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ denote the weight matrix and bias vector that map from hidden layer $k$ to hidden layer $k+1$ according to the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(k)} &= \\tanh\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, 5,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{h}^{(0)} \\in \\mathbb{R}^{1024}$ denotes the input layer, $n_k$ is the number of neurons in hidden layer $k$, and the tanh function is applied elementwise. Suppose the input features ${h}_i^{(0)}$ are each independently sampled from $N(0, \\frac{1}{2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467de7be",
   "metadata": {},
   "source": [
    "a) Compute the (post-)activations of each hidden layer after passing a single input example through the network (where the input example is sampled as described above), and save them in a variable called `layer_activations`. The following initialisation strategy should be used for the model parameters:\n",
    "\n",
    "1. Each element in each weight matrix $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$ is sampled from a standard normal distribution\n",
    "2. Each bias vector $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ is initialised to zero\n",
    "    \n",
    "Your answer for this part should use only TensorFlow objects and functions, and not use numpy or scipy at all. You can make use of the Keras module if you wish. Weight and bias parameters should be implemented with TF Variable objects.\n",
    "\n",
    "**(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174434e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f59bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae4439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7a9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5281d649",
   "metadata": {},
   "source": [
    "b) Create a plot for the normalised (density) histograms for the activation statistics in each of the hidden layers. Briefly comment on the result.\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da0756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebdc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d84a0c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ffb5db",
   "metadata": {},
   "source": [
    "c) Re-compute the activation statistics for the MLP under two different initialisation strategies:\n",
    "\n",
    "1. Glorot normal distribution initialisation for the weights $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$ and zero initialisation for the bias $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ for $k=0,\\ldots,4$\n",
    "1. Glorot uniform distribution initialisation for the weights $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$ and zero initialisation for the bias $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ for $k=0,\\ldots,4$\n",
    "\n",
    "For each initialisation strategy above, plot normalised histograms for the activation statistics in each hidden layer.\n",
    "\n",
    "**(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330a931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478ba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50541fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a7e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5592ae",
   "metadata": {},
   "source": [
    "d) Comment on your interpretation of the results in the previous parts of this question, what implications there might be for the successful training of the MLP model, and any limitations of the empirical study carried out.\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414e0a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72a470af",
   "metadata": {},
   "source": [
    "### Question 3 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a74398",
   "metadata": {},
   "source": [
    "Consider the following MLP model, designed as an image classifier for the MNIST dataset:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(0)} &:= \\mathbf{x}\\\\\n",
    "\\mathbf{h}^{(k)} &= \\sigma\\left( \\mathbf{W}^{(k-1)}\\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} \\right),\\qquad k=1,2\\\\\n",
    "\\hat{\\mathbf{y}} &= \\textrm{softmax}\\left( \\mathbf{W}^{(2)}\\mathbf{h}^{(2)} + \\mathbf{b}^{(2)} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{784}$ is the flattened image input, $\\mathbf{W}^{(k)}\\in\\mathbb{R}^{n_{k+1}\\times n_k}$ and $\\mathbf{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$ ($k=0,1,2$) are the model weights and biases, and $n_k$ is the number of neurons in the $k$-th layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba23a0",
   "metadata": {},
   "source": [
    "a) Construct this MLP model using the Sequential API. The model will have two hidden layers with 64 neurons each, using a sigmoid activation function, and take an input of shape `(28, 28)`. The output should be a 10-way softmax.\n",
    "\n",
    "Load the MNIST dataset from the Keras API. Normalise the input pixel values to the interval $[0,1]$ and convert the labels to one-hot vectors. Do not shuffle the dataset. Save the training inputs and targets as Tensors `x_train` and `y_train` respectively. The validation/test partition can be discarded.\n",
    "\n",
    "_Hint: you may find it helpful in later parts of this question to use separate Keras layers for the activation functions inside your model object._\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889633f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e241cd0-04ba-4196-aec6-656c86d29b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b6bc4-1a7d-4d0d-94ca-b9f924d5e98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74623685",
   "metadata": {},
   "source": [
    "b) Suppose that the loss function used to train the MLP is the categorical cross entropy loss function, and $L_i$ denotes the (scalar-valued) loss with respect to the $i$-th input example. Consider the pre-activations in the final layer, given by $\\mathbf{a}^{(3)} =  \\mathbf{W}^{(2)}\\mathbf{h}^{(2)} + \\mathbf{b}^{(2)}$. \n",
    "\n",
    "Show that the error $\\delta^{(k)} := \\frac{\\partial L_i}{\\partial \\mathbf{a}^{(3)}}$ given by the derivative of the loss with respect to the final layer pre-activation values is given by $\\hat{\\mathbf{y}} - \\mathbf{y}$, where $\\hat{\\mathbf{y}}$ is the output from the model, and $\\mathbf{y}$ is the ground truth label, represented as a one-hot vector.\n",
    "\n",
    "Write your answer below in Markdown. You do not need to write any code for this part.\n",
    "\n",
    "**(7 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9095306",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b97dbe",
   "metadata": {},
   "source": [
    "c) Write a function called `grads` that implements the backpropagation equations for this model to return the gradients of the categorical cross entropy loss function with respect to the parameters $\\mathbf{W}^{(2)}$ and $\\mathbf{b}^{(2)}$. This function should return these gradients as a list `[grads_W2, grads_b2]`, and it should take the following input arguments:\n",
    "\n",
    "* `model`: your model object, defined using the Sequential API\n",
    "* `inputs`: a Tensor of shape `(batch_size, 28, 28)`\n",
    "* `y_true`: a Tensor of shape `(batch_size, 10)` containing the true labels as one-hot vectors\n",
    "\n",
    "The function `grads` (and any other function it uses) should only use TensorFlow ops. In particular, it should not use automatic differentiation or other libraries (e.g. numpy).\n",
    "\n",
    "You should make sure that your code is clearly written, and variables sensibly named. You might find it helpful to write separate helper functions to be used in the `grads` function.\n",
    "\n",
    " **(15 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547a26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27311cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b6d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf916f-cb55-4718-9130-8d01c63cf6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b28dbac2",
   "metadata": {},
   "source": [
    "d) Compute the gradients on the first 16 examples in the training set using your `grads` function and model. Print out the gradients that are computed. \n",
    "\n",
    "**(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91434cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30679a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7debb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d22f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
