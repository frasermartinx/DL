{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 1: Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement a linear regression model in TensorFlow. You will implement the analytic solution, as well as a low-level training loop to update parameters using gradient descent.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\frase\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# If you would like to make further imports from Tensorflow, add them here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/life_expectancy_wikipedia.png\" title=\"Life expectancy\" style=\"width: 450px;\"/>\n",
    "<center><font style=\"font-size:12px\">source: <a href=https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy>wikipedia</a></font></center>\n",
    "\n",
    "#### The WHO Life Expectancy dataset\n",
    "In this formative assessment, you will use the [WHO Life Expectancy dataset](https://www.kaggle.com/kumarajarshi/life-expectancy-who) from Kaggle. This dataset was collected from the Global Health Observatory (GHO) data repository under the World Health Organization (WHO), for the purpose of health data analysis. The dataset includes multiple factors affecting life expectancy across 133 countries, divided into the broad categories of immunization related factors, mortality factors, economical factors and social factors.\n",
    "\n",
    "Your goal is to use TensorFlow to model the dataset using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>under-five deaths</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2928.000000</td>\n",
       "      <td>2928.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2744.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2385.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>2712.00000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>2.286000e+03</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2771.000000</td>\n",
       "      <td>2775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2007.518720</td>\n",
       "      <td>69.224932</td>\n",
       "      <td>164.796448</td>\n",
       "      <td>30.303948</td>\n",
       "      <td>4.602861</td>\n",
       "      <td>738.251295</td>\n",
       "      <td>80.940461</td>\n",
       "      <td>2419.592240</td>\n",
       "      <td>38.321247</td>\n",
       "      <td>42.035739</td>\n",
       "      <td>82.550188</td>\n",
       "      <td>5.93819</td>\n",
       "      <td>82.324084</td>\n",
       "      <td>1.742103</td>\n",
       "      <td>7483.158469</td>\n",
       "      <td>1.275338e+07</td>\n",
       "      <td>4.839704</td>\n",
       "      <td>4.870317</td>\n",
       "      <td>0.627551</td>\n",
       "      <td>11.992793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.613841</td>\n",
       "      <td>9.523867</td>\n",
       "      <td>124.292079</td>\n",
       "      <td>117.926501</td>\n",
       "      <td>4.052413</td>\n",
       "      <td>1987.914858</td>\n",
       "      <td>25.070016</td>\n",
       "      <td>11467.272489</td>\n",
       "      <td>20.044034</td>\n",
       "      <td>160.445548</td>\n",
       "      <td>23.428046</td>\n",
       "      <td>2.49832</td>\n",
       "      <td>23.716912</td>\n",
       "      <td>5.077785</td>\n",
       "      <td>14270.169342</td>\n",
       "      <td>6.101210e+07</td>\n",
       "      <td>4.420195</td>\n",
       "      <td>4.508882</td>\n",
       "      <td>0.210904</td>\n",
       "      <td>3.358920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.681350</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>63.100000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>4.685343</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>4.26000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>463.935626</td>\n",
       "      <td>1.957932e+05</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>72.100000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.755000</td>\n",
       "      <td>64.912906</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>5.75500</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1766.947595</td>\n",
       "      <td>1.386542e+06</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>12.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.702500</td>\n",
       "      <td>441.534144</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>360.250000</td>\n",
       "      <td>56.200000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>7.49250</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5910.806335</td>\n",
       "      <td>7.420359e+06</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>17.870000</td>\n",
       "      <td>19479.911610</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>212183.000000</td>\n",
       "      <td>87.300000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>17.60000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>50.600000</td>\n",
       "      <td>119172.741800</td>\n",
       "      <td>1.293859e+09</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>20.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year  Life expectancy   Adult Mortality  infant deaths  \\\n",
       "count  2938.000000       2928.000000      2928.000000    2938.000000   \n",
       "mean   2007.518720         69.224932       164.796448      30.303948   \n",
       "std       4.613841          9.523867       124.292079     117.926501   \n",
       "min    2000.000000         36.300000         1.000000       0.000000   \n",
       "25%    2004.000000         63.100000        74.000000       0.000000   \n",
       "50%    2008.000000         72.100000       144.000000       3.000000   \n",
       "75%    2012.000000         75.700000       228.000000      22.000000   \n",
       "max    2015.000000         89.000000       723.000000    1800.000000   \n",
       "\n",
       "           Alcohol  percentage expenditure  Hepatitis B       Measles   \\\n",
       "count  2744.000000             2938.000000  2385.000000    2938.000000   \n",
       "mean      4.602861              738.251295    80.940461    2419.592240   \n",
       "std       4.052413             1987.914858    25.070016   11467.272489   \n",
       "min       0.010000                0.000000     1.000000       0.000000   \n",
       "25%       0.877500                4.685343    77.000000       0.000000   \n",
       "50%       3.755000               64.912906    92.000000      17.000000   \n",
       "75%       7.702500              441.534144    97.000000     360.250000   \n",
       "max      17.870000            19479.911610    99.000000  212183.000000   \n",
       "\n",
       "              BMI   under-five deaths         Polio  Total expenditure  \\\n",
       "count  2904.000000         2938.000000  2919.000000         2712.00000   \n",
       "mean     38.321247           42.035739    82.550188            5.93819   \n",
       "std      20.044034          160.445548    23.428046            2.49832   \n",
       "min       1.000000            0.000000     3.000000            0.37000   \n",
       "25%      19.300000            0.000000    78.000000            4.26000   \n",
       "50%      43.500000            4.000000    93.000000            5.75500   \n",
       "75%      56.200000           28.000000    97.000000            7.49250   \n",
       "max      87.300000         2500.000000    99.000000           17.60000   \n",
       "\n",
       "       Diphtheria      HIV/AIDS            GDP    Population  \\\n",
       "count  2919.000000  2938.000000    2490.000000  2.286000e+03   \n",
       "mean     82.324084     1.742103    7483.158469  1.275338e+07   \n",
       "std      23.716912     5.077785   14270.169342  6.101210e+07   \n",
       "min       2.000000     0.100000       1.681350  3.400000e+01   \n",
       "25%      78.000000     0.100000     463.935626  1.957932e+05   \n",
       "50%      93.000000     0.100000    1766.947595  1.386542e+06   \n",
       "75%      97.000000     0.800000    5910.806335  7.420359e+06   \n",
       "max      99.000000    50.600000  119172.741800  1.293859e+09   \n",
       "\n",
       "        thinness  1-19 years   thinness 5-9 years  \\\n",
       "count            2904.000000          2904.000000   \n",
       "mean                4.839704             4.870317   \n",
       "std                 4.420195             4.508882   \n",
       "min                 0.100000             0.100000   \n",
       "25%                 1.600000             1.500000   \n",
       "50%                 3.300000             3.300000   \n",
       "75%                 7.200000             7.200000   \n",
       "max                27.700000            28.600000   \n",
       "\n",
       "       Income composition of resources    Schooling  \n",
       "count                      2771.000000  2775.000000  \n",
       "mean                          0.627551    11.992793  \n",
       "std                           0.210904     3.358920  \n",
       "min                           0.000000     0.000000  \n",
       "25%                           0.493000    10.100000  \n",
       "50%                           0.677000    12.300000  \n",
       "75%                           0.779000    14.300000  \n",
       "max                           0.948000    20.700000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to load and describe the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/Life Expectancy Data.csv\"))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work the following columns from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of columns to use from the DataFrame\n",
    "\n",
    "cols = ['Life expectancy ', 'Adult Mortality', 'Alcohol', ' BMI ',\n",
    "        'Polio', 'Total expenditure', 'Diphtheria ', ' HIV/AIDS', \n",
    "        'GDP', 'Income composition of resources', 'Schooling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following function, according to the following specifications:\n",
    "\n",
    "* Extract the columns above from the loaded DataFrame\n",
    "* Remove any rows with `NaN` values\n",
    "* Define a 1-D numpy array using the values in the `Life expectancy ` column. This will be the target variable\n",
    "* Define a 2-D numpy array using the values from all remaining columns. This array should have shape `(num_examples, num_features)`. These will be the input variables\n",
    "* The function should then return the tuple of constant `tf.Tensor` objects `(input_variables, target_variable)` of type `tf.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_inputs_and_targets(dataframe, columns):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and column list as above, and extracts the\n",
    "    numpy arrays as described above.\n",
    "    Your function should return a tuple (input_variables, target_variable) of Tensors.\n",
    "    \"\"\"\n",
    "    df = dataframe[columns]\n",
    "    df = df.dropna()\n",
    "    #print(list(df))\n",
    "    #pesky space after\n",
    "    y = df['Life expectancy ']\n",
    "    y = y.to_numpy()\n",
    "    x = df.iloc[:,df.columns != \"Life expectancy \"]\n",
    "    x = x.to_numpy()\n",
    "    y = tf.constant(y,tf.float32)\n",
    "    x = tf.constant(x,tf.float32)\n",
    "    return(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the input and target Tensors\n",
    "\n",
    "X, y = get_inputs_and_targets(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets and standardise the input scales\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = tf.constant(X_train), tf.constant(y_train)\n",
    "X_test, y_test = tf.constant(X_test), tf.constant(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model\n",
    "\n",
    "We will fit a simple model of the form\n",
    "\n",
    "$$\n",
    "y = f_\\theta(\\mathbf{x}) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $y\\in\\mathbb{R}$ is the target variable, $\\mathbf{x}\\in\\mathbb{R}^{10}$ are the input features, $\\theta\\in\\mathbb{R}^{11}$ are the model parameters, $\\epsilon\\sim\\mathcal{N}(0, 1)$ is the observation noise random variable, and $f_\\theta:\\mathbb{R}^{10}\\mapsto\\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\theta(\\mathbf{x}) &= \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m\\\\\n",
    "&= \\sum_{m=0}^{10} \\theta_m x_m.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the second line above we have defined $x_0=1$ to be the constant feature. The maximum likelihood solution is given by the normal equation\n",
    "\n",
    "$$\n",
    "\\theta_{ML} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}\\in\\mathbb{R}^{N\\times M}$ is the data matrix, $\\mathbf{y}\\in\\mathbb{R}^N$ are the targets, $N$ is the number of data examples, and $M$ are the number of features (including the constant feature).\n",
    "\n",
    "You should now complete the following function to implement the normal equation to compute the maximum likelhood solution. Your code should only use TensorFlow functions. \n",
    "\n",
    "* The arguments to the function are an `inputs` Tensor of shape `(num_examples, num_features)`, and a `targets` Tensor of shape `(num_examples,)`\n",
    "* The function should add a column of ones as the first column to the `inputs` Tensor for the constant feature\n",
    "* The function should output a 1-D Tensor of parameters of length `(num_features + 1,)` (the first entry will be the bias)\n",
    "\n",
    "_Hint: check [the docs](https://www.tensorflow.org/api/stable) for relevant TensorFlow functions, including the_ [`tf.linalg`](https://www.tensorflow.org/api_docs/python/tf/linalg) _module._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def normal_equation(inputs, targets):\n",
    "    \"\"\"\n",
    "    This function takes in inputs and targets Tensors, and implements the normal equation\n",
    "    as above, only using TensorFlow functions.\n",
    "    Your function should return a Tensor for the maximum likelihood solution for the parameters.\n",
    "    \"\"\"\n",
    "    #insert 1s col\n",
    "    inputs = inputs.numpy()\n",
    "    inputs = np.insert(inputs,0,1,axis = 1)\n",
    "    inputs = tf.constant(inputs)\n",
    "   \n",
    "    #now implement it\n",
    "    #(X^T * X)^-1\n",
    "    inv = tf.linalg.inv(tf.linalg.matmul(tf.linalg.matrix_transpose(inputs),inputs))\n",
    "    #(X^T * X)^-1 * X^T \n",
    "    param = tf.linalg.matmul(inv,tf.linalg.matrix_transpose(inputs))\n",
    "    #now to matmult with Y, we can expand dimension or do the tensordot:\n",
    "    targets = tf.expand_dims(targets,1)\n",
    "    param = tf.linalg.matmul(param,targets)\n",
    "    #or, without doing the expand_dims\n",
    "    #param = tf.tensordot(param,targets,1)\n",
    "    return param\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE weights:\n",
      "tf.Tensor(\n",
      "[[-2.0756807 ]\n",
      " [-0.19258314]\n",
      " [ 0.8645327 ]\n",
      " [ 0.4715445 ]\n",
      " [ 0.12050518]\n",
      " [ 0.85195696]\n",
      " [-2.7587125 ]\n",
      " [ 0.77138454]\n",
      " [ 1.812767  ]\n",
      " [ 3.0983598 ]], shape=(10, 1), dtype=float32)\n",
      "MLE bias:\n",
      "tf.Tensor([69.09331], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Run your function to compute the ML estimate\n",
    "\n",
    "theta_ml = normal_equation(X_train, y_train)\n",
    "bias_ml, weights_ml = theta_ml[0], theta_ml[1:]\n",
    "print(\"MLE weights:\")\n",
    "print(weights_ml)\n",
    "print(\"MLE bias:\")\n",
    "print(bias_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "You will now implement the (batch) gradient descent algorithm to find the MLE using optimization. To do this, you will make use of the `tf.Variable` class. Recall that a Variable object is a special kind of Tensor that is _mutable_, so we will use it for the model parameters.\n",
    "\n",
    "First, you should complete the following `get_variables` function to create Variable objects for the weights and bias of the linear regression model, as well as an iteration counter Variable.\n",
    "\n",
    "* The function takes `num_features` as an argument\n",
    "* The bias should be a `tf.Variable` with scalar shape, type `tf.float32`, and an initial value of zero. Set the name argument of this Variable to `\"bias\"`\n",
    "* The weights should be a 1-D `tf.Variable` of length `num_features`, type `tf.float32`, and with initial values sampled from a standard normal distribution. Set the name argument of this Variable to `\"weights\"`\n",
    "* Both weights and bias Variables should be trainable\n",
    "* Finally, the function should create a scalar Variable of type `tf.int32`, initialised to zero, with name argument set to `\"iteration\"`. This Variable should be non-trainable\n",
    "* The function should return the tuple of Variables `(weights, bias, iteration)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
       " array([[ 2.614705  ],\n",
       "        [-0.53720653],\n",
       "        [-0.6953684 ],\n",
       "        [ 0.8622861 ],\n",
       "        [ 1.3740011 ],\n",
       "        [-0.25463718],\n",
       "        [-2.3054397 ],\n",
       "        [-1.5883137 ],\n",
       "        [-0.4798559 ],\n",
       "        [ 0.49326068]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_variables(num_features):\n",
    "    \"\"\"\n",
    "    This function takes in the number of features as an argument, and creates tf.Variable objects\n",
    "    for the linear regression model weights and bias, as well as an iteration counter Variable.\n",
    "    Your function should return a tuple of two tf.Variable objects (weights, bias, iteration).\n",
    "    \"\"\"\n",
    "    bias = tf.constant(0,tf.float32)\n",
    "    bias = tf.Variable(bias)\n",
    "    weights = tf.random.normal((num_features,1))\n",
    "    weights = tf.Variable(weights)\n",
    "    iteration = tf.constant(0, tf.int32)\n",
    "    return weights, bias, iteration\n",
    "    \n",
    "get_variables(num_features=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create the Variables\n",
    "\n",
    "weights, bias, iteration = get_variables(num_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model itself by completing the following function. This function implements $f_\\theta(\\mathbf{x}) = \\theta_0 + \\sum_{m=1}^{10} \\theta_m x_m$ as above.\n",
    "\n",
    "* The function takes an `inputs` Tensor, `weights` and `bias` Variables as input\n",
    "* The `inputs` Tensor could be a batch of inputs of shape `(batch_size, num_features)`, or a single set of inputs of shape `(num_features,)`\n",
    "* The function should return the output Tensor $f_\\theta(\\mathbf{x})$\n",
    "* The output Tensor should have shape `(batch_size,)` (if passed a batch of inputs), or else should be a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def f(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    This function takes in an inputs Tensor, weights and bias Variables. It should compute and \n",
    "    return the output Tensor prediction. \n",
    "    \"\"\"\n",
    "    #fix online case\n",
    "    if len(inputs.shape) == 1:\n",
    "        inputs = tf.expand_dims(inputs,0)\n",
    "    #now in same boat\n",
    "    #do the inner product and add the bias\n",
    "    return( tf.squeeze(tf.tensordot(inputs,weights,1) +bias))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2.5266252 -2.8303046 -7.3751035], shape=(3,), dtype=float32)\n",
      "tf.Tensor(-3.2517135, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test your function on some dummy inputs\n",
    "\n",
    "inputs = tf.random.normal((3, 10), dtype=tf.float32)\n",
    "print(f(inputs, weights, bias))\n",
    "\n",
    "inputs = tf.random.normal((10,), dtype=tf.float32)\n",
    "print(f(inputs, weights, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to define the loss function to optimise. As we have assumed Gaussian noise $\\epsilon\\sim\\mathcal{N}(0, 1)$ and we are looking to find the maximum likelihood solution, this will be the mean squared error loss. The loss function that you should implement is therefore:\n",
    "\n",
    "$$\n",
    "{L}_{MSE}(\\theta) = \\frac{1}{M} \\sum_{\\mathbf{x}_i, y_i\\in\\mathcal{D}} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i = f_\\theta(\\mathbf{x}_i)$, and $(\\mathbf{x}_i, y_i)$ is an example input and output from the training dataset $\\mathcal{D}$. The function specifications are as follows:\n",
    "\n",
    "* The `mse` function takes two Tensors as arguments: `y_true` and `y_pred`\n",
    "* These two Tensors will have shape `(num_examples,)`\n",
    "* The loss function should compute and return the mean squared error loss (MSE) as a scalar Tensor\n",
    "* Use only TensorFlow functions inside your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function takes a batch of 'ground truth' values y_true and a corresponding batch\n",
    "    of model predictions y_pred, and computes the mean squared error.\n",
    "    Your function should return the MSE as a scalar Tensor.\n",
    "    \"\"\"\n",
    "    out = (y_true - y_pred)**2\n",
    "    return tf.reduce_mean(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4825.0684>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the initial loss on the training data\n",
    "\n",
    "mse(y_train, f(X_train, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE train loss: 15.953093528747559\n",
      "MLE test loss: 16.666954040527344\n"
     ]
    }
   ],
   "source": [
    "# Compute the train and test loss of the MLE\n",
    "\n",
    "print(\"MLE train loss: {}\".format(mse(y_train, f(X_train, weights_ml, bias_ml))))\n",
    "print(\"MLE test loss: {}\".format(mse(y_test, f(X_test, weights_ml, bias_ml))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the update step of gradient descent, that we will use inside the training loop. Recall this update uses the gradient of the loss with respect to the model parameters to make the update:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_\\theta {L}_{MSE}(\\theta_t),\\qquad t\\in\\mathbb{N}_0,\n",
    "$$\n",
    "\n",
    "where $\\eta>0$ is the learning rate.\n",
    "\n",
    "* The `gradient_descent_update` function takes the following arguments:\n",
    "  * `model_fn` is the function that defines the predictive function (the function `f` above)\n",
    "  * `inputs` and `targets` are the inputs and targets Tensors, of shape `(num_examples, 10)` and `(num_examples,)` respectively\n",
    "  * `w` and `b` are the Variables that represent the model parameters\n",
    "  * The `learning_rate` is the gradient descent hyperparameter\n",
    "* The function should compute the gradient descent update step (assuming the mean squared error loss as above), updating the `w` and `b` Variables accordingly, using the `learning_rate` passed in. It will not return anything; the Variables are updated in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def gradient_descent_update(model_fn, inputs, targets, w, b, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This function takes the model function, inputs batch, targets batch, weights Variable,\n",
    "    bias Variable and learning rate as arguments. It should update the Variables w and b\n",
    "    using the SGD update rule above for the MSE loss.\n",
    "    \"\"\"\n",
    "    #first we find the gradient\n",
    "    #grad_j = 2/M * sum(yi - yihat) * grad_j yi_hat\n",
    "    #for bias: grad_1 = 2 * mean((yi-yihat)*-1)\n",
    "    #for other terms: grad_j = 2*mean((yi-yihat)*-xj)\n",
    "    bias.assign(bias - learning_rate * -2 * tf.reduce_mean(targets - model_fn(inputs,weights,bias)))\n",
    "    \n",
    "    temp = tf.constant([tf.reduce_mean((targets - model_fn(inputs,weights,bias)) * inputs[:,i]).numpy() for i in range(10)])\n",
    "    temp = tf.expand_dims(temp,1)\n",
    "    weights.assign(weights - learning_rate * -2 *temp)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the update:\n",
      "<tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[ 0.12475546],\n",
      "       [-0.66685367],\n",
      "       [ 2.5240932 ],\n",
      "       [ 0.96928704],\n",
      "       [-1.57135   ],\n",
      "       [ 0.29526046],\n",
      "       [-0.91029835],\n",
      "       [ 1.0649941 ],\n",
      "       [ 0.12141252],\n",
      "       [ 0.93090737]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "\n",
      "After the update:\n",
      "<tf.Variable 'Variable:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[-0.32028723],\n",
      "       [-0.39290485],\n",
      "       [ 2.746272  ],\n",
      "       [ 1.1750401 ],\n",
      "       [-1.3252995 ],\n",
      "       [ 0.53883487],\n",
      "       [-1.2645434 ],\n",
      "       [ 1.2992897 ],\n",
      "       [ 0.56417465],\n",
      "       [ 1.3913937 ]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=6.909332>\n"
     ]
    }
   ],
   "source": [
    "# Test your gradient descent update function\n",
    "\n",
    "print(\"Before the update:\")\n",
    "print(weights)\n",
    "print(bias)\n",
    "gradient_descent_update(f, X_train, y_train, weights, bias, learning_rate=0.05)\n",
    "print(\"\\nAfter the update:\")\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to write the training loop in the following function. The training loop consists of a pre-defined number of iterations, where one iteration consists of the gradient descent update step which updates the weights and biases according to the gradient descent update rule. \n",
    "\n",
    "You should complete the following `training_loop` function according to the specifications:\n",
    "\n",
    "* The function takes the following arguments:\n",
    "  * `num_iterations`: a positive integer that defines the number of iterations to run the training loop\n",
    "  * `model_fn`: as before, the function that defines the predictive function\n",
    "  * `training_data`: a 2-tuple of Tensors `(inputs, targets)` for the complete training data\n",
    "  * `w`: the Variable that represents the model weights\n",
    "  * `b`: the Variable that represents the model bias\n",
    "  * `iteration`: a Variable used for counting the total number of updates\n",
    "  * `mse`: the loss function to evaluate the model (this will be your `mse` function above)\n",
    "  * `gradient_descent_update`: the function that implements the gradient descent update (this will be your `gradient_descent_update` function above)\n",
    "  * `learning_rate`: the learning rate for the gradient descent update\n",
    "* The function should perform the gradient descent update `num_iterations` times\n",
    "  * For each iterations, the parameters `w` and `b` should be updated according to `gradient_descent_update`, using the `learning_rate` provided\n",
    "  * At each iteration, the `iteration` Variable should be incremented by one\n",
    "* After every update, the model loss should be evaluated on the training data using the `mse` function and appended to a list as a scalar float\n",
    "* The list of losses should then be returned by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def training_loop(num_iterations, model_fn, training_data, w, b, iteration, \n",
    "                  mse=mse, gradient_descent_update=gradient_descent_update, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This function executes the training loop according to the specifications above. \n",
    "    It should run for num_iterations, updating the model parameters using the \n",
    "    gradient_descent_update function at every iteration.\n",
    "    The function should return the list of losses computed at each iteration, \n",
    "    using the mse function.\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    for i in range(num_iterations):\n",
    "        gradient_descent_update(f,training_data[0],training_data[1],w,b,learning_rate)\n",
    "        iteration += 1\n",
    "        loss.append(mse(training_data[1],f(training_data[0],weights,bias)).numpy())\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise the model parameters and run the training loop\n",
    "\n",
    "weights, bias, iteration = get_variables(num_features=10)\n",
    "losses = training_loop(1000, f, (X_train, y_train), weights, bias, iteration=iteration, \n",
    "                       mse=mse, gradient_descent_update=gradient_descent_update, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the losses\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model train loss: 15.954821586608887\n",
      "Model test loss: 16.689373016357422\n"
     ]
    }
   ],
   "source": [
    "# Compute the train and test loss of the learned weights\n",
    "\n",
    "print(\"Model train loss: {}\".format(mse(y_train, f(X_train, weights, bias))))\n",
    "print(\"Model test loss: {}\".format(mse(y_test, f(X_test, weights, bias))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your learned weights and bias to the exact solution computed earlier. They should be fairly close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights:\n",
      "[[-2.07872   ]\n",
      " [-0.18146311]\n",
      " [ 0.87031883]\n",
      " [ 0.4760362 ]\n",
      " [ 0.12662211]\n",
      " [ 0.8521871 ]\n",
      " [-2.7564769 ]\n",
      " [ 0.769567  ]\n",
      " [ 1.8732573 ]\n",
      " [ 3.0229988 ]]\n",
      "Learned bias:\n",
      "69.09313\n"
     ]
    }
   ],
   "source": [
    "# Print the learned weights and bias\n",
    "\n",
    "print(\"Learned weights:\")\n",
    "print(weights.numpy())\n",
    "print(\"Learned bias:\")\n",
    "print(bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact ML weights:\n",
      "[[-2.0756807 ]\n",
      " [-0.19258314]\n",
      " [ 0.8645327 ]\n",
      " [ 0.4715445 ]\n",
      " [ 0.12050518]\n",
      " [ 0.85195696]\n",
      " [-2.7587125 ]\n",
      " [ 0.77138454]\n",
      " [ 1.812767  ]\n",
      " [ 3.0983598 ]]\n",
      "Exact ML bias:\n",
      "[69.09331]\n"
     ]
    }
   ],
   "source": [
    "# Print the exact weights and bias\n",
    "\n",
    "print(\"Exact ML weights:\")\n",
    "print(weights_ml.numpy())\n",
    "print(\"Exact ML bias:\")\n",
    "print(bias_ml.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented the exact linear regression solution in TensorFlow as well as the gradient descent algorithm for approximating the maximum likelihood parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
