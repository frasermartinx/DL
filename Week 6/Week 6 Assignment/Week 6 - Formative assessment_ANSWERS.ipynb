{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 6: Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will develop an RNN language model to generate text.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from pathlib import Path\n",
    "\n",
    "# If you would like to make further imports from Tensorflow, add them here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/shakespeare.png\" title=\"Shakespeare\" style=\"width: 350px;\"/>  \n",
    "  \n",
    "#### The Shakespeare dataset\n",
    "\n",
    "In this assignment, you will use a subset of the [Shakespeare dataset](http://shakespeare.mit.edu). This dataset consists of a single text file with several excerpts concatenated together. The data is in raw text form, and so far has not yet had any preprocessing. \n",
    "\n",
    "Your goal is to construct an unsupervised RNN model that can generate text according to a distribution learned from the dataset. This will be a character-level sequence model, that will predict text one character at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into a string\n",
    "\n",
    "with open(Path('data/Shakespeare.txt'), 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of chunks of text\n",
    "\n",
    "text_chunks = text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a feel for what the text looks like, we will print a few chunks from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display some randomly selected text samples\n",
    "\n",
    "num_samples = 3\n",
    "inx = np.random.choice(len(text_chunks), num_samples, replace=False)\n",
    "for chunk in np.array(text_chunks)[inx]:\n",
    "    print(chunk)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Datasets\n",
    "\n",
    "As we will train a character-level model, we will split the text chunks into lists of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip any whitespace at the beginning or end of the strings\n",
    "\n",
    "text_chunks = [s.strip() for s in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and validation splits\n",
    "\n",
    "train_split, valid_split = train_test_split(text_chunks, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following `get_dataset` function to create a `tf.data.Dataset` object from one of the splits above. This function should use the `from_generator` static method to create the Dataset; check the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator). \n",
    "\n",
    "* This function takes `data_split` (one of the splits above), `min_len` and `max_len` as arguments\n",
    "* The `from_generator` method requires a callable in the first argument that returns a python generator. Your function should therefore include a nested function that returns a generator\n",
    "  * This generator should yield a single sentence from `data_split`\n",
    "* The `from_generator` method also requires the `output_signature` argument to be set, which defines the shape and type of the returned Tensor (see the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator))\n",
    "* Your function should then filter the Dataset, to filter out data examples with character length less than `min_len` or greater than `max_len` (see the [`tf.strings`](https://www.tensorflow.org/api_docs/python/tf/strings) module)\n",
    "* Your function should then return the Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_dataset(data_split, min_len, max_len):\n",
    "    \"\"\"\n",
    "    This function takes data_split, min_len, max_len and batch_size as arguments, and \n",
    "    returns a tf.data.Dataset object using the from_generator static method.\n",
    "    \"\"\"\n",
    "    def datagen():\n",
    "        for line in data_split:\n",
    "            yield line\n",
    "    dataset = tf.data.Dataset.from_generator(datagen, \n",
    "                                             output_signature=(tf.TensorSpec(shape=(), dtype=tf.string)))\n",
    "    dataset = dataset.filter(lambda s: tf.math.greater_equal(tf.strings.length(s), min_len))\n",
    "    dataset = dataset.filter(lambda s: tf.math.less_equal(tf.strings.length(s), max_len))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to create the training and validation Datasets\n",
    "\n",
    "train_ds = get_dataset(train_split, 10, 400)\n",
    "valid_ds = get_dataset(valid_split, 10, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert the sentence strings to integer tokens for processing by the recurrent neural network. We will convert the sentences to tokens at the character level; we will use the `TextVectorization` layer for this.\n",
    "\n",
    "You should now complete the following `tokenize` function to process the Dataset output into integer tokens.\n",
    "\n",
    "* The function takes the `train_ds` and `valid_ds` Datasets as arguments\n",
    "* It should create an instance of the `TextVectorization` layer\n",
    "  * This layer should be set up to allow unlimited number of tokens\n",
    "  * It should standardize the text by converting it to lower case\n",
    "  * It should split the input sentences at the character level\n",
    "* The `TextVectorization` object should be configured using both the `train_ds` and `valid_ds`\n",
    "  * The `adapt` method can be used for this purpose, see the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt)\n",
    "* Each Dataset should then be processed by applying the `TextVectorization` layer with the `map` method\n",
    "* The function should then return the processed Datasets and `TextVectorization` object in a tuple `(train_ds, valid_ds, text_vectorization)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def tokenize(train_ds, valid_ds):\n",
    "    \"\"\"\n",
    "    This function takes train_ds and valid_ds as arguments, and processes them with a \n",
    "    TextVectorization object to convert the characters into integer tokens.\n",
    "    It should then return the train_ds, valid_ds and TextVectorization object\n",
    "    \"\"\"\n",
    "    text_vectorization = TextVectorization(max_tokens=None, standardize='lower', split='character')\n",
    "    text_vectorization.adapt(train_ds.concatenate(valid_ds))\n",
    "    train_ds = train_ds.map(text_vectorization)\n",
    "    valid_ds = valid_ds.map(text_vectorization)\n",
    "    return train_ds, valid_ds, text_vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to process the Datasets and create the TextVectorization object\n",
    "\n",
    "train_ds, valid_ds, text_vectorization = tokenize(train_ds=train_ds, valid_ds=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will receive a sequence of characters and predict the next character in the sequence. At training time, the model can be passed an input sequence, with the target sequence is shifted by one.\n",
    "\n",
    "For example, consider the expression `to be or not to be` from Shakespeare's play 'Hamlet'. Given the input `to be or not to b`, the correct prediction is `o be or not to be`. Notice that the prediction is the same length as the input.\n",
    "\n",
    "![sequence_prediction_example](figures/to-be-or-not-to-be.png)\n",
    "\n",
    "You should now write the following `create_inputs_and_targets` function to further process the current Dataset objects, to return input and target Tensors. The function has arguments `dataset`, `batch_size` and `shuffle_buffer`.\n",
    "\n",
    "* First, your function should call the `padded_batch` method (see the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch)), using the `batch_size` argument\n",
    "  * This method will pad each batch of examples with empty strings so they are all the same length and can fit into a single Tensor\n",
    "* The Dataset elements should then be split into input and output Tensors, as described above\n",
    "  * The input Tensor should contain the first `max_seq_len - 1` tokens of each sequence \n",
    "  * The output Tensor should contain the last `max_seq_len - 1` tokens of each sequence\n",
    "* If `shuffle_buffer` is `None` (the default), then the Dataset should not be shuffled\n",
    "* If `shuffle_buffer` is an integer, then it should be used to shuffle the Dataset\n",
    "* The function should then end with a call to `prefetch`, using the `tf.data.AUTOTUNE` argument\n",
    "* Your function should then return the (maybe) shuffled and batched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def create_inputs_and_targets(dataset, batch_size, shuffle_buffer=None):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and shuffle_buffer as arguments.\n",
    "    It should batch the dataset, split into inputs and targets, maybe shuffle,\n",
    "    and prefetch the dataset. It should then return the Dataset object.\n",
    "    \"\"\"\n",
    "    dataset = dataset.padded_batch(batch_size)\n",
    "    def inputs_and_targets(batch):\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        return inputs, targets\n",
    "    \n",
    "    dataset = dataset.map(inputs_and_targets)\n",
    "    if shuffle_buffer is not None:\n",
    "        dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to create batched input and output sequences\n",
    "\n",
    "train_ds = create_inputs_and_targets(train_ds, 32, shuffle_buffer=100)\n",
    "valid_ds = create_inputs_and_targets(valid_ds, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: an alternative method for preparing these Datasets could be to split the strings into lists of characters and use the* [`StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) *class from the* `tf.keras.layers` *module*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the recurrent neural network model\n",
    "\n",
    "You are now ready to build your RNN character-level language model. You should write the following function to build and compile the model. The function takes arguments `text_vectorization` (created earlier), `embedding_dim` (for the `Embedding` layer), and `gru_units` (for the GRU layer). \n",
    "\n",
    "Using the functional API, your function should build your model according to the following specifications:\n",
    "\n",
    "* The first layer should be an `Input` layer, with a single dimension for the (variable) sequence length\n",
    "* The second layer should be an Embedding layer with an embedding dimension of `embedding_dim`, and the vocabulary size set using the `text_vectorization` object\n",
    "  * *Hint: The* `TextVectorization` *object has a* `get_vocabulary` *method that returns the vocabulary stored in the object*\n",
    "  * The Embedding layer should also mask the zero padding in the input sequences\n",
    "* The next layer should be a (uni-directional) GRU layer with number of units set by the `gru_units` argument\n",
    "  * The GRU layer should return the full sequence, instead of just the output state at the final time step.\n",
    "  * It should also return its internal state\n",
    "* The output of the GRU layer should then be fed through a final `Dense` layer with number of units set to vocabulary size, and no activation function\n",
    "* The network should have multiple outputs consisting of the `Dense` layer output and the internal state of the GRU layer\n",
    "* The model should then be compiled.\n",
    "  * Use the Adam optimizer with the default arguments\n",
    "  * For the `loss` argument, you should pass a list of losses, one for each model output. The `Dense` layer output should have a cross entropy loss, and the GRU internal state loss can be `None`\n",
    "  * Similarly, use a sparse categorical accuracy metric, just for the `Dense` layer output\n",
    "\n",
    "In total, the network should have 4 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_model(text_vectorization, embedding_dim, gru_units):\n",
    "    \"\"\"\n",
    "    This function takes a vocabulary size and batch size, and builds and returns a \n",
    "    Sequential model according to the above specification.\n",
    "    \"\"\"\n",
    "    vocab_size = len(text_vectorization.get_vocabulary())\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(None,), name=\"token_input\")\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                                          mask_zero=True, name='embedding')(inputs)\n",
    "    h, state = tf.keras.layers.GRU(units=gru_units, return_sequences=True, \n",
    "                                   return_state=True, name='gru')(embedding)\n",
    "    preds = tf.keras.layers.Dense(vocab_size, name='preds')(h)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[preds, state])\n",
    "    \n",
    "    losses = [\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        None\n",
    "    ]\n",
    "    model.compile(optimizer='adam', \n",
    "                  metrics=[['sparse_categorical_accuracy'], []], \n",
    "                  loss=losses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and print the model summary\n",
    "\n",
    "rnn_model = get_model(text_vectorization, 256, 1024)\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = rnn_model.fit(train_ds, validation_data=valid_ds, epochs=15, \n",
    "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot accuracy vs epoch and loss vs epoch\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['preds_sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_preds_sparse_categorical_accuracy'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(1 + np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(1 + np.arange(len(history.history['preds_sparse_categorical_accuracy'])))\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a text generation algorithm\n",
    "\n",
    "An algorithm to generate text is as follows:\n",
    "\n",
    "1. Specify a seed string (e.g. `'ROMEO:'`) to get the network started, and a define number of characters for the model to generate, `num_generation_steps`.\n",
    "2. Tokenize the seed string to obtain a list containing one list of the integer tokens.\n",
    "3. Reset the initial state of the recurrent network layer to zeros. \n",
    "4. Convert the token list into a Tensor (or numpy array) and pass it to your model as a batch of size one.\n",
    "5. Get the model prediction (logits) for the last time step and extract the state of the recurrent layer.\n",
    "6. Use the logits to construct a categorical distribution and sample a token from it.\n",
    "7. Repeat the following for `num_generation_steps - 1` steps:\n",
    "\n",
    "    1. Use the saved state of the recurrent layer and the last sampled token to get new logit predictions\n",
    "    2. Use the logits to construct a new categorical distribution and sample a token from it.\n",
    "    3. Save the updated state of the recurrent layer.    \n",
    "\n",
    "8. Take the final list of tokens and convert to text using the TextVectorization layer vocabulary.\n",
    "\n",
    "Note that we have built our RNN model to return the internal state of the recurrent layer, as well as the logits output from the `Dense` layer. For the GRU layer, the internal state is a single Tensor of shape `(batch_size, gru_units)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's current recurrent state\n",
    "\n",
    "for inputs, outputs in train_ds.take(1):\n",
    "    print(rnn_model(inputs)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will break the algorithm down into two steps. First, you should complete the following `sample_token` function that takes a sequence of tokens of any length and returns a token prediction for the last time step. The specification is as follows:\n",
    "\n",
    "* The function takes the `model` instance, `token_sequence` Tensor, and optional `initial_state` Tensor for the GRU layer\n",
    "* The `token_sequence` will be an integer Tensor with shape `(batch_size, seq_length)`\n",
    "  * The `seq_length` will be greater or equal to one\n",
    "* If the function argument `initial_state` is `None`, then the function should reset the state of the recurrent layer to zeros\n",
    "* Otherwise, if the function argument `initial_state` is a 2D Tensor or numpy array, it should be used as the initial state of the GRU layer\n",
    "* Get the model's prediction (logits) for the last time step only\n",
    "* Use the logits to form a categorical distribution and sample from it (*hint: you might find the* `tf.random.categorical` *function useful for this; see the documentation [here](https://www.tensorflow.org/api_docs/python/tf/random/categorical)*)\n",
    "* The function should then return the sample as a 2D integer Tensor of shape `(batch_size, 1)` as well as an updated GRU layer state of shape `(batch_size, gru_units)` in a tuple `(samples, updated_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def sample_token(model, token_sequence, initial_state=None):\n",
    "    \"\"\"\n",
    "    This function takes a model object, a token sequence and an optional initial\n",
    "    state for the recurrent layer. The function should return the logits prediction\n",
    "    for the final time step as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    h = token_sequence\n",
    "    updated_state = None\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if i == 2:  \n",
    "            h, updated_state = layer(h, initial_state=initial_state)\n",
    "        else:\n",
    "            h = layer(h)\n",
    "    final_step = h[:, -1, :]  # (batch_size, num_tokens)\n",
    "    samples = tf.random.categorical(final_step, 1)  # (batch_size, 1)\n",
    "    return samples, updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function by passing in a dummy token sequence\n",
    "\n",
    "sample_token(rnn_model, tf.constant([[30, 2, 24], [16, 12, 33]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should complete the following function to generate text from the model, given a seed string.\n",
    "\n",
    "* This function takes the `model` instance, `seed_string`, `text_vectorization`, `num_generation_steps` and `sample_token` as arguments\n",
    "* The function should first convert the `seed_string` to integer tokens using the `text_vectorization` object, and store them in a 2D integer Tensor with batch size equal to one\n",
    "* The function should then run an internal loop for `num_generation_steps`:\n",
    "  * In the first iteration through the loop, the integer token sequence should be passed to the `sample_token` function (passed in as an argument), to get the next sample token and updated GRU state\n",
    "  * The `initial_state` can be set to `None` in the first iteration, in which case it is initialised to zeros\n",
    "  * For the remaining iterations, the `sample_token` function should be called using the sampled token (with batch size and sequence length of one) and updated internal GRU state\n",
    "* The `text_vectorization` object should then be used to convert the final sequence of integer tokens back to characters, and then concatenated to a single string\n",
    "  * The final string will have length given by `num_generation_steps` plus the length of the initial seed string\n",
    "* Your function should then return this final string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def generate_text(model, seed_string, text_vectorization, num_generation_steps, sample_token=sample_token):\n",
    "    \"\"\"\n",
    "    This function takes a model object, a seed string, a TextVectorization object and a \n",
    "    number of steps to generate characters as arguments. It should generate text \n",
    "    according to the above directions and return the extended string.\n",
    "    \"\"\"\n",
    "    token_sequence = text_vectorization(seed_string)[tf.newaxis, ...]  # (1, seq_length)\n",
    "    input_sequence = token_sequence\n",
    "    initial_state = None\n",
    "    for _ in range(num_generation_steps):\n",
    "        sample, updated_state = sample_token(model, input_sequence, initial_state=initial_state)\n",
    "        token_sequence = tf.concat((token_sequence, sample), axis=1)\n",
    "        input_sequence = sample\n",
    "        initial_state = updated_state\n",
    "    \n",
    "    inx_to_chars = {i: c for i, c in enumerate(text_vectorization.get_vocabulary())}\n",
    "    final_token_sequence = tf.squeeze(token_sequence).numpy()\n",
    "    final_char_sequence = [inx_to_chars[token] for token in final_token_sequence]\n",
    "    return ''.join(final_char_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text from the model\n",
    "\n",
    "You are now ready to generate text from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed string and number of generation steps\n",
    "\n",
    "init_string = 'ROMEO:'\n",
    "num_generation_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model and function above to generate text\n",
    "\n",
    "print(generate_text(rnn_model, init_string, text_vectorization, num_generation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now built and trained a character-level RNN language model on text data, and used it to generate new text examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
