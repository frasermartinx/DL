{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e6c497",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Summative assessment\n",
    "### Coursework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedeb7a9",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "This coursework is released on **Wednesday 21st February 9.00** and is due by **Wednesday 6th March 23.59**. It is worth **40%** of your overall mark. There are 4 questions in this assessment, and a total of 100 marks are available. **You should attempt to answer all questions.** In addition to the total number of marks per question below, an additional 10 marks is available for presentation and clarity/quality of code.\n",
    "\n",
    "This assessment assesses your ability to design, implement, train and evaluate a deep learning model for a classification task using multimodal data.\n",
    "\n",
    "You can make imports as and when you need them throughout the notebook, and add code cells where necessary. Make sure your notebook executes correctly in sequence before submitting.\n",
    "\n",
    "#### Submission instructions\n",
    "\n",
    "The submission for this assessment will consist of a notebook (.ipynb file) and a PDF submission.\n",
    "\n",
    "Ensure your notebook executes correctly in order. Save your notebook .ipynb file **after you have executed it** (so that outputs are all showing). It is recommended to also submit a PDF copy of your executed notebook, in case the .ipynb file is corrupted for some reason. \n",
    "\n",
    "Upload a zip file containing your notebook and separate PDF file(s) to Coursera by the deadline above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942eeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You will need the following imports for this assessment. You can make additional imports when you need them\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import (Layer, Input, Dense, GRU, Embedding, Conv2D, BatchNormalization, Activation, GlobalMaxPooling2D)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb167a-1d60-4635-b929-cd1bb7d98146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need access to a GPU for this coursework\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcd578",
   "metadata": {},
   "source": [
    "#### The CLEVR Dataset\n",
    "\n",
    "This assessment makes use of the [CLEVR Dataset](https://cs.stanford.edu/people/jcjohns/clevr/). This dataset is a visual question answering dataset, and consists of images with corresponding text questions and answers about the image.  \n",
    "\n",
    "* Johnson, J., Hariharan, B., van der Maaten, L., Li, F.-F., Zitnick, C. L. & Girshick, R. (2016), \"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\", *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 1988-1997.\n",
    "\n",
    "The original dataset consists of a training set of 70,000 images and 699,989 questions, a validation set of 15,000 images and 149,991 questions, and a test set of 15,000 images and 14,988 questions. In this coursework you will work with a subset of the training and validation splits, which have been preprocessed and prepared for you. \n",
    "\n",
    "The data is stored in TFRecord format, which is a data format that is efficient for TensorFlow to work with. You can read about the TFRecord format [here](https://www.tensorflow.org/tutorials/load_data/tfrecord) if you are interested, but there is no need to find out about TFRecord for this assessment. The code to read in the TFRecord data to Dataset objects is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TFRecordDataset([os.path.join('data', 'train', f) \n",
    "                                    for f in os.listdir(os.path.join('data', 'train')) if f.endswith('tfrecords')])\n",
    "val_ds = tf.data.TFRecordDataset([os.path.join('data', 'val', f) \n",
    "                                    for f in os.listdir(os.path.join('data', 'val')) if f.endswith('tfrecords')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following helper function will parse the TFRecord files to return a dictionary of TensorFlow objects\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"question\": tf.io.VarLenFeature(dtype=tf.string),\n",
    "        \"answer\": tf.io.VarLenFeature(dtype=tf.string)\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=features)\n",
    "    parsed_features[\"question\"] = tf.sparse.to_dense(parsed_features[\"question\"])\n",
    "    parsed_features[\"answer\"] = tf.sparse.to_dense(parsed_features[\"answer\"])\n",
    "    image = tf.io.decode_raw(parsed_features[\"image\"], tf.int32)\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    parsed_features[\"image\"] = image\n",
    "    return parsed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(parse_function)\n",
    "val_ds = val_ds.map(parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cad08-c46a-452e-bd37-42fb4e5c108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023241f",
   "metadata": {},
   "source": [
    "Your task in this assessment is to develop a deep learning model to predict the answer for a given question about an image.\n",
    "\n",
    "You will need to implement special customised layers and a sophisticated model architecture, making use of both CNN and RNN models. You will process the data, train and evaluate the specified model, and then write a proposal for your own modified architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88838a0b",
   "metadata": {},
   "source": [
    "### Question 1 (Total 15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15773a28",
   "metadata": {},
   "source": [
    "The training and validation datasets both return dictionaries with keys `\"image\"`, `\"question\"` and `\"answer\"`. For each image, there are multiple questions and answers. The question and answer entries in the dictionary are both lists of strings of the same length, with aligned questions and answers for the given image. The image entry is a 224x224x3 integer Tensor. These images have been resized from the original size of 480x320, so they appear slightly stretched (this can be ignored). \n",
    "\n",
    "a) Inspect the contents of the dataset by displaying at least one image and it's corresponding questions and answers.\n",
    "\n",
    "**(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16938a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04040394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da3be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfc75066-c79e-4d03-b066-dc53b3367c48",
   "metadata": {},
   "source": [
    "b) The training and validation Datasets should be processed as follows:\n",
    "\n",
    "* The image pixel values should be scaled to the interval $[0, 1]$.\n",
    "* The answers should be (sparse) encoded as integer labels. You will need to compute the total number of distinct answers to do this.\n",
    "* The questions should be tokenized and represented as a sequence of integer tokens. The questions should be split on whitespace and standardized by lowercasing and removing punctuation. \n",
    "* A single question-answer pair should be uniformly sampled from the available questions and answers for each image (so each image should appear exactly once per epoch with a single question-answer pair).\n",
    "* The inputs to the model will be the question and the image. The targets will be the answer. Process the Datasets so that they return a tuple of 2 elements corresponding to inputs and targets.\n",
    "* Shuffle the training Dataset, and batch both Datasets with batch size 64.\n",
    "\n",
    "Print out the number of distinct answer labels, as well as the number of tokens in the vocabulary computed from the questions.\n",
    "\n",
    "Print out the element_spec of one of the Datasets after processing. \n",
    "\n",
    "**(12 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe8690-c899-4be6-b482-0064d14fce49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d81af-20b6-43db-a21b-f1de5c996961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86e4f8-2511-4091-a174-759ce267b829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec774f4b-bdf4-4521-9b9a-5de7de35685a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d10df-418c-481c-8ac3-b898a7931c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1075f29a-81c2-4a11-9bde-768e9fb269e5",
   "metadata": {},
   "source": [
    "### Question 2 (Total 35 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf37ac-40e3-40eb-894b-28cd8bdb8432",
   "metadata": {},
   "source": [
    "The model that you will implement for the visual question answering task was first proposed in the paper\n",
    "\n",
    "* Perez, E., Strub, F., de Vries, H. & Courville, A. (2018), \"FiLM: visual reasoning with a general conditioning layer\", in _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence_, New Orleans, Louisiana, USA.\n",
    "\n",
    "The main idea is the introduction of a specialised layer called a FiLM layer (Feature-wise Linear Modulation). The purpose of this layer is to modify the predictions that are made by a CNN prediction model (the central stack coloured in brown in the figure below). The CNN prediction model takes the image as input, and outputs a categorical distribution over the set of possible answers.\n",
    "\n",
    "The FiLM layer uses information stored in a vector embedding (which comes from the question text) to modify the post-activations of the CNN prediction model. This vector embedding is produced by a gated recurrent unit (GRU) network (referred to in the original paper as the FiLM generator) as the final hidden layer representation after processing the input question. This vector embedding is also referred to as the conditioning signal.\n",
    "\n",
    "The overall model architecture is shown in the figure below:\n",
    "\n",
    "<center><img src=\"figures/model.png\" alt=\"Model architecture\" style=\"width: 450px;\"/></center>\n",
    "<center>Overall model architecture</center>\n",
    "\n",
    "The question is tokenized, and learned embeddings are processed sequentially by the GRU network/FiLM generator. There are potentially multiple FiLM layers within the CNN prediction model. Each FiLM layer uses the GRU embedding $\\mathbf{q}$ (the conditioning signal) to modify the output of a convolutional layer within the CNN prediction model, as described in part c)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c54e1-344e-45e4-b513-748092b31bb4",
   "metadata": {},
   "source": [
    "a) Implement the FiLM generator as a 2-layer stacked GRU network, using an embedding dimension of 64, and 128 neurons for both of the layers of the GRU. The network should output the final 128-dimensional embedding. Print the model summary.\n",
    "\n",
    "**(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28806d5a-f033-443b-9c7f-8a9ddcfed163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63d3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462e459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "879bea7c-6b09-4fc0-9f32-5aba04f8b7f4",
   "metadata": {},
   "source": [
    "b) The first block of the CNN prediction model is a feature extractor CNN which does not make use of the conditioning signal $\\mathbf{q}$ from the GRU network. This block takes the image as input, and passes it through two sub-blocks, each consisting of the following layers:\n",
    "\n",
    "* A 2D convolutional layer with 128 filters, a 4x4 kernel, 2x2 strides, 'SAME' padding, and no activation function\n",
    "* A batch normalisation layer\n",
    "* An element-wise ReLU activation\n",
    "\n",
    "Implement the feature extractor CNN and print the model summary.\n",
    "\n",
    "**(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4bf2c-f0d9-436d-a876-19a4f7b4735a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1f306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412b67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48c19dc6-a633-4d39-bd3c-42cc01e2bcc4",
   "metadata": {},
   "source": [
    "c) Implement a custom layer class for the FiLM layer as described below. This class should subclass the base `Layer` class in the `tensorflow.keras.layers` module. \n",
    "\n",
    "This layer will need to take two inputs when it is called: the conditioning signal $\\mathbf{q}$, as well as the previous convolutional layer output $\\mathbf{h}$. \n",
    "\n",
    "The FiLM layer passes the conditioning signal $\\mathbf{q}$ output by the GRU FiLM generator through a linear layer (dense layer with no activation function) to produce $\\gamma$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "\\gamma = \\textrm{Linear}(\\mathbf{q}) \\quad\\quad \\beta = \\textrm{Linear}(\\mathbf{q}).\n",
    "$$\n",
    "\n",
    "Both $\\gamma$ and $\\beta$ are vectors, with length equal to the number of feature maps (or channels) in the output of a convolutional layer $\\mathbf{h}$. These post-activations are then modulated via the feature-wise affine transformation:\n",
    "\n",
    "$$\n",
    "\\textrm{FiLM}(\\mathbf{h} | \\gamma, \\beta)_{h, w, c} = \\gamma_c \\mathbf{h}_{h, w, c} + \\beta_c,\n",
    "$$\n",
    "where the subscripts $h, w, c$ index the height, width and channel dimensions respectively.\n",
    "\n",
    "Create an instance of your custom layer class and test it on some dummy inputs to verify it works as expected.\n",
    "\n",
    "**(10 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc796e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65365654-5ddb-49b0-b914-ce3d05348e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27482d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51fee9b3-93e3-42dc-bb78-3508871ebe8f",
   "metadata": {},
   "source": [
    "d) The second main block of the CNN network consists of a number of ResBlocks. Each ResBlock consists of the following layers:\n",
    "\n",
    "<center><img src=\"figures/resblock.png\" alt=\"ResBlock\" style=\"width: 150px;\"/></center>\n",
    "\n",
    "\n",
    "* A 1x1 convolutional layer with 128 channels and ReLU activation function\n",
    "* A 3x3 convolutional layer with 128 channels and no activation function\n",
    "* A BatchNormalization layer, where the usual $\\gamma$ and $\\beta$ parameters are not used\n",
    "* a FiLM layer, that also uses the conditioning signal $\\mathbf{q}$ from the GRU network\n",
    "* An elementwise ReLU activation function\n",
    "* The output is then added to the output of the first convolutional layer\n",
    "\n",
    "Each convolutional layer uses 'SAME' padding.\n",
    "\n",
    "Implement the ResBlock as another custom layer. Similar to the FiLM layer, this layer will also need to take two inputs when it is called: the conditioning signal $\\mathbf{q}$, as well as the previous convolutional layer output $\\mathbf{h}$. \n",
    "\n",
    "Create an instance of your custom layer class and test it on some dummy inputs to verify it works as expected.\n",
    "\n",
    "**(8 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463e72-abf5-45ad-b1c7-52faf3efad90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494edb1-6826-496d-9782-3ee1f9317abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea053bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc319491-6d56-4fca-a70c-bca662bd7749",
   "metadata": {},
   "source": [
    "e) At several points in the model, two coordinate feature maps will be added to the output of a convolutional layer. This operation will be applied before each ResBlock and the classifier component of the CNN prediction model. \n",
    "\n",
    "These two feature maps indicate relative $x$ and $y$ spatial position, and are each scaled from $-1$ to $1$ across the height and width dimensions. These two feature maps are concatenated as two extra channels to the convolutional layer output.\n",
    "\n",
    "Implement this operation as another custom layer class called `AddSpatialCoordinates`. The layer should be able to accept input Tensors with arbitrary height, width and channel dimensions. This custom layer will not have any trainable variables.\n",
    "\n",
    "Create an instance of your custom layer class and test it on some dummy inputs to verify it works as expected.\n",
    "\n",
    "**(7 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a0fc0-fad4-4755-a24e-c4fd1a273af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab8bd4-59c4-4bfb-bb45-34b19b80b3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6195e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "155f8b64-b651-4c48-b5f2-f8003313bbc5",
   "metadata": {},
   "source": [
    "f) The final main block of the CNN network is a classifier block. This block consists of the following layers:\n",
    "\n",
    "* 1x1 convolution with 512 output channels, ReLU activation, and 'SAME' padding\n",
    "* Global max pooling across height and width dimensions\n",
    "* Dense layer with 512 neurons and ReLU activation\n",
    "* Final Dense layer with $n_c$ neurons and softmax activation, where $n_c$ is the number of output labels\n",
    "\n",
    "Once you have implemented the classifier, you should bring all components together to build the complete model. This model consists of the following:\n",
    "\n",
    "* GRU FiLM generator as defined in part a) that processes the sequence of question tokens and outputs an embedding $\\mathbf{q}$ of dimension 128\n",
    "* Feature extractor block as defined in part b) that processes the input image\n",
    "* The output of the feature extractor should then be extended with spatial coordinate feature maps by passing it through your `AddSpatialCoordinates` layer\n",
    "* This should be followed by just one ResBlock custom layer, that takes in two inputs: the output from the previous `AddSpatialCoordinates` layer and the question embedding $\\mathbf{q}$. We will only use one ResBlock due to computational limitations\n",
    "* The output of the ResBlock should then also be extended with spatial coordinate feature maps by passing it through your `AddSpatialCoordinates` layer\n",
    "* The output from the previous `AddSpatialCoordinates` layer should then be sent through the classifier block to obtain the final output prediction\n",
    "\n",
    "Implement the complete model according to the above spec, and print the model summary.\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10caf0dc-59a5-4580-9135-6b84625bce1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85191bba-5657-49b9-9ec6-2bbaaf6950ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821366f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "696825b0-167a-4fab-b9ca-f5b527cd7a38",
   "metadata": {},
   "source": [
    "### Question 3 (Total 30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce2b50-6bff-4fff-b2fa-5e0bdb8d6937",
   "metadata": {},
   "source": [
    "a) You should now train your model from question 2 using a cross entropy loss function. Train the model for 20 epochs, with an Adam optimizer with learning rate 3e-4. You should track model performance on the validation set, including the accuracy. \n",
    "\n",
    "Your code should be structured to account for restarting broken training runs. You will need to save your model every epoch, and save all of the model's training and validation performance up to that point (a convenient method is to use the `CSVLogger` callback). In the case of a broken training run, the required data should be loaded, and the training run resumed from the last saved checkpoint. You do not need to use early stopping in the training run.\n",
    "\n",
    "When training has completed, compute and print the final evaluation of your model on the validation set.\n",
    "\n",
    "_NB: The model would need to be larger and trained for longer to achieve good performance on this task. The model and training have been scaled down to accommodate infrastructure limitations on the Coursera platform. You should implement the architecture as specified in this assessment, but you can train the model for longer if you wish. The performance of the resulting model is **not** part of the marking criteria._\n",
    "\n",
    "**(15 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02bbdf-ee43-4157-a026-5f3361bf19cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2304e81-33c5-4c1b-8124-81d85be49f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccd7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a6b4267-3914-4135-ac7e-395b3d05df2f",
   "metadata": {},
   "source": [
    "b) Plot the loss and accuracy over the course of training on the training and validation sets.\n",
    "\n",
    "Select at least one sample image and question from the validation set, and compute the model predictions. Display the image, question, ground truth answer and model predictive distribution over the set of answers.\n",
    "\n",
    "**(7 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97343dde-440b-41e1-85a2-3721d9993096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47ebed-5bbf-4db0-927f-7fc44acb4d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edd87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161f7414-b0c2-4548-bf74-1914c4d81595",
   "metadata": {},
   "source": [
    "c) Explain why adding spatial coordinate feature maps as in 2e) is beneficial for the proposed model and task from questions 1 and 2.\n",
    "\n",
    "**(3 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153da97f-2d13-4f65-b160-b9bc998a057f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7531217f-e2ef-41b2-a122-7d5c611d3089",
   "metadata": {},
   "source": [
    "d) An alternative method to FiLM to incorporate conditioning information would be to concatenate the conditional embedding $\\mathbf{q}$ with the channel dimension in a convolutional layer input $\\mathbf{h}^{(k-1)}$ at every spatial location (in other words, concatenate constant feature maps with the input $\\mathbf{h}^{(k-1)}$ to a convolutional layer). \n",
    "\n",
    "Explain how this method would compare in terms of computation and parameter efficiency with applying the FiLM layer computation outlined in 2c) to the output $\\mathbf{h}^{(k)}$ of the convolutional layer. You can assume the convolutional layer has no activation function.\n",
    "\n",
    "**(5 marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf89ffc-3fc5-499d-b013-6763ce082504",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2202a29",
   "metadata": {},
   "source": [
    "### Question 4 (Total 10 marks)\n",
    "\n",
    "Provide a separate PDF report with your evaluation and conclusions on the model and training results in this assessment.\n",
    "\n",
    "In addition, compare the experiment conducted in this assessment with that described in section 2 of the [original paper](https://arxiv.org/abs/1709.07871). In particular, discuss how the model architecture and training algorithm differ.\n",
    "\n",
    "Your report should be no more than 1 page.\n",
    "\n",
    "**(10 marks)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
