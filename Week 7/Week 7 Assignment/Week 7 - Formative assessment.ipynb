{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 7: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement and train a Transformer classifier model.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell to import all required packages. \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (TextVectorization, Dense, MultiHeadAttention, LayerNormalization, \n",
    "                                     Layer, Embedding, Dropout, GlobalAveragePooling1D)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/IMDb.png\" title=\"IMDb\" style=\"width: 550px;\"/>  \n",
    "  \n",
    "#### The IMDb dataset\n",
    "\n",
    "In this assignment, you will use the [IMDb dataset](https://https://www.imdb.com/interfaces/). This is a sentiment analysis dataset of movie reviews with binary labels. It contains 25,000 training examples and a further 25,000 for testing. \n",
    "\n",
    "* Maas, A.L.,  Daly, R.E.,  Pham, P.T.,  Huang, D.,  Ng, A.Y. & Potts, C. (2011), \"Learning Word Vectors for Sentiment Analysis\", _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, 142-150.\n",
    "\n",
    "Your goal is to build and train an encoder-only Transformer classifier model to predict the sentiment labels from the review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare the data\n",
    "For this assignment, you will load the IMDb dataset from the TensorFlow Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'text': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to load the data and print the element_spec\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data = tfds.load(\"imdb_reviews\", split=\"train\", read_config=tfds.ReadConfig(try_autocache=False))\n",
    "test_data = tfds.load(\"imdb_reviews\", split=\"test\", read_config=tfds.ReadConfig(try_autocache=False))\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During a sleepless night, I was switching through the channels & found this embarrassment of a movie. What were they thinking?<br /><br />If this is life after \"Remote Control\" for Kari (Wuhrer) Salin, no wonder she's gone nowhere.<br /><br />And why did David Keith take this role? It's pathetic!<br /><br />Anyway, I turned on the movie near the end, so I didn't get much of the plot. But this must've been the best part. This nerdy college kid brings home this dominatrix-ish girl...this scene is straight out of the comic books -- or the cheap porn movies. She calls the mother anal retentive and kisses the father \"Oh, I didn't expect tongue!\" Great lines!<br /><br />After this, I had to see how it ended..<br /><br />Well, of course, this bitch from hell has a helluva past, so the SWAT team is upstairs. And yes...they surround her! And YES YES! The kid blows her brains out!!!! AHAHHAHAHAHA!!<br /><br />This is must-see TV. <br /><br />\n",
      "Label: 0\n",
      "\n",
      "\"Wild Tigers I have Known.\" It will only be showing in big cities, to be sure. It is one of those films SO artsy, that it makes no sense what so ever, except to the director! I HATE those! And all of those oh-so-alternative/artsy people try DESPERATELY to find \"metaphors\" in what is EVIDENT horseshit.<br /><br />There was NO plot, no story, no moral, no chronology, and nothing amusing or even touching. To me, it was a bunch of scenes thrown together that had nothing to do with one another, and were all for \"show\" to show how \"artsy\" and \"visual\" they could get. It was an ATTEMPT at yet ANOTHER teen angst film, but missed the mark on every level humanly possible. Then the credits roll! I was waiting for it to make SENSE! I was waiting for \"the good part.\" I own about 60 independent films in my DVD collection, many of which could arguably be called \"art house\" films. This will NOT be amongst them. You will be very angry at yourself for paying to see this film, much less ever buying it on DVD.\n",
      "Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View some samples\n",
    "\n",
    "for example in train_data.shuffle(100).take(2):\n",
    "    print(example['text'].numpy().decode(\"utf-8\"))\n",
    "    print(f\"Label: {example['label'].numpy()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the input sentences\n",
    "\n",
    "We will need to convert the text into integer tokens to be able to process them in the Transformer. To do this we will use a `TextVectorization` layer and adapt it to the training data. You should now complete the following function to create and prepare this layer as follows:\n",
    "\n",
    "* The function takes a `dataset` (a `tf.data.Dataset` object) as an argument, which has the same spec as `train_data` or `test_data` above. It also takes a `max_tokens` argument\n",
    "* The `TextVectorization` should be configured to use a maximum of `max_tokens` tokens (including the masking and OOV tokens)\n",
    "* It should standardize the input text by lower-casing the text and removing punctuation\n",
    "* It should split the text on whitespace\n",
    "* You should use the `adapt` method to compute the vocabulary using `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def configure_textvectorization(dataset, max_tokens):\n",
    "    \"\"\"\n",
    "    This function should create a TextVectorization layer and configure it as above.\n",
    "    The function should then return the TextVectorization layer.\n",
    "    \"\"\"\n",
    "    tv = TextVectorization(max_tokens=max_tokens\n",
    "                           ,standardize='lower_and_strip_punctuation', #default\n",
    "                           split = \"whitespace\"#default \n",
    "                           )\n",
    "    tv.adapt(dataset.map(lambda x: x[\"text\"]))\n",
    "    return tv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to create and configure the TextVectorization layer\n",
    "\n",
    "MAX_TOKENS = 20000\n",
    "text_vectorization = configure_textvectorization(train_data, max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The Three Stooges has always been some of the many actors that I have loved. I love just about every one of the shorts that they have made. I love all six of the Stooges (Curly, Shemp, Moe, Larry, Joe, and Curly Joe)! All of the shorts are hilarious and also star many other great actors and actresses which a lot of them was in many of the shorts! In My opinion The Three Stooges is some of the greatest actors ever and is the all time funniest comedy team! <br /><br />One of My favorite Stooges shorts with Shemp is none other than Husbands Beware! All appearing in this short are the beautiful Christine McIntyre, Dee Green, Doris Houck, Alyn Lockwood, Johnny Kascier, Nancy Saunders, Lu Leonard, Maxine Gates, and Emil Sitka. Green and McIntyre provide great performances here! There are so many funny parts here. This is a very hilarious short. There is another similar Three Stooges short like this one called Brideless Groom and I recommend both!', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[    2   290  4453    44   203    75    47     5     2   106   152    12\n",
      "    10    26   434    10   116    41    43   168    29     5     2  3072\n",
      "    12    35    26    91    10   116    32  1534     5     2  4453  5924\n",
      " 13237  6360  2750   886     3  5924   886    32     5     2  3072    24\n",
      "   650     3    79   332   106    81    85   152     3  1588    60     4\n",
      "   169     5    98    14     8   106     5     2  3072     8    56   657\n",
      "     2   290  4453     7    47     5     2   797   152   122     3     7\n",
      "     2    32    62  1466   220   773    13    13    29     5    56   499\n",
      "  4453  3072    17 13237     7   588    81    71  3046  6078    32  3306\n",
      "     8    11   347    24     2   300  6146 16087  5167  1423  8998     1\n",
      "     1     1  1736     1  2335     1     1  4258     1  6466     3 11024\n",
      "     1  1423     3 16087  1678    85   349   132    48    24    38   106\n",
      "   162   511   132    11     7     4    53   650   347    48     7   154\n",
      "   714   290  4453   347    39    11    29   483     1 16196     3    10\n",
      "   369   191], shape=(170,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Test your TextVectorization layer\n",
    "\n",
    "for example in train_data.shuffle(100).take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(text_vectorization(example['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m train_data\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "for example in train_data.take(1):\n",
    "    print(example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "You should now complete the following function `preprocess_dataset` which you will use to preprocess the `train_data` and `test_data` Dataset objects according to the following spec:\n",
    "\n",
    "* The function takes `dataset`, `text_vectorization_layer`, `max_seq_len`, `batch_size` and `shuffle_buffer_size` as arguments\n",
    "    * `dataset` is a `tf.data.Dataset` object with the same spec as `train_data` or `test_data` above\n",
    "* The `text_vectorization_layer` should be used to convert the text into integer tokens\n",
    "* The maximum length of the token sequences should be `max_seq_len`. Any token sequences longer than this should be truncated\n",
    "* The Datasets should return a tuple of `(tokens, label)` Tensors\n",
    "* The Datasets should be shuffled with buffer size `shuffle_buffer_size`, and then batched with `batch_size`. Note that the sequences will not be the same length, so the batches should be padded with zero masking tokens where necessary (see [the docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch))\n",
    "* The function should then return the preprocessed `dataset` Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def preprocess_dataset(dataset, text_vectorization_layer, max_seq_len, batch_size, shuffle_buffer_size):\n",
    "    \"\"\"\n",
    "    This function should preprocess the Dataset object as above.\n",
    "    The function should then return the preprocessed Dataset.\n",
    "    \"\"\"\n",
    "    #need the usual dataset form, since currently each example is an item not tuple\n",
    "    def input_target(example):\n",
    "        return example[\"text\"], example[\"label\"]\n",
    "    def tokenize_fct(text,labels):\n",
    "        return text_vectorization_layer(text), labels\n",
    "    def truncate_seq(tokens, labels):\n",
    "        return tokens[:max_seq_len], labels\n",
    "    dataset =  dataset.map(input_target).map(tokenize_fct).map(truncate_seq)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).padded_batch(batch_size)\n",
    "    return dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to preprocess the Datasets\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "train_data = preprocess_dataset(train_data, text_vectorization, MAX_SEQ_LEN, \n",
    "                                batch_size=32, shuffle_buffer_size=500)\n",
    "test_data = preprocess_dataset(test_data, text_vectorization, MAX_SEQ_LEN, \n",
    "                               batch_size=32, shuffle_buffer_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 200), dtype=int64, numpy=\n",
      "array([[2175,    7,   34, ...,    0,    0,    0],\n",
      "       [1181,   10,   39, ...,    0,    0,    0],\n",
      "       [  10,   41,  183, ...,    2,   64, 3219],\n",
      "       ...,\n",
      "       [   9,  399,   39, ...,    0,    0,    0],\n",
      "       [  10,  405,   11, ...,    0,    0,    0],\n",
      "       [ 142,    2,  584, ...,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0], dtype=int64)>)\n"
     ]
    }
   ],
   "source": [
    "# Inspect a data minibatch\n",
    "\n",
    "for example in train_data.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we pass this integer tokens Tensor through our Transformer, we will need to be careful to not use the zero padding tokens. The mechanism to handle this is masking (see [this guide](https://www.tensorflow.org/guide/keras/masking_and_padding)), and our custom layers will need to make use of this masking mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer architecture\n",
    "\n",
    "We will use an encoder-only Transformer classifier architecture for the task of sentiment prediction. This will consist of a single encoder block, followed by a classifier head.\n",
    "\n",
    "<img src=\"figures/encoder-only_transformer.png\" alt=\"Transformer\" style=\"width: 250px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encodings and Embedding layer\n",
    "\n",
    "You will now implement the input embedding and positional encoding stage of the Transformer. Your model will use the deterministic positional encoding scheme $\\mathbf{P}\\in\\mathbb{R}^{n\\times d_{model}}$ as in the original Transformer:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{ti} &= \\left\\{\n",
    "\\begin{array}{c}\n",
    "\\sin(\\omega_k t)\\quad\\text{for }i=2k+1,\\quad(\\text{for some }k\\in\\mathbb{N}_0)\\\\\n",
    "\\cos(\\omega_k t)\\quad\\text{for }i=2k+2\\quad(\\text{for some }k\\in\\mathbb{N}_0)\n",
    "\\end{array}\n",
    "\\right.\\\\\n",
    "\\omega_k &= \\frac{1}{10000^{2k/d_{model}}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t=1,\\ldots,n$ and $i=1,\\ldots,d_{model}$.\n",
    "\n",
    "You should now complete the following `positional_encodings` function to compute the positional encoding $\\mathbf{P}\\in\\mathbb{R}^{n\\times d_{model}}$.\n",
    "\n",
    "* The function takes `seq_len` (the number of time steps) and `d_model` the embedding dimension as integer arguments\n",
    "* The function should compute a 2D Tensor of shape `(seq_len, d_model)` of positional encodings according to the above equations (be careful with python's zero-indexing, the python indices are off-by-one from the mathematical indices above)\n",
    "* The function should then return the Tensor of positional encodings with type `tf.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def positional_encodings(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    This function should compute the positional encodings as above.\n",
    "    The function should then return the Tensor of positional encodings.\n",
    "    \"\"\"\n",
    "    max_wavelength = 10000.\n",
    "\n",
    "    time = np.arange(seq_len)\n",
    "    inx = np.arange(d_model)\n",
    "    #make the grids, it is done element wise\n",
    "    I, time = np.meshgrid(inx, time)\n",
    "    pe_even = np.sin(time / max_wavelength**(I/d_model))\n",
    "    pe_odd = np.cos(time / max_wavelength**(I/d_model))\n",
    "    #put them in the right positions\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, ::2] = pe_even[:, ::2]\n",
    "    pe[:, 1::2] = pe_odd[:, ::2]\n",
    "    return tf.constant(pe, dtype=tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]]),\n",
       " array([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [5, 5, 5]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = np.array([1,2,3,4,5])\n",
    "pos = np.array([1,2,3])\n",
    "np.meshgrid(pos,time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,16) into shape (10,15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run your function to get the positional encodings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#this does the positional encodings for the maximum length, we may only need subsets of this, since not all sequences will be the same length\u001b[39;00m\n\u001b[0;32m      3\u001b[0m D_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m31\u001b[39m\n\u001b[1;32m----> 4\u001b[0m pos_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mpositional_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m pos_encodings\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mpositional_encodings\u001b[1;34m(seq_len, d_model)\u001b[0m\n\u001b[0;32m     20\u001b[0m pe \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((seq_len, d_model))\n\u001b[0;32m     21\u001b[0m pe[:, ::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m pe_even[:, ::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m \u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m pe_odd[:, ::\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconstant(pe, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10,16) into shape (10,15)"
     ]
    }
   ],
   "source": [
    "# Run your function to get the positional encodings\n",
    "#this does the positional encodings for the maximum length, we may only need subsets of this, since not all sequences will be the same length\n",
    "D_MODEL = 32\n",
    "pos_encodings = positional_encodings(10, D_MODEL)\n",
    "pos_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional encodings should be added to the token embeddings in the first stage of the Transformer.\n",
    "\n",
    "You should now complete the `__init__` and `call` methods for the following custom layer `InputEmbeddings`, which builds and returns a model that converts the integer token sequence into a sequence of embeddings, and then adds positional encodings.\n",
    "\n",
    "* The initialiser takes the following required arguments:\n",
    "    * `d_model`: the dimension of the embedding vectors\n",
    "    * `pos_encodings`: the Tensor of positional encodings of shape `(max_seq_len, d_model)`, as computed by the `positional_encodings` function\n",
    "    * `max_tokens`: The maximum number of integer tokens used in the input (including the masking and OOV tokens)\n",
    "* The custom layer should create an `Embedding` layer with the correct input and output dimensions, that is set to mask incoming zero tokens. This layer should be set as the class attribute `embedding`\n",
    "* The `call` method takes a Tensor of integer tokens of shape `(batch_size, n)` as input, where `n` is the maximum sequence length in the batch\n",
    "* The `call` method should use the `Embedding` lookup layer to convert the inputs to embedding vectors, and return the sum of the embedding vectors and positional encodings\n",
    "\n",
    "_NB: The custom layer also implements the `compute_mask` method, which has been completed for you. This is a method should be implemented whenever a layer should produce a mask (see [this guide](https://www.tensorflow.org/guide/keras/masking_and_padding) for more information). Note that this method references `self.embedding`. In this instance we want to pass on the same mask that is produced by the `Embedding` layer, so the `compute_mask` method simply calls the existing method from the `Embedding` layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following class.\n",
    "# Make sure not to change the class or methods name or arguments.\n",
    "\n",
    "class InputEmbeddings(Layer):\n",
    "    \"\"\"\n",
    "    This custom layer should take a batch of integer tokens as input, \n",
    "    converts them into embeddings and adds the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, pos_encodings, max_tokens, name='input_embeddings', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.pos_encodings = pos_encodings\n",
    "        #output in dmodel\n",
    "        self.embedding = Embedding(input_dim = max_tokens, output_dim = d_model, mask_zero = True)\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.embedding.compute_mask(inputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs is an integer Tensor of shape (batch_size, n), where n is \n",
    "        the maximum sequence length in this batch (n \\le max_seq_len)\n",
    "        \"\"\"       \n",
    "        input_embedding = self.embedding(inputs)\n",
    "        #take the positional encodings we neeed\n",
    "        n = inputs.shape[-1]\n",
    "        return input_embedding + self.pos_encodings[:n,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom layer\n",
    "\n",
    "input_embeddings = InputEmbeddings(D_MODEL, pos_encodings, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your custom layer on an input\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    h = input_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 200), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our custom layer is producing a mask\n",
    "\n",
    "h._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder block\n",
    "\n",
    "You will now implement the encoder block of the Transformer model. This block consists of a multi-head attention block with a residual connection followed by layer normalisation, and then a pointwise feedforward network with residual connection again followed by layer normalisation.\n",
    "\n",
    "The multi-head attention block will need to account for the masking corresponding to the zero padding tokens in the input. The way the `MultiHeadAttention` layer handles this is through the `attention_mask` argument when it is called. \n",
    "\n",
    "The incoming mask is a boolean Tensor with shape `(batch_size, seq_len)`, where `seq_len` is the length of the sequence of embedding vectors being input to the `MultiHeadAttention` layer. The multi-head attention is performing self-attention, so the shape of the mask required by the `attention_mask` will be `(batch_size, seq_len, seq_len)`.\n",
    "\n",
    "Before implementing the encoder block, you should complete the following function `get_attention_mask`, which takes a single argument `mask`, which is a boolean Tensor of shape `(batch_size, seq_len)`, or `None`. If `mask` is `None`, then this function should return `None`. Otherwise, the function should return a boolean Tensor of shape `(batch_size, seq_len, seq_len)` which will be used by the `MultiHeadAttention` layer.\n",
    "\n",
    "For a single example in the batch, suppose the vector mask $\\mathbf{m}\\in\\mathbb{R}^n$, where $n$ is the sequence length. We would like to convert this vector mask to a matrix mask $\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$, where the $i,j$-th element $M_{ij}$ is given by the element $\\min (i,j)$ of the vector $\\mathbf{m}$.\n",
    "\n",
    "For example, if the incoming boolean mask was as follows:\n",
    "\n",
    "```\n",
    "mask = [[True, True, False],\n",
    "        [True, False, False]]\n",
    "```\n",
    "\n",
    "where the batch size is 2 and the sequence length is 3, then the mask returned by the function `get_attention_mask` should look like:\n",
    "\n",
    "```\n",
    "returned_mask = [[[True, True, False],\n",
    "                  [True, True, False],\n",
    "                  [False, False, False]],\n",
    "                 [True, False, False],\n",
    "                 [False, False, False],\n",
    "                 [False, False, False]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True]\n",
      "  [ True]\n",
      "  [False]]\n",
      "\n",
      " [[ True]\n",
      "  [False]\n",
      "  [False]]]\n",
      "[[[ True  True False]]\n",
      "\n",
      " [[ True False False]]]\n",
      "[[[ True  True False]\n",
      "  [ True  True False]\n",
      "  [False False False]]\n",
      "\n",
      " [[ True False False]\n",
      "  [False False False]\n",
      "  [False False False]]]\n"
     ]
    }
   ],
   "source": [
    "mask = np.array([[True,True,False],\n",
    "        [True, False, False]])\n",
    "print(mask[:,:,None])\n",
    "print(mask[:,None,:])\n",
    "print(mask[:,:,None]&mask[:,None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_attention_mask(mask=None):\n",
    "    \"\"\"\n",
    "    This function should compute the attention mask as described above.\n",
    "    The function should then return the boolean Tensor.\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        return mask\n",
    "    mask1 = mask[:, :, None]\n",
    "    mask2 = mask[:, None, :]\n",
    "    return mask1 & mask2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 3), dtype=bool, numpy=\n",
       "array([[[ True,  True, False],\n",
       "        [ True,  True, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[ True, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your mask function\n",
    "input_mask = tf.constant([[True, True, False], [True, False, False]])\n",
    "get_attention_mask(input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should complete the following `EncoderBlock` custom layer, that implements the encoder block. \n",
    "\n",
    "* The initialiser takes the following required arguments:\n",
    "    * `num_heads`: the number of attention heads to use in the multi-head attention\n",
    "    * `key_dim`: the key (and query and value) dimension to use in the multi-head attention\n",
    "    * `d_model`: the embedding dimension of the Transformer\n",
    "    * `ff_dim`: the width of the hidden layer in the feedforward network in the encoder block\n",
    "* The initialiser sets `self.support_masking = True` (this has been done for you), so that the incoming boolean mask will also be included in the output of the `EncoderBlock` layer\n",
    "* The `EncoderBlock` layer should create `MultiHeadAttention`, `LayerNormalization` and `Dense` layers in the initializer as required \n",
    "    * The feedforward network should have one hidden layer of size `ff_dim` with a ReLU activation\n",
    "    * The output layer of the feedforward network should have size `d_model` and no activation\n",
    "* The operation of the custom layer is as follows:\n",
    "    * The input to the layer is a Tensor of shape `(batch_size, seq_len, d_model)`\n",
    "    * The call method also takes a `mask` argument, which will be a boolean mask of shape `(batch_size, seq_len)`. The call method should use your `get_attention_mask` function to compute the attention mask\n",
    "    * Pass the input through the `MultiHeadAttention` layer (performing self-attention), passing the attention mask in the `attention_mask`. Add the resulting Tensor output to the input (residual connection) and pass through a `LayerNormalization` layer\n",
    "    * Pass the resulting Tensor $h$ through the feedforward network, add this to $h$ (residual connection) and pass through a `LayerNormalization` layer\n",
    "    * The custom layer should then return the resulting Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following class.\n",
    "# Make sure not to change the class or methods name or arguments.\n",
    "\n",
    "class EncoderBlock(Layer):\n",
    "    \"\"\"\n",
    "    This custom layer should take a Tensor of shape (batch_size, seq_len, d_model) as input.\n",
    "    It should carry out the operations as described above and return the resulting Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, key_dim, d_model, ff_dim, name='encoder_block', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.supports_masking = True  # This will pass on any incoming mask\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, key_dim)\n",
    "        self.ff = Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        #one for each layernormalisation point\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        \n",
    "    #mask is passed automatically in keras calling\n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\"\n",
    "        inputs is a Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"    \n",
    "        attention_mask = get_attention_mask(mask)\n",
    "        h = self.multihead_attention(inputs,inputs, attention_mask = attention_mask)\n",
    "        h = self.layernorm1(h + inputs)\n",
    "        h_ff = self.ff(h)\n",
    "        return self.layernorm2(h + h_ff)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EncoderBlock instance\n",
    "\n",
    "encoder_block = EncoderBlock(num_heads=2, key_dim=16, d_model=32, ff_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your layer on a dummy input\n",
    "\n",
    "inputs = tf.random.normal((16, 200, 32))\n",
    "h = encoder_block(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your layer on the output from the input_embeddings layer\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    h = input_embeddings(tokens)\n",
    "    h = encoder_block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 200), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the mask has been propagated\n",
    "\n",
    "h._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier head\n",
    "\n",
    "The final stage of our Transformer model is the classifier head. This stage consists of the following layers:\n",
    "\n",
    "* A `GlobalAveragePooling1D` layer, that takes an incoming Tensor of shape `(batch_size, seq_len, d_model)` and reduces out the time axis to produce a Tensor of shape `(batch_size, d_model)`\n",
    "* A dropout layer\n",
    "* A dense layer with ReLU activation\n",
    "* A dropout layer\n",
    "* A dense layer with a single neuron output and sigmoid activation function\n",
    "\n",
    "The final dense layer outputs the probability of a positive sentiment label.\n",
    "\n",
    "You should now complete the following `get_classifier_head` function, which takes the arguments `d_model`, `dropout_rate` and `units`. The function should build and return a Sequential Model object, according to the above specification, where `dropout_rate` is used in both `Dropout` layers and `units` is used to define the width of the intermediate `Dense` layer. The `d_model` input should be used to set the `input_shape` in the first layer.\n",
    "\n",
    "Note that the [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer will automatically use the incoming mask when used in the `Sequential` model, see [the guide](https://www.tensorflow.org/guide/keras/masking_and_padding) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_classifier_head(d_model, dropout_rate, units):\n",
    "    \"\"\"\n",
    "    This function should compute classifier head model as described above.\n",
    "    The function should then return the Model object.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        GlobalAveragePooling1D(input_shape=(None, d_model)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the classifier head\n",
    "\n",
    "classifier_head = get_classifier_head(D_MODEL, dropout_rate=0.1, units=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 681\n",
      "Trainable params: 681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "classifier_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n",
       "array([[0.4942952 ],\n",
       "       [0.5062677 ],\n",
       "       [0.517485  ],\n",
       "       [0.50191146],\n",
       "       [0.51002496],\n",
       "       [0.510962  ],\n",
       "       [0.50928396],\n",
       "       [0.47465256]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your classifier head model on a dummy input\n",
    "\n",
    "inputs = tf.random.normal((8, 200, D_MODEL))\n",
    "classifier_head(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Transformer classifier\n",
    "\n",
    "We now have all the components to build the complete Transformer classifier. You should now complete the following function `get_transformer_classifier` to build and compile the model.\n",
    "\n",
    "The function takes the arguments `input_embeddings_layer`, `encoder_block_layer` and `classifier_head_layer`. It should use these layers to build a `Sequential` model, and compile it with a suitable loss function and optimizer, and an accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_transformer_classifier(input_embeddings_layer, encoder_block_layer, classifier_head_layer):\n",
    "    \"\"\"\n",
    "    This function should compute classifier head model as described above.\n",
    "    The function should then return the Model object.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        input_embeddings_layer,\n",
    "        encoder_block_layer,\n",
    "        classifier_head_layer\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the compiled Transformer classifier model\n",
    "\n",
    "input_embeddings = InputEmbeddings(D_MODEL, pos_encodings, MAX_TOKENS)\n",
    "encoder_block = EncoderBlock(num_heads=2, key_dim=16, d_model=32, ff_dim=32)\n",
    "classifier_head = get_classifier_head(D_MODEL, dropout_rate=0.1, units=20)\n",
    "transformer = get_transformer_classifier(input_embeddings, encoder_block, classifier_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Transformer classifier model\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    outputs = transformer(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 19s 22ms/step - loss: 0.5170 - accuracy: 0.7080 - val_loss: 0.3479 - val_accuracy: 0.8480\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.2515 - accuracy: 0.9009 - val_loss: 0.3571 - val_accuracy: 0.8554\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_data, validation_data=test_data, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unlabelled data\n",
    "\n",
    "The IMDB dataset also contains a split without labels. The following cell loads this dataset split and applies a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_data = tfds.load(\"imdb_reviews\", split=\"unsupervised\", \n",
    "                              read_config=tfds.ReadConfig(try_autocache=False))\n",
    "unsupervised_data = unsupervised_data.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie is pretty awful. it lacks suspense and is all around boring and worthless. i connected with none of the characters and now as i think back, i can't even remember how it ends. don't waste your time w/ it. actually i think why i don't remember the end is because i fast forwarded to the end, i do that sometimes, so i can say i've seen a film but i don't have to actually sit and watch the whole thing, i usually get the gist though. it's a bad habit really, if it's getting too long or i'm just feeling too fidgety or it's boring i'll fast forward. i always feel a little guilty when i do it though. like i'm disrespecting the director, and i am, but i've got other things to do than watch a crappy film. now mind you i've only done this a few times and this film happens to be on my 'sucky' film list.\n",
      "\n",
      "Transformer probability of positive label: 0.0049238489009439945\n"
     ]
    }
   ],
   "source": [
    "for example in unsupervised_data.take(1):\n",
    "    print(example['text'].numpy().decode(\"utf-8\"))\n",
    "    tokens = text_vectorization(example['text'])\n",
    "    tokens = tokens[tf.newaxis, :MAX_SEQ_LEN]  # Add dummy batch dimension and truncate\n",
    "    prob = transformer(tokens).numpy().squeeze()\n",
    "    print(f\"\\nTransformer probability of positive label: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented and trained an encoder-only Transformer classifier model for the task of sentiment prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
