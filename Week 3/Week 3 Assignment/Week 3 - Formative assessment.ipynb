{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4415b31e",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 3: Loss functions and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04209c02",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to train an MLP model with both the high-level Keras API and a custom training loop, using the automatic differentiation tools from TensorFlow and a custom loss function.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c426b102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\University-local\\Imperial\\Term 2\\Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# If you would like to make further imports from Tensorflow, add them here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e208f",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"figures/adelie.jpg\" title=\"AdÃ©lie\" style=\"width: 275px;\"/> </td>\n",
    "<td> <img src=\"figures/chinstrap.jpg\" title=\"Chinstrap\" style=\"width: 275px;\"/> </td>\n",
    "    <td> <img src=\"figures/gentoo.jpg\" title=\"Gentoo\" style=\"width: 275px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<center><font style=\"font-size:12px\">source: <a href=https://en.wikipedia.org/wiki/Penguin>wikipedia</a></font></center>\n",
    "\n",
    "#### The Palmer Penguins dataset\n",
    "In this formative assessment, you will use the [Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html). These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the [Palmer Station Long Term Ecological Research Program](https://pal.lternet.edu/), part of the [US Long Term Ecological Research Network](https://lternet.edu/). The dataset consists of measurements for three penguin species observed in the Palmer Archipelago, Antarctica.\n",
    "\n",
    "* Gorman, K.B., Williams, T.D. & Fraser, W.R. (2014), \"Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis)\", PLoS ONE **9** (3):e90081, https://doi.org/10.1371/journal.pone.0090081\n",
    "\n",
    "Your goal is to model the dataset using an MLP network, trained using the automatic differentiation tools in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df944d5",
   "metadata": {},
   "source": [
    "#### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72588990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body Mass (g)</th>\n",
       "      <th>Clutch Completion</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Culmen Depth (mm)</th>\n",
       "      <th>Culmen Length (mm)</th>\n",
       "      <th>Date Egg</th>\n",
       "      <th>Delta 13 C (o/oo)</th>\n",
       "      <th>Delta 15 N (o/oo)</th>\n",
       "      <th>Flipper Length (mm)</th>\n",
       "      <th>Individual ID</th>\n",
       "      <th>Island</th>\n",
       "      <th>Region</th>\n",
       "      <th>Sample Number</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Species</th>\n",
       "      <th>Stage</th>\n",
       "      <th>studyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>3350.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>11/22/09</td>\n",
       "      <td>-23.90309</td>\n",
       "      <td>8.96436</td>\n",
       "      <td>189.0</td>\n",
       "      <td>N64A1</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>119</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>PAL0910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5850.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.6</td>\n",
       "      <td>48.4</td>\n",
       "      <td>11/29/07</td>\n",
       "      <td>-25.48025</td>\n",
       "      <td>7.82080</td>\n",
       "      <td>213.0</td>\n",
       "      <td>N37A2</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>14</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>PAL0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>5500.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>48.1</td>\n",
       "      <td>11/9/09</td>\n",
       "      <td>-26.22664</td>\n",
       "      <td>8.45738</td>\n",
       "      <td>209.0</td>\n",
       "      <td>N29A2</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>110</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>PAL0910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>42.4</td>\n",
       "      <td>12/3/07</td>\n",
       "      <td>-24.68790</td>\n",
       "      <td>9.35138</td>\n",
       "      <td>181.0</td>\n",
       "      <td>N73A1</td>\n",
       "      <td>Dream</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>21</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>PAL0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>4875.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>47.5</td>\n",
       "      <td>11/25/09</td>\n",
       "      <td>-26.23613</td>\n",
       "      <td>8.12691</td>\n",
       "      <td>212.0</td>\n",
       "      <td>N14A1</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>Anvers</td>\n",
       "      <td>89</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Adult, 1 Egg Stage</td>\n",
       "      <td>PAL0910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Body Mass (g) Clutch Completion Comments  Culmen Depth (mm)  \\\n",
       "285         3350.0               Yes      NaN               17.0   \n",
       "85          5850.0               Yes      NaN               14.6   \n",
       "186         5500.0               Yes      NaN               15.1   \n",
       "73          3600.0               Yes      NaN               17.3   \n",
       "275         4875.0               Yes      NaN               14.0   \n",
       "\n",
       "     Culmen Length (mm)  Date Egg  Delta 13 C (o/oo)  Delta 15 N (o/oo)  \\\n",
       "285                35.7  11/22/09          -23.90309            8.96436   \n",
       "85                 48.4  11/29/07          -25.48025            7.82080   \n",
       "186                48.1   11/9/09          -26.22664            8.45738   \n",
       "73                 42.4   12/3/07          -24.68790            9.35138   \n",
       "275                47.5  11/25/09          -26.23613            8.12691   \n",
       "\n",
       "     Flipper Length (mm) Individual ID     Island  Region  Sample Number  \\\n",
       "285                189.0         N64A1  Torgersen  Anvers            119   \n",
       "85                 213.0         N37A2     Biscoe  Anvers             14   \n",
       "186                209.0         N29A2     Biscoe  Anvers            110   \n",
       "73                 181.0         N73A1      Dream  Anvers             21   \n",
       "275                212.0         N14A1     Biscoe  Anvers             89   \n",
       "\n",
       "        Sex    Species               Stage studyName  \n",
       "285  FEMALE     Adelie  Adult, 1 Egg Stage   PAL0910  \n",
       "85     MALE     Gentoo  Adult, 1 Egg Stage   PAL0708  \n",
       "186    MALE     Gentoo  Adult, 1 Egg Stage   PAL0910  \n",
       "73   FEMALE  Chinstrap  Adult, 1 Egg Stage   PAL0708  \n",
       "275  FEMALE     Gentoo  Adult, 1 Egg Stage   PAL0910  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to load and sample the data\n",
    "\n",
    "df = pd.read_csv(Path(\"./data/penguins.csv\"))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdf591",
   "metadata": {},
   "source": [
    "We will work the following columns from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c04f998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of columns to use as input features from the DataFrame\n",
    "\n",
    "input_cols = ['Body Mass (g)', 'Culmen Depth (mm)', 'Culmen Length (mm)', 'Flipper Length (mm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c941a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the column to use for the target variable\n",
    "\n",
    "target_col = ['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c6f93",
   "metadata": {},
   "source": [
    "We will also use the `MinMaxScaler` from `sklearn` to scale the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc26098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMax Scaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb8dbb",
   "metadata": {},
   "source": [
    "You should now complete the following `get_inputs_and_targets` function, according to the following specifications:\n",
    "\n",
    "* The function takes `dataframe`, `input_columns`, `target_column`, `minmaxscaler` as arguments\n",
    "* Extract the inputs and target columns from the loaded DataFrame using `input_columns` and `target_column` lists\n",
    "* Remove any rows with `NaN` values\n",
    "* Scale the input features to the range $[0, 1]$ using the `minmaxscaler`\n",
    "* The function should then return a tuple of constant `tf.Tensor` objects `(input_variables, target_variable)`\n",
    "  * `input_variables` should be of type `tf.float32`, with shape `(num_examples, num_features)` \n",
    "  * `target_variable` should be of type `tf.string`, with shape `(num_examples,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7aa41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_inputs_and_targets(dataframe, input_columns, target_column, minmaxscaler):\n",
    "    \"\"\"\n",
    "    This function takes in the loaded DataFrame and column lists as above, and a\n",
    "    MinMaxScaler object. The function should extract the input and target features as \n",
    "    above, and return a tuple (input_variables, target_variable) of Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe = dataframe[target_column + [ i for i in input_columns]].dropna()\n",
    "    input_cols = dataframe[input_columns]\n",
    "    target_col = dataframe[target_column]\n",
    "    scaler.fit(input_cols)\n",
    "    input_cols = scaler.transform(input_cols)\n",
    "    input_cols = tf.constant(input_cols, dtype = tf.float32)\n",
    "    target_col = tf.constant(target_col, dtype = tf.string)\n",
    "    target_col = tf.squeeze(target_col)\n",
    "    \n",
    "    return (input_cols, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "235025a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342,)\n"
     ]
    }
   ],
   "source": [
    "# Run your function to get the input and target Tensors\n",
    "\n",
    "X, y = get_inputs_and_targets(df, input_cols, target_col, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e622b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41be4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into tf.data.Dataset objects\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2dd4cb",
   "metadata": {},
   "source": [
    "The target variable needs to be further processed to convert the string labels to integer labels to train our model.\n",
    "\n",
    "You should now complete the following `get_dataset` function according to the following specifications:\n",
    "\n",
    "* The function takes `data` as an argument, which is a tuple of numpy arrays `(inputs, targets)`\n",
    "* The training and test data should be loaded into a `tf.data.Dataset` object\n",
    "* The `get_dataset` function should contain a nested function that is used in the `map` method of the Dataset to process the targets\n",
    "  * The string targets should be converted to integer labels according to the following mapping:</br>\n",
    "  `{\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2}`\n",
    "* The resulting Dataset should return a tuple of `(inputs, targets)` Tensors, of types `tf.float32` and `tf.int32` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e7394be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_dataset(data):\n",
    "    \"\"\"\n",
    "    This function takes a tuple of numpy arrays, and creates a tf.data.Dataset\n",
    "    object according to the above description.\n",
    "    The function should then return the Dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    def convert_label(inputs,target):\n",
    "        if target == \"Adelie\":\n",
    "            target = 0\n",
    "        elif target == \"Chinstrap\":\n",
    "            target = 0\n",
    "        else:\n",
    "            target = 2\n",
    "        return inputs,target\n",
    "    return dataset.map(convert_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c526825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(4,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the training and test Datasets and print the element_spec\n",
    "\n",
    "train_ds = get_dataset(data=(X_train, y_train))\n",
    "test_ds = get_dataset(data=(X_test, y_test))\n",
    "\n",
    "print(train_ds)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75483d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, batch and prefetch the Datasets\n",
    "\n",
    "train_ds = train_ds.shuffle(X_train.shape[0]).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.shuffle(X_test.shape[0]).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0ef8c",
   "metadata": {},
   "source": [
    "#### MLP model\n",
    "\n",
    "You should now complete the following `get_model` function to build the MLP model we will use to train on the Palmer Penguins dataset.\n",
    "\n",
    "* The function takes `hidden_units`, `output_units`, `input_shape`, `rate` as arguments\n",
    "* You should build the model using the `Sequential` API\n",
    "* `hidden_units` is a list of integers, specifying the width of the hidden layers within the model\n",
    "  * Each hidden layer should use a sigmoid activation function\n",
    "  * Each fully connected layer should be followed by a batch normalization layer, and then a dropout layer with dropout rate equal to `rate`\n",
    "* The first layer in the model should set the input shape using the `input_shape` argument\n",
    "* `output_units` is an integer specifying the number of neurons in the final output layer\n",
    "  * The final output layer should not use an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a08142e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_model(hidden_units, output_units, input_shape, rate):\n",
    "    \"\"\"\n",
    "    This function should create an MLP model according to the above description.\n",
    "    The function should then return the model.\n",
    "    \"\"\"\n",
    "    penguin_model = Sequential()\n",
    "    for i, neurons in enumerate(hidden_units):\n",
    "        if i == 0:\n",
    "            penguin_model.add(Dense(neurons, activation = \"sigmoid\", input_shape = input_shape))\n",
    "            \n",
    "        else:\n",
    "            penguin_model.add(Dense(neurons, activation='sigmoid'))\n",
    "        penguin_model.add(BatchNormalization())\n",
    "        penguin_model.add(Dropout(rate=rate))\n",
    "    #finally, deal with output layer :\n",
    "    if len(hidden_units) == 0:\n",
    "        penguin_model.add(Dense(output_units, input_shape=input_shape))\n",
    "    else:\n",
    "        penguin_model.add(Dense(output_units))\n",
    "    return penguin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0362ea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 10)                50        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273 (1.07 KB)\n",
      "Trainable params: 233 (932.00 Byte)\n",
      "Non-trainable params: 40 (160.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use your function to create a model and print the summary\n",
    "\n",
    "model = get_model(hidden_units=[10, 10], output_units=3, input_shape=(4,), rate=0.8)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe24fbf",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "We will train the model using a categorical cross entropy loss function. Since the final layer in the model does not use an activation function, it is returning the logits to be used in the computation of the loss function.\n",
    "\n",
    "The categorical cross entropy for a single data example $(x, y)$ is given by:\n",
    "\n",
    "$$\n",
    "l(y, \\hat{y}) = -\\sum_{j=1}^C y_{j} \\log \\hat{y}_{j},\\tag{1}\n",
    "$$\n",
    "\n",
    "where $C$ is the number of classes (in our case $C=3$), $y, \\hat{y}\\in\\mathbb{R}^C$, and $\\hat{y}_{j}$ is equal to the probability of the label $j$ as predicted by our neural network $f_\\theta$ with parameters $\\theta$, given the input $x$. In the above formulation the target label $y$ is represented as a one-hot vector. In our case, $y$ will be length three with two zeros and a single 1 in the place of the correct label.\n",
    "\n",
    "Note also that the our model defined above outputs logits $z_j$, not probabilities. The probabilities are computed using the softmax function:\n",
    "\n",
    "$$\n",
    "\\hat{y_j} = \\frac{\\exp(z_j)}{\\sum_{k=1}^3 \\exp(z_k)}.\n",
    "$$\n",
    "\n",
    "The loss function we want to minimise is the categorical cross entropy \\eqref{cce} averaged over all examples in the training data. In practice, we will estimate this loss function by sampling minibatches of data and computing the average categorical cross entropy over the minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74634f",
   "metadata": {},
   "source": [
    "You should now complete the following `loss_function` function, to compute the categorical cross entropy loss as above.\n",
    "\n",
    "In TensorFlow, loss functions have the signature `loss(y_true, y_pred)`, where `y_true` is the ground truth Tensor and `y_pred` is the model prediction given the inputs. The `compute_loss` function follows this signature, so we would be able to pass it to the `loss` argument directly when calling `model.compile`.\n",
    "\n",
    "* The function takes `y_true` and `y_pred` as arguments\n",
    "  * `y_true` is a batch of ground truth inputs, of shape `(num_examples,)` and type `tf.int32`\n",
    "  * `y_pred` is a batch of model predictions, of shape `(num_examples, 3)` and type `tf.float32`\n",
    "* The function should compute the categorical cross entropy as above\n",
    "  * Bear in mind that `y_pred` will be a batch of logits, not probabilities\n",
    "  * `y_true` contains the integer-encoded labels (either 0, 1 or 2)\n",
    "* The function should average the categorical cross entropy over the minibatch, and return the result as a scalar Tensor\n",
    "\n",
    "_Hint: you might find the functions [`tf.math.reduce_logsumexp`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp) and [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) useful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function should compute the categorical cross entropy loss as described above.\n",
    "    The function should return a scalar Tensor with the computed loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a ground truth and predictions Tensor to test your function\n",
    "\n",
    "inputs, y_true = next(iter(train_ds))\n",
    "y_pred = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d34d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss on the batch of data using your function\n",
    "\n",
    "loss_function(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see that your computed loss agrees with the built-in TensorFlow function\n",
    "\n",
    "tf.reduce_mean(tf.keras.metrics.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4d44a",
   "metadata": {},
   "source": [
    "#### Train your model with the high-level Keras API\n",
    "\n",
    "You should now complete the following `train_model_keras` function to train the MLP model using the high-level Keras API.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `opt`, `training_dataset` and `epochs` as arguments\n",
    "* The function should use the high-level Keras API to compile and train the model\n",
    "  * Use the `compile` method to compile `mlp_model` using the loss function `loss_fn`, `opt` optimizer and accuracy metric\n",
    "  * Train with the `fit` method, using `training_dataset` for `epochs` epochs\n",
    "* The function should then return the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model_keras(mlp_model, loss_fn, opt, training_dataset, epochs):\n",
    "    \"\"\"\n",
    "    This function should use the compile and fit methods to train the MLP model.\n",
    "    The function should return the history from the training.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7270c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SGD optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84685aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the MLP model\n",
    "\n",
    "model = get_model(hidden_units=[40, 20], output_units=3, input_shape=(4,), rate=0.5)\n",
    "history = train_model_keras(model, loss_fn=loss_function, opt=optimizer, \n",
    "                            training_dataset=train_ds, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24e93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be267cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c4b9d",
   "metadata": {},
   "source": [
    "#### Train your model with a custom training loop\n",
    "\n",
    "You will now implement a custom training loop to train an MLP model on the Palmer Penguins dataset, making use of the automatic differentiation tools in TensorFlow.\n",
    "\n",
    "First you should complete the following `train_step` function, which will implement the core operations of computing the loss and gradients, and updating the model parameters.\n",
    "\n",
    "* The function takes the arguments `mlp_model`, `loss_fn`, `opt` and `train_batch`\n",
    "* `train_batch` is a tuple of `(inputs, targets)` Tensors yielded from the Dataset\n",
    "* The function should compute the batch loss using `train_batch`, `mlp_model` and `loss_fn`\n",
    "  * The model should be run in training mode (see [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#call))\n",
    "* It should then compute the gradients and update the model parameters using the optimizer `opt`\n",
    "* It should return a tuple of three Tensors: `(loss, y_true, y_pred)`\n",
    "  * `loss` is the scalar batch loss as computed by `loss_fn`\n",
    "  * `y_true` is the ground truth Tensor for the batch\n",
    "  * `y_pred` is the model predictions Tensor for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "@tf.function\n",
    "def train_step(mlp_model, loss_fn, opt, train_batch):\n",
    "    \"\"\"\n",
    "    This function should perform the update step as described above.\n",
    "    The function should return a tuple of Tensors (loss, y_true, y_pred).\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f02162",
   "metadata": {},
   "source": [
    "You should now complete the following `train_model_custom` function to perform the custom training loop. We will use two metric objects (defined below) to record the loss and accuracy values over the course of training. See the docs for the base [`Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) class to see the generic methods that are available.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `opt`, `training_dataset`, `train_step_fn`, `epochs`, `loss_metric` and `accuracy_metric` as arguments\n",
    "* The custom training loop should consist of an outer loop for the epochs, that runs for `epochs` number of times\n",
    "* At the start of each epoch, the metric states should be reset using the `reset_state` method\n",
    "* Within each epoch, the function should loop over `training_dataset` to pull batches of data\n",
    "* For each batch, it should use `train_step_fn` to update the model parameters\n",
    "  * This function returns a tuple of Tensors `(loss, y_true, y_pred)`\n",
    "  * For each batch, the metrics should also be updated, using the `update_state` method\n",
    "* The average loss and accuracy over each epoch should each be stored in a list of floats\n",
    "  * The average loss and accuracy can be retrieved from the metrics at the end of the epoch using the `result` method\n",
    "* The function should return a tuple of the two lists `(epoch_losses, epoch_acc)` for average loss and accuracy scores per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388df053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss and accuracy metrics and optimizer\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a722382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def train_model_custom(mlp_model, loss_fn, opt, training_dataset, train_step_fn, epochs, \n",
    "                       loss_metric=loss_metric, accuracy_metric=accuracy_metric):\n",
    "    \"\"\"\n",
    "    This function should run the custom training loop as described above.\n",
    "    The function should return a tuple of two lists with the loss and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to run the custom training loop\n",
    "\n",
    "model = get_model(hidden_units=[40, 20], output_units=3, input_shape=(4,), rate=0.5)\n",
    "epoch_losses, epoch_acc = train_model_custom(model, loss_fn=loss_function, opt=optimizer,\n",
    "                                             training_dataset=train_ds, \n",
    "                                             train_step_fn=train_step, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_acc)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab0ad8",
   "metadata": {},
   "source": [
    "#### Evaluate your model\n",
    "\n",
    "Finally, you will also implement custom code to evaluate your model. First you should complete the following `test_step` function, which is similar to the `train_step` function, except that it does not compute gradients or update the model parameters.\n",
    "\n",
    "* The function takes the arguments `mlp_model`, `loss_fn` and `test_batch`\n",
    "* `test_batch` is a tuple of `(inputs, targets)` Tensors yielded from the Dataset\n",
    "* The function should compute the batch loss using `test_batch`, `mlp_model` and `loss_fn`\n",
    "  * The model should be run in inference mode (see [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#call))\n",
    "* The function should return a tuple of three Tensors: `(loss, y_true, y_pred)`\n",
    "  * `loss` is the scalar batch loss as computed by `loss_fn`\n",
    "  * `y_true` is the ground truth Tensor for the batch\n",
    "  * `y_pred` is the model predictions Tensor for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "@tf.function\n",
    "def test_step(mlp_model, loss_fn, test_batch):\n",
    "    \"\"\"\n",
    "    This function should perform the evaluation step as described above.\n",
    "    The function should return a tuple of Tensors (loss, y_true, y_pred).\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482a2c6",
   "metadata": {},
   "source": [
    "Now you should complete the following `test_model_custom` function that will evaluate the model on a test dataset. This will be similar to the `train_model_custom` function, except that no optimizer is used/needed and no parameter updates are made.\n",
    "\n",
    "* The function takes `mlp_model`, `loss_fn`, `test_dataset`, `test_step_fn`, `loss_metric` and `accuracy_metric` as arguments\n",
    "* The evaluation should make one complete iteration loop through `test_dataset`\n",
    "* At the start of the loop, the metric states should be reset using the `reset_state` method\n",
    "* For each batch, you should use `test_step_fn` to compute the loss and model prediction\n",
    "  * This function returns a tuple of Tensors `(loss, y_true, y_pred)`\n",
    "  * For each batch, the metrics should also be updated, using the `update_state` method\n",
    "* The average loss and accuracy should be retrieved from the metrics at the end of the loop using the `result` method\n",
    "* The function should return a tuple of two floats `(avg_loss, avg_acc)` for average loss and accuracy scores over the `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7544a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def test_model_custom(mlp_model, loss_fn, test_dataset, test_step_fn, \n",
    "                      loss_metric=loss_metric, accuracy_metric=accuracy_metric):\n",
    "    \"\"\"\n",
    "    This function should run the custom evaluation loop as described above.\n",
    "    The function should return a tuple of two floats for the loss and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to evaluate the model\n",
    "\n",
    "avg_loss, avg_acc = test_model_custom(model, loss_function, test_ds, test_step)\n",
    "print(f\"Test loss: {avg_loss}\")\n",
    "print(f\"Test accuracy: {avg_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4d204",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now written custom code to implement a loss function, training loop and evaluation loop for an MLP model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
