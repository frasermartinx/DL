{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Week 9: Normalising flows II: NICE, RealNVP & Glow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. NICE / RealNVP](#nicerealnvp)\n",
    "\n",
    "[3. Bijector subclassing (\\*)](#bijector_subclassing)\n",
    "\n",
    "[4. Glow](#glow)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In the last week of the course, we introduced the setting for normalising flows, which we used for the problem of density estimation. We saw that the change of variables formula is central to the training of normalising flow models, as it allows us to compute the data likelihood. This also demonstrated that in practical implementations, normalising flow models require two key properties: invertibility, and efficient computation of the Jacobian determinant.\n",
    "\n",
    "We saw that autoregressive models are one example of normalising flows. These types of models are capable of modelling highly complex data distributions, but come with the potential drawback that running these models at test time can be very slow in practice. \n",
    "\n",
    "This week, we'll be going through another popular class of normalising flow models, which are known as the NICE, RealNVP and Glow models. This class of models can actually viewed as a special case of autoregressive models. In practice, they circumvent the problem of the slow inference of autoregressive models.\n",
    "\n",
    "We will again see how these models can be implemented using TensorFlow and TensorFlow Probability, and also look at how we can create our own subclassed Bijectors, so that we have full flexibility when developing normalising flow models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"nicerealnvp\"></a>\n",
    "## NICE / RealNVP\n",
    "\n",
    "#### NICE\n",
    "NICE stands for \"nonlinear independent components estimation\", and is a deep learning architecture framework for density estimation tasks. A key motivation for the proposed framework given in the abstract of the original paper ([Dinh 2015](#Dinh15)) is as follows:\n",
    "\n",
    "> It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables.\n",
    "\n",
    "As with many normalising flow examples, a typical choice for a base distribution would be an isotropic Gaussian, which is then transformed by the deep learning model. An important aspect is the efficient calculation of the Jacobian determinant of the transformation. \n",
    "\n",
    "In this section, we will describe the NICE architecture, and the RealNVP architecture that is built upon it. We will follow the exposition of the original papers, and think of the forward transformation as acting on the data input example. Note however that this is in contrast to the usual bijector convention of using the forward transformation for sampling, and the inverse transformation for computing log probs.\n",
    "\n",
    "#### Affine coupling layer\n",
    "The basic building block of the NICE architecture is the affine coupling layer. Given an input $\\mathbf{x}\\in\\mathbb{R}^D$, we split it into two blocks $(\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$, where $d<D$ (usually $d\\approx D / 2$), and apply a transformation of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\label{nice_acl1}\\tag{1}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D} + t(\\mathbf{x}_{1:d}),\\label{nice_acl2}\\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t:\\mathbb{R}^d\\mapsto\\mathbb{R}^{D-d}$ is an arbitrarily complex function, such as a neural network. It is easy to see that the coupling layer as above has an identity Jacobian matrix, and is trivially invertible:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\\\\n",
    "\\mathbf{x}_{d+1:D} &= \\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d}).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Several coupling layers can be composed together to obtain a more complex, layered transformation. Note that a coupling layer leaves part of its input unchanged, and so the roles of the two subsets should be interchanged in alternating layers. \n",
    "\n",
    "If we examine the Jacobian, we can see that at least three coupling layers are needed to allow all dimensions to influence each other (this is left as an exercise for the reader). In the NICE paper, networks were composed with four coupling layers.\n",
    "\n",
    "#### RealNVP\n",
    "RealNVP stands for real-valued, non-volume preserving ([Dinh et al 2017](#Dinh17)). It was a follow-up work to the NICE paper, in which the affine coupling layer was modified as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d},\\label{realnvp_acl1}\\tag{3}\\\\\n",
    "\\mathbf{z}_{d+1:D} &= \\mathbf{x}_{d+1:D}\\odot \\exp(s(\\mathbf{x}_{1:d})) + t(\\mathbf{x}_{1:d}),\\label{realnvp_acl2}\\tag{4}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $s$ and $t$ stand for scale and translation, and are both functions that map from $\\mathbb{R}^d$ to $\\mathbb{R}^{D-d}$. The name RealNVP emphasises the fact that the transformation \\eqref{realnvp_acl1}-\\eqref{realnvp_acl2} is no longer volume-preserving, as is the case with \\eqref{nice_acl1}-\\eqref{nice_acl2}, due to the additional scaling provided by the term $\\exp(s(\\mathbf{x}_{1:d}))$. We use the network output $s(\\mathbf{x}_{1:d})$ as a log-scale parameter for numerical stability.\n",
    "\n",
    "As before, the inverse transformation is no more complex than the forward propagation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d},\\label{realnvp_inv_acl1}\\tag{5}\\\\\n",
    "\\mathbf{x}_{d+1:D} &= (\\mathbf{z}_{d+1:D} - t(\\mathbf{z}_{1:d})) \\odot \\exp(-s(\\mathbf{z}_{1:d})).\\label{realnvp_inv_acl2}\\tag{6}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<img src=\"figures/affine_coupling_layer.png\" alt=\"RealNVP: forward pass\" style=\"width: 800px;\"/>\n",
    "<center>The forward and inverse passes of the RealNVP affine coupling layer</center>\n",
    "\n",
    "Now, the Jacobian is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\mathbb{I}_d & \\mathbf{0}\\\\\n",
    "\\frac{\\partial \\mathbf{z}_{d+1:D}}{\\partial \\mathbf{x}_{1:d}} & \\text{diag}\\,(\\exp (s(\\mathbf{x}_{1:d})))\n",
    "\\end{array}\n",
    "\\right]\\in\\mathbb{R}^{D\\times D}\n",
    "$$\n",
    "\n",
    "and the log of the absolute value of the Jacobian determinant is easily calculated as $\\sum_j s(\\mathbf{x}_{1:d})_j$.\n",
    "\n",
    "#### Spatial and channel-wise masking\n",
    "Observe that the partitioning $\\mathbf{x}\\rightarrow (\\mathbf{x}_{1:d}, \\mathbf{x}_{d+1:D})$ can be implemented using a binary mask $b\\in\\{0, 1\\}^{n_h\\times n_w\\times c}$, so that the forward pass \\eqref{realnvp_acl1}-\\eqref{realnvp_acl2} can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})).\\label{realnvp_acl}\\tag{7}\n",
    "$$\n",
    "\n",
    "Similarly, the inverse pass \\eqref{realnvp_inv_acl1}-\\eqref{realnvp_inv_acl2} can be written\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))).\\label{realnvp_inv_acl}\\tag{8}\n",
    "$$\n",
    "\n",
    "RealNVP implements two types of masking for image data $\\mathbf{x}\\in\\mathbb{R}^{n_h\\times n_w\\times c}$: spatial checkerboard and channel-wise masking. A spatial checkerboard mask applies the same partitioning to every channel dimension, as illustrated in the following figure.\n",
    "\n",
    "<img src=\"figures/checkerboard_mask.png\" alt=\"Checkerboard masking\" style=\"width: 600px;\"/>\n",
    "<center>Spatial checkerboard masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a spatial checkerboard mask $b_s\\in\\{0, 1\\}^{6\\times 6}$, which is broadcast along the channel dimension</center>\n",
    "\n",
    "A channel mask instead operates along the channel dimension, and applies the same partitioning at every spatial location, as in the following figure.\n",
    "\n",
    "<img src=\"figures/channel_mask.png\" alt=\"Channel masking\" style=\"width: 600px;\"/>\n",
    "<center>Channel masking in RealNVP. (a) A layer input $\\mathbf{h}\\in\\mathbb{R}^{6\\times 6\\times 4}$ without masking, and (b) multiplied elementwise by a channel mask $b_c\\in\\{0, 1\\}^{4}$, which is broadcast across the spatial dimensions</center>\n",
    "\n",
    "As in the NICE framework, we want to ensure that all dimensions are able to interact with each other. The RealNVP architecture consists of three layers of alternating checkerboard masks, where the partitions are permuted. \n",
    "\n",
    "<img src=\"figures/alternating_masks.png\" alt=\"Alternating masks\" style=\"width: 900px;\"/>\n",
    "<center>Three affine coupling layers, with alternating masks in between layers</center>\n",
    "\n",
    "#### Squeeze operation\n",
    "In the RealNVP architecture, after the three affine coupling layers with checkerboard masking there is a squeeze operation, where the spatial dimensions of the layer are divided into $2\\times 2\\times c$ subsquares, and reshaped into $1\\times 1\\times 4c$. The figure below illustrates this operation for a single channel:\n",
    "\n",
    "<img src=\"figures/squeeze.png\" alt=\"Squeeze operation\" style=\"width: 600px;\"/>\n",
    "<center>The squeeze operation. The spatial dimensions are halved, and the channel dimension is quadrupled</center>\n",
    "\n",
    "Following the squeeze operation, there are three more affine coupling layers, this time using channel masking, and again permuting the partitions between each layer.\n",
    "\n",
    "#### Multiscale architecture\n",
    "The final component of the RealNVP framework is the multiscale architecture. With the squeeze operation, the spatial dimensions are downsampled, but the channel dimensions are increased. In order to reduce the overall layer sizes in the deeper layers, dimensions are factored out as latent variables at regular intervals.\n",
    "\n",
    "After one of the blocks of coupling-squeeze-coupling described above, half of the dimensions are factored out as latent variables, while the other half is further processed through subsequent layers. \n",
    "\n",
    "<img src=\"figures/factor_out_latent_variables.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/>\n",
    "<center>Example showing how latent variables are factored out in the multiscale architecture. A layer input $\\mathbf{h}^{(k)}\\in\\mathbb{R}^{8\\times 8\\times 2}$ will be reshaped to a $4\\times4\\times8$-shaped tensor after the coupling-squeeze-coupling block. Half of this tensor is absorbed into the base distribution as a latent variable $\\mathbf{z}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ and the remainder $\\mathbf{h}^{(k+1)}\\in\\mathbb{R}^{4\\times 4\\times 4}$ is processed through further layers of the network</center>\n",
    "\n",
    "The complete RealNVP model has multiple levels of the multiscale architecture. This results in latent variables that represent different scales of features in the model. After a number of these levels, the final scale does not use the squeezing operation, and instead applies four affine coupling layers with alternating checkerboard masks to produce the final latent variable.\n",
    "\n",
    "<img src=\"figures/realnvp.png\" alt=\"Multiscale architecture\" style=\"width: 800px;\"/>\n",
    "<center>The end-to-end RealNVP architecture. Each scale consists of a block of 3 coupling layers (with checkerboard mask), squeeze, 3 coupling layers (with channel mask), followed by half of the dimensions factored out as a latent variable. The final scale consists only of 4 coupling layers (with checkerboard mask) to produce the final latent variable</center>\n",
    "\n",
    "The following summarises the forward pass $\\mathbf{z} = f(\\mathbf{x})$ of the overall architecture with $L$ scales. The functions $f^{(1)},\\ldots,f^{(L-1)}$ consist of the coupling-squeeze-coupling block, whereas the function $f^{(L)}$ consists of 4 coupling layers with checkerboard masks.\n",
    "\n",
    ">\n",
    ">$$\\begin{align}\\mathbf{h}^{(0)}&=\\mathbf{x}\\\\ (\\mathbf{z}^{(k+1)}, \\mathbf{h}^{(k+1)})&=f^{(k+1)}(\\mathbf{h}^{(k)}),\\qquad k=0,\\ldots, L-2\\\\ \\mathbf{z}^{(L)}&= f^{(L)}(\\mathbf{h}^{(L-1)})\\\\\n",
    "\\mathbf{z} &= (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})\\end{align}$$\n",
    ">\n",
    "\n",
    "The latent variables factored out at each scale are reshaped and concatenated to produce a single latent variable $\\mathbf{z} = (\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(L)})$, which is assumed to be distributed according to a known base distribution (e.g. a diagonal Gaussian).\n",
    "\n",
    "As a final note, the architecture described in this section was further developed with the Glow model ([Kingma and Dhariwal 2018](#Kingma18)), where the checkerboard and channel-wise masking was replaced with 1x1 convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bijector_subclassing\"></a>\n",
    "## Bijector subclassing\n",
    "\n",
    "In this section we will build a partial implementation of the RealNVP architecture. In particular, we will use bijector subclassing to implement the affine coupling layer as a bijector object, using a binary mask:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})) & \\text{(forward pass)}\\\\\n",
    "\\mathbf{x} &= b\\odot \\mathbf{z} + (1-b)\\odot((\\mathbf{z}-t(b\\odot\\mathbf{z}))\\odot \\exp(-s(b\\odot \\mathbf{z}))) & \\text{(inverse pass)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AffineCouplingLayer class\n",
    "\n",
    "class AffineCouplingLayer(tfb.Bijector):\n",
    "\n",
    "    def __init__(self, shift_and_log_scale_fn, mask, **kwargs):\n",
    "        super(AffineCouplingLayer, self).__init__(\n",
    "            forward_min_event_ndims=1, **kwargs)\n",
    "        self.shift_and_log_scale_fn = shift_and_log_scale_fn\n",
    "        self.b = tf.cast(mask, tf.float32)\n",
    "\n",
    "    def _forward(self, z):\n",
    "        t, log_s = self.shift_and_log_scale_fn(self.b * z)\n",
    "        x = self.b * z + (1 - self.b) * (z * tf.exp(log_s) + t)\n",
    "        return x\n",
    "\n",
    "    def _inverse(self, x):\n",
    "        t, log_s = self.shift_and_log_scale_fn(self.b * x)\n",
    "        z = self.b * x + (1 - self.b) * ((x - t) * tf.exp(-log_s))\n",
    "        return z\n",
    "\n",
    "    def _forward_log_det_jacobian(self, z):\n",
    "        _, log_s = self.shift_and_log_scale_fn(self.b * z)\n",
    "        return tf.reduce_sum(log_s * (1 - self.b), axis=-1)  \n",
    "\n",
    "    def _inverse_log_det_jacobian(self, x):\n",
    "        _, log_s = self.shift_and_log_scale_fn(self.b * x)\n",
    "        return -tf.reduce_sum(log_s * (1 - self.b), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a `shift_and_log_scale_fn` with the following structure:\n",
    "\n",
    "<img src=\"figures/shift_and_log_scale_fn.png\" alt=\"Shift and log-scale network\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example shift_and_log_scale_fn\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "def get_shift_and_log_scale_fn(input_size, hidden_units=[32, 32], activation='relu'):\n",
    "    inputs = Input(shape=input_size)\n",
    "    h1 = inputs\n",
    "    h2 = inputs\n",
    "    for units in hidden_units:\n",
    "        h1 = Dense(units, activation=activation)(h1)\n",
    "        h2 = Dense(units, activation=activation)(h2)\n",
    "    shift = Dense(input_size)(h1)\n",
    "    log_scale = Dense(input_size, activation='tanh')(h2)\n",
    "    return Model(inputs=inputs, outputs=[shift, log_scale])\n",
    "\n",
    "shift_and_log_scale = get_shift_and_log_scale_fn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary mask\n",
    "\n",
    "mask = tf.constant([1, 1, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the AffineCouplingLayer\n",
    "\n",
    "aff = AffineCouplingLayer(shift_and_log_scale, mask)\n",
    "\n",
    "z = tf.random.normal((2, 8))\n",
    "print(z)\n",
    "x = aff.forward(z)  \n",
    "print(x)\n",
    "print(aff.inverse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Use the `forward_log_det_jacobian` and `inverse_log_det_jacobian` methods to compute the log Jacobian determinant on some dummy inputs. Verify that the `forward_log_det_jacobian` method gives the same as first computing the forward transformation, and then taking the negative of the `inverse_log_det_jacobian` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two moons dataset\n",
    "We will now create a normalising flow using the `AffineCouplingLayer` and train it on a two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "\n",
    "train_data = make_moons(n_samples=50000, noise=0.05)[0].astype(np.float32)\n",
    "val_data = make_moons(n_samples=1000, noise=0.05)[0].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(train_data[:1000, 0], train_data[:1000, 1], alpha=0.2)\n",
    "plt.title(\"Two moons data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation Datasets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.map(lambda d: (d, d))\n",
    "train_dataset = train_dataset.shuffle(500).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_data)\n",
    "val_dataset = val_dataset.map(lambda d: (d, d))\n",
    "val_dataset = val_dataset.shuffle(500).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and train the normalising flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bijectors chain\n",
    "\n",
    "num_layers = 8\n",
    "bijectors = []\n",
    "mask = tf.constant([1, 0])\n",
    "\n",
    "for l in range(num_layers):\n",
    "    shift_and_log_scale = get_shift_and_log_scale_fn(2, hidden_units=[256, 256], \n",
    "                                                     activation='relu')\n",
    "    aff = AffineCouplingLayer(shift_and_log_scale, mask)\n",
    "    mask = 1 - mask\n",
    "    bijectors.append(aff)\n",
    "    \n",
    "realnvp_bijector = tfb.Chain(list(reversed(bijectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base distribution\n",
    "\n",
    "base = tfd.MultivariateNormalDiag(loc=tf.zeros((2,)), scale_diag=tf.ones((2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformed distribution\n",
    "\n",
    "flow = realnvp_bijector(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "\n",
    "inputs = Input(shape=(2,))\n",
    "outputs = flow.log_prob(inputs)\n",
    "realnvp_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class SaveSamples(Callback):\n",
    "    \n",
    "    def __init__(self, plot_folder='./plots', **kwargs):\n",
    "        super(SaveSamples, self).__init__(**kwargs)\n",
    "        self.plot_folder = plot_folder\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def plot(self, num):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        ax = plt.gca()\n",
    "        plt.xlim([-1.5, 2.5])\n",
    "        plt.ylim([-1, 1.5])\n",
    "        ax.set_aspect('equal')\n",
    "        samples = flow.sample(2000)\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./plots/{:05d}.png\".format(num))\n",
    "        plt.close()\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        if not os.path.exists(self.plot_folder):\n",
    "            os.makedirs(self.plot_folder)\n",
    "        self.iteration = 0\n",
    "        self.plot(self.iteration + 1)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.iteration += 1\n",
    "        if self.iteration % 30 == 0:\n",
    "            self.plot((self.iteration // 30) + 1)\n",
    "        \n",
    "save_samples = SaveSamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_pred)\n",
    "    \n",
    "realnvp_model.compile(loss=nll, optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
    "history = realnvp_model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[save_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.title(\"Loss vs epochs\")\n",
    "plt.ylabel(\"NLL\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a movie file\n",
    "\n",
    "! ffmpeg -i ./plots/%05d.png -c:v libx264 -vf fps=10 -pix_fmt yuv420p -start_number 00000 samples.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"samples.mp4\",embed=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model transformations\n",
    "\n",
    "samples = flow.sample(2000)\n",
    "noise = realnvp_bijector.inverse(samples)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15,10))\n",
    "h = noise\n",
    "for i, (bij, ax) in enumerate(zip(bijectors, axs.flat)):\n",
    "    ax.scatter(h[:, 0], h[:, 1], alpha=0.1)\n",
    "    ax.set_title(f\"After {i} steps of flow\")\n",
    "    h = bij.forward(h)\n",
    "axs[2, 2].scatter(samples[:, 0], samples[:, 1], alpha=0.1)\n",
    "axs[2, 2].set_title(\"Model samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "filelist = [ f for f in os.listdir('./plots') if f.endswith(\".png\") ]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./plots', f))\n",
    "if os.path.exists('samples.mp4'):\n",
    "    os.remove('samples.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Try re-running the two moons example again, but using a bi-modal base distribution. Is the flow able to more easily approximate the two moons distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"glow\"></a>\n",
    "## Glow\n",
    "\n",
    "Glow ([Kingma et al 2018](#Kingma18)) is short for Generative Flow, and this model builds directly from the RealNVP model. It uses the same multiscale architecture setup, where the latent variables are factored out at different levels with the same mechanism as before. \n",
    "\n",
    "However, Glow replaces the coupling-squeeze-coupling blocks with a different computation block. The Glow model aims to both simplify the RealNVP architecture, as well as to generalise a part of it. \n",
    "\n",
    "As we'll see, Glow still retains the affine coupling layer as the core computation of the model, and also still uses the squeeze operation, and the channel-wise masking. But it removes the checkerboard mask entirely, and also uses a different transformation instead of the alternating mask mechanism of RealNVP.\n",
    "\n",
    "<img src=\"figures/glow_schematic.png\" alt=\"Glow multiscale architecture\" style=\"width: 800px;\"/>\n",
    "<center>Glow uses the same multiscale architecture as in RealNVP, but replaces the coupling-squeeze-coupling block</center>\n",
    "\n",
    "The computation block used in the Glow model is shown in the diagram below.\n",
    "\n",
    "<img src=\"figures/glow_step_arrows_labels.png\" alt=\"Glow copmutation block\" style=\"width: 400px;\"/>\n",
    "<center>The computation block used in the Glow model: a squeeze layer, followed by $K$ steps of flow, each consisting of an actnorm layer, an invertible 1x1 convolution, and an affine coupling layer</center>\n",
    "\n",
    "The squeeze and affine couple layers are the same as in the RealNVP model. The affine coupling layer uses a fixed channel-wise mask. The input to the computation block is first passed through the squeeze layer, which halves each of the spatial dimensions of the input, and multiplies the size of the channel dimension by four.\n",
    "\n",
    "This is followed by a number of steps of flow, indicated in the diagram above by a green block. A single step of flow consists of an actnorm layer, followed by an invertible 1x1 convolution layer - both of which are new to the Glow model - followed by an affine coupling layer.\n",
    "\n",
    "Within each level of the Glow model, the squeeze operation is followed by $K$ steps of flow, before half of the neurons are factored out as latent variables as part of the multi scale architecture that we've seen before.\n",
    "\n",
    "#### Activation normalisation (actnorm)\n",
    "\n",
    "Actnorm is actually a replacement for the batch normalisation layers that are used within the RealNVP model. Recall that batchnorm is used in RealNVP in the shift and scale networks $s$ and $t$ that are used to parameterise the transformation inside the affine coupling layers. It's also applied to the output of the whole affine coupling layer, and this is where we need to compute the log-Jacobian determinant of the transformation, shown in the following table.\n",
    "\n",
    "| Transformation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| log-Jacobian determinant |\n",
    "| :--- | --- |\n",
    "| $\\hat{h}_j^{(k)} = \\frac{\\large h_j^{(k)} - \\mu_{jm}^{(k)}}{\\sqrt{\\left( \\sigma_{jm}^{(k)} \\right)^2 + \\epsilon}} $ | $z_j^{(k)} = \\gamma_j^{(k)}\\hat{h}_j^{(k)} + \\beta_j^{(k)} $ |\n",
    "| $ -\\frac{1}{2} \\sum_j \\log \\left( \\left( \\sigma_{jm}^{(k)}\\right)^2 + \\epsilon \\right)$ | $\\sum_j \\log \\left| \\gamma_j^{(k)} \\right|$ |\n",
    "\n",
    "<center>Batch normalisation transformations and log-Jacobian determinants</center>\n",
    "\n",
    "The potential problem with batch normalisation is that it relies on minibatch statistics being reasonable estimates of the dataset statistics. If the minibatch statistics are high variance, then this can impact performance. In the case of the original Glow implementation, practical computing constraints meant that the the authors needed to reduce the minibatch size to one, and batchnorm then became less effective.\n",
    "\n",
    "The solution in the Glow architecture is to introduce actnorm, or activation normalisation, as an alternative to batchnorm. The transformation of actnorm is a simple affine transformation per feature, parameterised by scale and shift parameters gamma and beta:\n",
    "\n",
    "$$\n",
    "z_j^{(k)} = \\gamma^{(k)}_j h^{(k)}_j + \\beta^{(k)}_j,\n",
    "$$\n",
    "\n",
    "where the subscript $j$ is indexing the feature dimension and $k$ indexes the layer.\n",
    "\n",
    "Actnorm does not compute minibatch statistics, which means that the transformation is more stable, but also less flexible, because it's not aware of the statistics of the given input minibatch, and so it's not able to normalise the input activations towards a target mean and variance.\n",
    "\n",
    "However, the parameters $\\gamma^{(k)}_j$ and $\\beta^{(k)}_j$ are initialised based on the statistics of a sample minibatch. In particular, they're initialised such that the output activations of the layer have zero mean and unit variance on the given sample minibatch. This is an example of data-dependent initialisation, and is trying to initialise these parameters in a good place for the particular dataset, even though the post-activations will drift away from zero mean and unit variance during training.\n",
    "\n",
    "Clearly this transformation is trivially invertible so long as all the gammas are nonzero, and in practice these are parameterised to ensure this.\n",
    "\n",
    "The log-Jacobian determinant is given by \n",
    "\n",
    "$$\n",
    "\\sum_j \\log \\left| \\gamma_j^{(k)} \\right|,\n",
    "$$\n",
    "\n",
    "which is a straightforward and efficient computation.\n",
    "\n",
    "Note that the expressions above assume rank-one inputs, so that in particular $\\mathbf{h}^{(k)}$ would be a vector. When using the Glow model on image inputs, the inputs $\\mathbf{h}^{(k)}$ will be 3-dimensional tensors with shape $(h, w, c)$, and the parameters $\\gamma^{(k)}_j \\in\\mathbb{R}^c$ and $\\beta^{(k)}_j \\in\\mathbb{R}^c$ are shared across every spatial location. In this case, the log-Jacobian determinant is multiplied by $hw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invertible 1x1 convolution\n",
    "\n",
    "Recall the following affine coupling layer sequence in RealNVP, that consists of alternating binary channel-wise masks.\n",
    "\n",
    "<img src=\"figures/alternating_masks.png\" alt=\"Alternating masks\" style=\"width: 900px;\"/>\n",
    "<center>Three affine coupling layers, with alternating channel masks in between layers</center>\n",
    "\n",
    "The forward transformation in one of the affine coupling layers can be written as\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x})),\n",
    "$$\n",
    "\n",
    "where $b$ is the channel mask, that zeros out half of the neurons in the channel dimension. RealNVP composes three affine coupling layers together like this, reversing the binary mask each time. \n",
    "\n",
    "Instead, we could equivalently think of using a fixed binary mask, and permuting the neurons in the layer in the channel dimension after each affine coupling layer. The permutation matrix $W\\in\\mathbb{R}^{c\\times c}$ would have the following block structure.\n",
    "\n",
    "$$\n",
    "W = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\mathbf{0} & \\mathbf{I} \\\\\n",
    "\\mathbf{I} & \\mathbf{0}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This same permutation matrix would then be applied at each spatial location. This is a special case of a convolutional layer, where the convolutional kernel has spatial dimensions of 1 by 1, where there is no bias term in the convolutional layer, and where the number of output channels is the same as the number of input channels.\n",
    "\n",
    "So an equivalent formulation to the sequence of affine coupling layers with an alternating binary channel mask that we looked at before in RealNVP, is where we instead use a 1x1 convolution operation.\n",
    "\n",
    "The main contribution of the Glow model is to generalise the permutation of the channel dimensions by using a learned 1x1 convolution. This convolutional layer will have the same number of input and output channels, and doesn't use a bias term.\n",
    "\n",
    "So then we can write the transformation as a 2-dimensional convolution as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(k+1)} = \\text{conv2d}(\\mathbf{h}^{(k)}; \\mathbf{W}^{(k)}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(k)}$ is the convolutional kernel. The log Jacobian determinant will then be\n",
    "\n",
    "$$\n",
    "h\\cdot w \\cdot \\log~ \\left|~ \\text{det} (\\mathbf{W}^{(k)})~\\right|,\n",
    "$$\n",
    "\n",
    "where $h$ and $w$ are the sizes of the height and width dimensions respectively.\n",
    "\n",
    "The problem here is that it can be expensive to compute the determinant of the convolutional kernel $\\mathbf{W}^{(k)}$. This computation scales as $\\mathcal{O}(c^3)$, where $c$ is the number of channels.\n",
    "\n",
    "A solution to this problem is to use the LU decomposition of the $c\\times c$ matrix $\\mathbf{W}^{(k)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{(k)} = \\mathbf{P}^{(k)} \\mathbf{L}^{(k)} \\left( \\mathbf{U}^{(k)} + \\text{diag}\\left(\\mathbf{s}^{(k)}\\right) \\right).\n",
    "$$\n",
    "\n",
    "In the above, $\\mathbf{P}^{(k)}$ is some permutation matrix, $\\mathbf{L}^{(k)}$ is lower triangular with ones on the diagonal, $\\mathbf{U}^{(k)}$ is upper triangular, and we have separated out the diagonal elements so that $\\mathbf{U}^{(k)}$ is upper triangular with zeros on the diagonal, and the diagonal elements are written as the diagonalisation of a vector $\\mathbf{s}^{(k)}$.\n",
    "\n",
    "In practical implementations, we can directly parameterise the convolutional kernel as in the above expression. The log Jacobian determinant can now be written as $h$ times $w$ times the sum of the log of the absolute values of the elements of the vector $\\mathbf{s}^{(k)}$. This computation now scales as $\\mathcal{O}(c)$, so is much more efficient.\n",
    "\n",
    "The Glow paper initialises the convolutional kernel $\\mathbf{W}^{(k)}$ as a random rotation matrix, and computes the LU decomposition in order to initialise the parameterisation you see above. This will set the permutation matrix $\\mathbf{P}^{(k)}$, which remains fixed throughout the training. The matrices $\\mathbf{L}^{(k)}$ and $\\mathbf{U}^{(k)}$ as well as the vector $\\mathbf{s}^{(k)}$ are all learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine Coupling Layer\n",
    "\n",
    "The third layer in a single step of flow is the affine coupling layer with a (fixed) binary channel-wise mask $b$. The forward transformation and the log-Jacobian determinant are shown again below.\n",
    "\n",
    "<img src=\"figures/realnvp_forward.png\" alt=\"RealNVP: forward pass\" style=\"width: 400px;\"/>\n",
    "\n",
    "| Transformation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| log-Jacobian determinant |\n",
    "| :--- | --- |\n",
    "| $\\mathbf{z} = b\\odot \\mathbf{x} + (1-b)\\odot(\\mathbf{x}\\odot \\exp(s(b\\odot \\mathbf{x})) + t(b\\odot\\mathbf{x}))$ | $\\sum_j \\log \\left| s_j \\right| $ |\n",
    "\n",
    "----\n",
    "\n",
    "The actnorm, invertible 1x1 convolution and affine coupling layers are the three layers that make up a single step of flow of the Glow model. Within each level of the Glow architecture, the inputs are first passed through the squeeze layer, and through $K$ steps of flow, before half of the neurons are factored out as latent variables. The remaining neurons continue on for further processing. This is the same multi scale architecture we saw in the RealNVP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "## References\n",
    "\n",
    "<a class=\"anchor\" id=\"Dinh15\"></a>\n",
    "* Dinh, L., Krueger, D. & Bengio, Y. (2015),\"NICE: Non-linear Independent Components Estimation\", in *3rd International Conference on Learning Representations, (ICLR)*, San Diego, CA, USA, May 7-9, 2015.\n",
    "<a class=\"anchor\" id=\"Dinh17\"></a>\n",
    "* Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2017), \"Density estimation using Real NVP\",  in *5th International Conference on Learning Representations, (ICLR)*, Toulon, France, April 24-26, 2017.\n",
    "<a class=\"anchor\" id=\"Kingma18\"></a>\n",
    "* Kingma, D. P. & Dhariwal, P. (2018), \"Glow: Generative Flow with Invertible 1x1 Convolutions\", in *Advances in Neural Information Processing Systems*, **31**, 10215--10224."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
